{
  "date": "2025-10-21",
  "preset": "general",
  "summary": "## 🌟 오늘의 AI \u0026 Tech 하이라이트\n\n오늘 기술 업계의 가장 큰 소식은 NVIDIA와 Google Cloud의 파트너십 확장입니다. [^34] 이제 누구나 클라우드를 통해 최신 AI 하드웨어와 디지털 트윈 기술을 쉽게 사용할 수 있게 되었습니다. [^34] 동시에 Docker는 복잡한 AI 모델을 개인 컴퓨터에서 손쉽게 실행할 수 있는 새로운 도구를 공개하며, AI 기술의 대중화를 앞당기고 있습니다. [^2]\n\n## 📊 주요 뉴스 브리핑\n\n### 🏢 기업 \u0026 산업 동향\n\n- **NVIDIA와 Google Cloud, AI 동맹 강화**: Google Cloud가 NVIDIA의 최신 RTX PRO 6000 Blackwell GPU를 탑재한 G4 가상머신(VM)의 정식 출시를 발표했습니다. [^34][^36] 이로써 기업들은 고성능 AI 추론뿐만 아니라, 영화 수준의 그래픽 및 시뮬레이션 작업을 클라우드에서 더욱 원활하게 수행할 수 있게 되었습니다. [^34] 또한, 산업용 디지털 트윈 플랫폼인 NVIDIA Omniverse와 로봇 시뮬레이션 도구 Isaac Sim이 Google Cloud Marketplace에 등록되어 제조업, 자동차 등 다양한 산업의 디지털 전환이 가속화될 전망입니다. [^34]\n\n- **Anthropic, Claude의 코드 실행 능력 강화**: AI 모델 Claude 개발사 Anthropic이 '코드 샌드박싱(Code Sandboxing)' 기능을 도입했습니다. [^25] 이는 AI가 코드를 실행할 때마다 사용자에게 권한을 요청하는 번거로움을 줄이고, 지정된 안전한 환경(샌드박스) 안에서 자율적으로 코드를 테스트하고 디버깅하게 하는 기술입니다. [^25] 이를 통해 개발자의 작업 효율을 높이고, 민감한 시스템 파일 접근을 원천적으로 차단하여 보안을 크게 강화했습니다. [^25]\n\n- **Google Gemini API, 지도와 결합**: 개발자들은 이제 Gemini API에서 Google Maps 데이터를 직접 활용할 수 있게 되었습니다. [^23] 이 '그라운딩(Grounding)' 기능은 AI의 추론 능력을 2억 5천만 개가 넘는 장소의 최신 정보와 결합하여, \"여기서 15분 안에 걸어갈 수 있는 최고의 이탈리아 식당 추천해줘\"와 같은 위치 기반 질문에 훨씬 더 정확하고 유용한 답변을 제공합니다. [^23]\n\n### 🔬 기술 혁신 \u0026 연구\n\n- **Docker, 개인 PC에서 AI 모델 실행 간소화**: Docker가 'Open WebUI'라는 새로운 확장 프로그램을 통해 로컬 AI 모델 실행 방식을 획기적으로 개선했습니다. [^2] 이제 Docker Desktop 사용자는 클릭 몇 번만으로 LLaMA 3, Mistral 같은 강력한 언어 모델을 개인 컴퓨터에 설치하고, ChatGPT처럼 편리한 채팅 인터페이스로 활용할 수 있습니다. [^2] 파일 업로드, 음성 입력 등 고급 기능도 지원되어, 인터넷 연결 없이 개인 정보 유출 걱정 없이 AI 비서를 사용하는 시대가 열렸습니다. [^2]\n\n- **Log4Shell 사태의 교훈**: GitHub는 인터넷 역사상 최악의 취약점으로 꼽히는 'Log4Shell' 사태를 되돌아보는 심층 분석 기사를 공개했습니다. [^1] 이 사건은 전 세계 수많은 시스템이 소수의 자원봉사 유지보수 개발자에 의존하는 오픈소스 생태계의 취약성을 드러냈으며, 단순한 코드 문제가 아닌 인적 지원과 보안 교육의 중요성을 일깨워주었습니다. [^1]\n\n- **GNU Octave, 웹 브라우저에서 실행**: 이제 GNU Octave를 JupyterLite와 결합하여 언제 어디서든 웹 브라우저만으로 컴퓨팅 작업을 수행할 수 있게 되었습니다. [^5] 별도의 프로그램 설치 없이 복잡한 계산과 데이터 분석이 가능해져 접근성이 크게 향상되었습니다. [^5]\n\n### 🌐 트렌드 \u0026 인사이트\n\n- **AI의 '산업화'와 '민주화' 동시 진행**: NVIDIA와 Google의 협력은 AI 기술을 특정 전문가의 영역에서 벗어나, 다양한 산업 현장에 적용하는 '산업화' 단계를 보여줍니다. [^34] 동시에 Docker의 사례는 강력한 AI를 개인 개발자나 일반 사용자도 쉽게 활용할 수 있도록 하는 '민주화' 흐름을 명확히 보여주고 있습니다. [^2] 클라우드와 개인용 컴퓨터 양쪽에서 AI 기술의 장벽이 빠르게 허물어지고 있습니다.\n\n- **파인튜닝의 귀환**: 거대 언어 모델(LLM)의 성능이 상향 평준화되면서, 특정 작업에 모델을 최적화하는 '파인튜닝' 기술이 다시 주목받고 있습니다. [^3] 범용 모델만으로는 해결할 수 없는 전문 분야의 문제를 해결하고, 더 적은 비용으로 높은 효율을 얻기 위한 기업들의 노력이 이어지고 있습니다. [^3]\n\n## 💡 오늘의 테이크어웨이\n\n- **AI, 이제 '쓰는' 시대에서 '만들고 쓰는' 시대로**: 클라우드에서는 최고 사양의 AI 인프라를 빌려 쓸 수 있고, 내 컴퓨터에서는 Docker 같은 도구로 나만의 AI를 만들 수 있게 되었습니다. 이제 AI는 단순히 소비하는 대상을 넘어, 우리 각자의 필요에 맞게 직접 구성하고 활용하는 개인화된 도구로 진화하고 있습니다.\n- **오픈소스 없이는 AI도 없다**: Log4Shell 사태가 보여주듯, 화려한 AI 기술의 기반에는 이름 없는 개발자들이 관리하는 수많은 오픈소스 프로젝트가 있습니다. 우리가 사용하는 기술의 안전과 지속가능성은 결국 이들에 대한 지원과 건강한 생태계 조성에 달려있음을 기억해야 합니다.\n\n[^1]: Inside the breach that broke the internet: The untold story of Log4Shell - https://github.blog/open-source/inside-the-breach-that-broke-the-internet-the-untold-story-of-log4shell/\n[^2]: Docker Model Runner Meets Open WebUI: A Simpler Way to Run Local AI Models - https://www.docker.com/blog/open-webui-docker-desktop-model-runner/\n[^3]: 파인튜닝의 귀환에 대한 사례 - https://news.hada.io/topic?id=23793\n[^5]: GNU Octave가 JupyterLite와 만남: 언제 어디서나 컴퓨팅 가능함 - https://news.hada.io/topic?id=23791\n[^23]: Grounding with Google Maps: Now Available in the Gemini API - https://blog.google/technology/developers/grounding-google-maps-gemini-api/\n[^25]: Claude Code Sandboxing - https://www.anthropic.com/engineering/claude-code-sandboxing\n[^34]: NVIDIA and Google Cloud Accelerate Enterprise AI and Industrial Digitalization - https://blogs.nvidia.com/blog/nvidia-google-cloud-enterprise-ai-industrial-digitalization/\n[^36]: The G4 VM is GA: Expanding our NVIDIA GPU portfolio for visual computing and AI - https://cloud.google.com/blog/products/compute/g4-vms-powered-by-nvidia-rtx-6000-blackwell-gpus-are-ga/",
  "systemPrompt": "당신은 매일 아침 기술 뉴스를 정리해주는 친절한 AI 큐레이터입니다.\n\n**역할과 목표:**\n- 바쁜 현대인들이 출근길에 3-5분으로 기술 업계 동향을 파악할 수 있는 일간 브리핑 제공\n- 기술에 관심있는 누구나 이해할 수 있도록 친근하고 명확한 설명\n- 단순 사실 나열이 아닌, 맥락과 의미를 전달하는 스토리텔링\n\n**작성 스타일:**\n- 친근하면서도 전문적인 톤 유지\n- 어려운 기술 개념은 일상적인 비유로 설명\n- 각 소식이 우리 일상과 미래에 미치는 영향 중심으로 서술\n- 긍정적이고 희망적인 관점 유지하되, 중요한 우려사항도 균형있게 전달\n\n**보고서 구조 (반드시 이 형식을 지켜주세요):**\n\n\u003cREPORT_STRUCTURE_START\u003e\n## 🌟 오늘의 AI \u0026 Tech 하이라이트\n\n{{오늘 가장 주목할 만한 기술 소식 2-3줄 요약}}\n\n## 📊 주요 뉴스 브리핑\n\n### 🏢 기업 \u0026 산업 동향\n\n{{주요 기업들의 새로운 발표, 전략 변화, 시장 동향}}\n- 각 기업 소식을 2-3문장으로 간결하게\n- 일반인도 이해할 수 있는 맥락 설명 포함\n\n### 🔬 기술 혁신 \u0026 연구\n\n{{새로운 기술 개발, 연구 성과, 혁신적인 서비스}}\n- 기술의 실제 활용 가능성과 영향력 중심\n- 복잡한 기술도 쉽게 풀어서 설명\n\n### 🌐 트렌드 \u0026 인사이트\n\n{{업계 트렌드, 전문가 의견, 미래 전망}}\n- 개별 뉴스를 연결한 큰 그림 제시\n- 우리 생활에 미칠 영향 예측\n\n## 💡 오늘의 테이크어웨이\n\n{{오늘 뉴스에서 얻을 수 있는 핵심 통찰 1-2개}}\n- 실용적이고 기억하기 쉬운 메시지로\n\u003cREPORT_STRUCTURE_END\u003e\n\n**중요 출력 지침:**\n- 응답에 URL 접근 상태, 분석 과정, 내부 처리 정보 등의 디버그 내용을 포함하지 마세요\n- 바로 완성된 마크다운 보고서만 출력하세요\n- 응답은 반드시 \"## 🌟 오늘의 AI \u0026 Tech 하이라이트\"로 시작해야 합니다\n- 어떤 메타 정보나 과정 설명도 포함하지 말고, 순수한 뉴스 브리핑만 제공하세요\n- GitHub Flavored Markdown을 완벽히 지원하도록 작성하세요\n- ⚠️ \u003cREPORT_STRUCTURE_START\u003e와 \u003cREPORT_STRUCTURE_END\u003e 사이의 구조만 복제하세요 (태그 자체는 출력하지 마세요)\n\n**인용 규칙:**\n- 🚨 CRITICAL: 본문에 인용 없으면 완전히 실패입니다! 🚨\n- 모든 사실, 수치, 회사명, 발표 내용, 기술명 뒤에 반드시 [^1], [^2], [^3] 형태 인용 필수\n- 본문 작성 규칙: 문장을 쓸 때마다 \"이 정보는 어느 기사에서 왔는가?\"를 자문하고 즉시 [^숫자] 추가\n- 🔥 중요: 한 문장에서 동일한 기사에서 나온 여러 정보는 문장 끝에 한 번만 인용하세요\n  - 올바른 예: \"xAI가 Grok 4를 출시하여 OpenAI와 Google을 제쳤다고 발표했습니다.[^1]\"\n  - 잘못된 예: \"xAI[^1]가 Grok 4[^1]를 출시하여 OpenAI[^1]와 Google[^1]을 제쳤다고 발표했습니다.[^1]\"\n- 중요: 여러 개를 인용할 때 [^3, ^4] 금지! 반드시 [^3][^4] 형태로 연속 작성\n- 반드시 문서 맨 끝에 footnote 정의를 다음 형식으로 추가하세요:\n  [^1]: 기사제목 - https://example.com/article-url\n  [^2]: 기사제목 - https://example.com/article-url\n- 🔥 중요: footnote에서 링크 URL은 반드시 클릭 가능한 형태로 포함해야 합니다\n- 기업 이름은 피드 내용에 등장하는 기업들만 언급하고, 임의로 특정 기업을 예시로 들지 마세요",
  "userPrompt": "다음 RSS 피드 데이터를 분석하여 일간 기술 뉴스 브리핑을 작성해주세요.\n\n다음은 최신 AI 관련 피드 데이터입니다:\n\n1. **Inside the breach that broke the internet: The untold story of Log4Shell**\n   - 출처: GitHub Blog\n   - 링크: https://github.blog/open-source/inside-the-breach-that-broke-the-internet-the-untold-story-of-log4shell/\n\n2. **Docker Model Runner Meets Open WebUI: A Simpler Way to Run Local AI Models**\n   - 출처: Docker Blog\n   - 링크: https://www.docker.com/blog/open-webui-docker-desktop-model-runner/\n\n3. **파인튜닝의 귀환에 대한 사례**\n   - 출처: GeekNews\n   - 링크: https://news.hada.io/topic?id=23793\n\n4. **가정에서 시작되는 우정**\n   - 출처: GeekNews\n   - 링크: https://news.hada.io/topic?id=23792\n\n5. **GNU Octave가 JupyterLite와 만남: 언제 어디서나 컴퓨팅 가능함**\n   - 출처: GeekNews\n   - 링크: https://news.hada.io/topic?id=23791\n\n6. **Gleam OTP – 액터 기반 내결함성 멀티코어 프로그램 개발**\n   - 출처: GeekNews\n   - 링크: https://news.hada.io/topic?id=23790\n\n7. **VMware에서 벗어나기 위해 사람들이 무엇을 하고 있는지**\n   - 출처: GeekNews\n   - 링크: https://news.hada.io/topic?id=23789\n\n8. **싱글 보드 컴퓨터 비교**\n   - 출처: GeekNews\n   - 링크: https://news.hada.io/topic?id=23787\n\n9. **DeepSeek OCR**\n   - 출처: GeekNews\n   - 링크: https://news.hada.io/topic?id=23786\n\n10. **Duke Nukem: Zero Hour N64 ROM 리버스 엔지니어링 프로젝트 100% 달성**\n   - 출처: GeekNews\n   - 링크: https://news.hada.io/topic?id=23785\n\n11. **기술공화국 선언**\n   - 출처: GeekNews\n   - 링크: https://news.hada.io/topic?id=23784\n\n12. **브라우저 기반 DuckDB용 SQL IDE인 Duck-UI**\n   - 출처: GeekNews\n   - 링크: https://news.hada.io/topic?id=23783\n\n13. **Transportation SEC says SpaceX is behind on moon trip and will reopen contracts**\n   - 출처: Hacker News\n   - 링크: https://www.cnbc.com/2025/10/20/nasa-duffy-spacex-artemis-moon-landing.html\n\n14. **A Looking Glass Half Empty, Part 2: A Series of Unfortunate Events**\n   - 출처: Hacker News\n   - 링크: https://www.filfre.net/2025/10/a-looking-glass-half-empty-part-2-a-series-of-unfortunate-events/\n\n15. **Cut Up Your Books**\n   - 출처: Hacker News\n   - 링크: https://kobold.blog/cut-up-your-books/\n\n16. **Distribution of Correlation**\n   - 출처: Hacker News\n   - 링크: https://www.johndcook.com/blog/2025/10/20/distribution-of-correlation/\n\n17. **Has your iPhone typing accuracy been getting worse? This video may vindicate you**\n   - 출처: Hacker News\n   - 링크: https://www.phonearena.com/news/has-your-iphone-typing-accuracy-been-getting-worse-this-video-may-vindicate-you_id175022\n\n18. **Does Your Business Need a Content Strategy?**\n   - 출처: Hacker News\n   - 링크: https://www.punch-tape.com/blog/does-your-business-need-a-content-strategy\n\n19. **AI Coding Sucks [video]**\n   - 출처: Hacker News\n   - 링크: https://www.youtube.com/watch?v=0ZUkQF6boNg\n\n20. **Witness-Network.org**\n   - 출처: Hacker News\n   - 링크: https://witness-network.org/\n\n21. **Cancer drug combo slashes risk of death by more than 40%**\n   - 출처: Hacker News\n   - 링크: https://newatlas.com/cancer/prostate-cancer-drug-combo/\n\n22. **Morsel-Driven Parallelism: A NUMA-Aware Query Evaluation Framework [pdf]**\n   - 출처: Hacker News\n   - 링크: https://db.in.tum.de/~leis/papers/morsels.pdf\n\n23. **Grounding with Google Maps: Now Available in the Gemini API**\n   - 출처: Hacker News\n   - 링크: https://blog.google/technology/developers/grounding-google-maps-gemini-api/\n\n24. **Retinal Implant Restores Central Vision in Patients with Advanced AMD**\n   - 출처: Hacker News\n   - 링크: https://www.upmc.com/media/news/102025-retinal-implant\n\n25. **Claude Code Sandboxing**\n   - 출처: Hacker News\n   - 링크: https://www.anthropic.com/engineering/claude-code-sandboxing\n\n26. **More streaming video is bad**\n   - 출처: Hacker News\n   - 링크: https://www.slowboring.com/p/more-streaming-video-is-bad\n\n27. **Adherence of traffic-related particles to human red blood cells in vivo**\n   - 출처: Hacker News\n   - 링크: https://publications.ersnet.org/content/erjor/early/2025/09/04/2312054100767-2025\n\n28. **China claims America is biggest bit burglar**\n   - 출처: Hacker News\n   - 링크: https://www.theregister.com/2025/10/20/china_accuses_us_cyber_warfare/\n\n29. **I turned my CV into an AI chatbot**\n   - 출처: Hacker News\n   - 링크: https://iamluismarcos.com/\n\n30. **Elixir-like pipes in Ruby (oh no not again)**\n   - 출처: Hacker News\n   - 링크: https://zverok.space/blog/2024-11-16-elixir-pipes.html\n\n31. **Aegaeon: Effective GPU Pooling for Concurrent LLM Serving on the Market**\n   - 출처: Hacker News\n   - 링크: https://dl.acm.org/doi/pdf/10.1145/3731569.3764815\n\n32. **Figma is reporting service disruptions**\n   - 출처: Hacker News\n   - 링크: https://status.figma.com\n\n33. **Open Source AI Week — How Developers and Contributors Are Advancing AI Innovation**\n   - 출처: NVIDIA Blog\n   - 링크: https://blogs.nvidia.com/blog/open-source-ai-week/\n\n34. **NVIDIA and Google Cloud Accelerate Enterprise AI and Industrial Digitalization**\n   - 출처: NVIDIA Blog\n   - 링크: https://blogs.nvidia.com/blog/nvidia-google-cloud-enterprise-ai-industrial-digitalization/\n\n35. **How AI Is Unlocking Level 4 Autonomous Driving**\n   - 출처: NVIDIA Blog\n   - 링크: https://blogs.nvidia.com/blog/level-4-autonomous-driving-ai/\n\n36. **The G4 VM is GA: Expanding our NVIDIA GPU portfolio for visual computing and AI**\n   - 출처: AI \u0026 Machine Learning\n   - 링크: https://cloud.google.com/blog/products/compute/g4-vms-powered-by-nvidia-rtx-6000-blackwell-gpus-are-ga/\n\n37. **G4 VMs under the hood: A custom, high-performance P2P fabric for multi-GPU workloads**\n   - 출처: AI \u0026 Machine Learning\n   - 링크: https://cloud.google.com/blog/products/compute/g4-vms-p2p-fabric-boosts-multi-gpu-workloads/\n\n38. **Google named a Leader in the 2025 IDC MarketScape for Worldwide GenAI Life-Cycle Foundation Model Software**\n   - 출처: AI \u0026 Machine Learning\n   - 링크: https://cloud.google.com/blog/products/ai-machine-learning/google-named-a-leader-in-the-2025-idc-marketscape/\n\n39. **Building scalable AI agents: Design patterns with Agent Engine on Google Cloud**\n   - 출처: AI \u0026 Machine Learning\n   - 링크: https://cloud.google.com/blog/topics/partners/building-scalable-ai-agents-design-patterns-with-agent-engine-on-google-cloud/\n\n\n\n**분석 지침:**\n- 반드시 위에 명시된 마크다운 헤더 구조를 정확히 따르세요\n- 각 섹션은 2-3개 포인트로 제한\n- 구체적인 수치와 데이터 활용으로 신뢰성 확보\n\n**🌟 URL 컨텍스트 활용 지침:**\n- 제공된 URL의 내용을 적극적으로 활용하여 깊이 있는 분석을 제공하세요\n- 단순 요약이 아닌, 기사의 핵심 인사이트와 숨겨진 의미를 발굴하세요\n- 여러 기사 간의 연결점을 찾아 큰 그림을 그려주세요\n- 기술적 세부사항과 실제 영향력을 균형있게 다루세요\n\n**🎯 독자 재미 극대화 지침:**\n- 딱딱한 기술 뉴스를 생동감 있게 전달하세요\n- 적절한 비유와 실생활 예시로 복잡한 개념을 쉽게 설명하세요\n- 놀라운 사실이나 의외의 관점을 제시하여 호기심을 자극하세요\n- 스토리텔링 요소를 활용하여 뉴스를 하나의 이야기로 엮어주세요\n- 각 프리셋의 톤에 맞는 위트와 유머를 적절히 활용하세요\n\n**🚨 인용 검수 체크리스트:**\n1. 본문의 모든 사실, 수치, 기업명, 기술명에 [^숫자] 인용이 있는가?\n2. 문서 맨 끝에 모든 footnote 정의가 있고, 각각 클릭 가능한 URL을 포함하는가?\n3. [^1]: 기사제목 - https://링크 형식이 정확한가?\n4. 본문에서 언급한 모든 [^숫자]에 대응하는 footnote가 있는가?\n- 최종 제출 전 필수 검토: 위 체크리스트를 모두 확인하세요",
  "articles": [
    {
      "title": "Inside the breach that broke the internet: The untold story of Log4Shell",
      "link": "https://github.blog/open-source/inside-the-breach-that-broke-the-internet-the-untold-story-of-log4shell/",
      "source": "GitHub Blog",
      "category": "tech",
      "publishedAt": "2025-10-20T16:00:16Z",
      "description": "\u003cp\u003eLog4Shell proved that open source security isn't guaranteed and isn’t just a code problem. It's about supporting, enabling, and empowering the people behind the projects that build our digital infrastructure.\u003c/p\u003e\n\u003cp\u003eThe post \u003ca href=\"https://github.blog/open-source/inside-the-breach-that-broke-the-internet-the-untold-story-of-log4shell/\"\u003eInside the breach that broke the internet: The untold story of Log4Shell\u003c/a\u003e appeared first on \u003ca href=\"https://github.blog\"\u003eThe GitHub Blog\u003c/a\u003e.\u003c/p\u003e\n"
    },
    {
      "title": "Docker Model Runner Meets Open WebUI: A Simpler Way to Run Local AI Models",
      "link": "https://www.docker.com/blog/open-webui-docker-desktop-model-runner/",
      "source": "Docker Blog",
      "category": "tech",
      "publishedAt": "2025-10-20T13:46:26Z",
      "description": "Hi, I’m Sergei Shitikov - a Docker Captain and Lead Software Engineer living in Berlin. I'm focused on DevOps, developer experience, open source, and local AI tools. I created this extension to make it easier for anyone - even without a technical background - to get started with local LLMs using Docker Model Runner and..."
    },
    {
      "title": "파인튜닝의 귀환에 대한 사례",
      "link": "https://news.hada.io/topic?id=23793",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-21T06:34:17+09:00",
      "description": "\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003e파인튜닝\u003c/strong\u003e 방식이 최근 \u003cstrong\u003eTinker\u003c/strong\u003e 등 새로운 플랫폼의 등장을 계기로 다시 주목을 받음\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eLoRA\u003c/strong\u003e와 같은 저비용 파인튜닝 기법이 확산되어 기존 전체 학습(Fine-Tuning)에 비해 경제성과 실용성이 크게 향상됨\u003c/li\u003e\n\u003cli\u003e오픈소스와 \u003cstrong\u003e자가 관리형...\u003c/p\u003e"
    },
    {
      "title": "가정에서 시작되는 우정",
      "link": "https://news.hada.io/topic?id=23792",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-21T02:44:10+09:00",
      "description": "\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003e자기 사랑\u003c/strong\u003e이 타인과의 \u003cstrong\u003e우정의 바탕\u003c/strong\u003e임\u003c/li\u003e\n\u003cli\u003e자신과의 \u003cstrong\u003e내적 조화\u003c/strong\u003e 없이는 건강한 대인 관계 형성 어려움\u003c/li\u003e\n\u003cli\u003e우정의 성향은 \u003cstrong\u003e환경과 타인에게 받은 영향\u003c/strong\u003e에도 좌우됨\u003c/li\u003e\n\u003cli\u003e진정한 우정은 \u003cstrong\u003e상호성, 인식, 변화 수용\u003c/stro...\u003c/p\u003e"
    },
    {
      "title": "GNU Octave가 JupyterLite와 만남: 언제 어디서나 컴퓨팅 가능함",
      "link": "https://news.hada.io/topic?id=23791",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-21T02:41:09+09:00",
      "description": "\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eMedium\u003c/strong\u003e이 전 세계적인 \u003cstrong\u003e호스팅 장애\u003c/strong\u003e로 현재 접속 불가 상태임\u003c/li\u003e\n\u003cli\u003e회사 측은 \u003cstrong\u003e문제 해결\u003c/strong\u003e을 위해 신속히 대응 중임\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e읽기와 쓰기\u003c/strong\u003e 기능 모두 일시적으로 제한됨\u003c/li\u003e\n\u003cli\u003e사용자 경험에 \u003cstrong\u003e일시적 불편\u003c/strong\u003e이 발생함...\u003c/p\u003e"
    },
    {
      "title": "Gleam OTP – 액터 기반 내결함성 멀티코어 프로그램 개발",
      "link": "https://news.hada.io/topic?id=23790",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-21T02:38:08+09:00",
      "description": "\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eGleam OTP\u003c/strong\u003e는 \u003cstrong\u003e액터 모델\u003c/strong\u003e을 활용하여 내결함성과 멀티코어 성능을 갖춘 프로그램 개발 지원\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e완전한 타입 안전성\u003c/strong\u003e과 \u003cstrong\u003eErlang OTP와의 호환성\u003c/strong\u003e을 목표로 하는 것이 특징임\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e슈퍼바이저\u003c/strong\u003e를 통한 장애 복구 및...\u003c/p\u003e"
    },
    {
      "title": "VMware에서 벗어나기 위해 사람들이 무엇을 하고 있는지",
      "link": "https://news.hada.io/topic?id=23789",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-21T01:33:34+09:00",
      "description": "\u003cul\u003e\n\u003cli\u003e많은 기업들이 \u003cstrong\u003eVMware의 높은 라이선스 비용\u003c/strong\u003e과 최근 변화로 인해 대안을 모색 중임\u003c/li\u003e\n\u003cli\u003e여러 중소 규모 IT팀은 \u003cstrong\u003eProxmox VE\u003c/strong\u003e, \u003cstrong\u003eoVirt\u003c/strong\u003e와 같은 \u003cstrong\u003e오픈소스 가상화 솔루션\u003c/strong\u003e으로 전환을 검토함\u003c/li\u003e\n\u003cli\u003eKVM, Xen, Hyper-V 등 \u003cstrong\u003e다른 하...\u003c/p\u003e"
    },
    {
      "title": "싱글 보드 컴퓨터 비교",
      "link": "https://news.hada.io/topic?id=23787",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-21T00:33:18+09:00",
      "description": "\u003cul\u003e\n\u003cli\u003e다양한 \u003cstrong\u003e싱글 보드 컴퓨터(SBC)\u003c/strong\u003e 를 종합적으로 비교할 수 있는 웹사이트 제공임\u003c/li\u003e\n\u003cli\u003e이 사이트는 \u003cstrong\u003e성능 벤치마크\u003c/strong\u003e와 실제 사양을 바탕으로 상세한 비교 정보 제공임\u003c/li\u003e\n\u003cli\u003e사용자는 프로젝트 목적에 맞는 \u003cstrong\u003e최적의 SBC 선택\u003c/strong\u003e을 쉽게 할 수 있음\u003c/li\u003e\n\u003cli...\u003c/p\u003e"
    },
    {
      "title": "DeepSeek OCR",
      "link": "https://news.hada.io/topic?id=23786",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-20T23:58:14+09:00",
      "description": "\u003cp\u003e한 줄 요약\u003c/p\u003e\n\u003cp\u003e문서/대화 기록을 이미지(시각 토큰) 로 바꿔서 LLM 컨텍스트를 크게 줄이고(≈7–20×), 다시 텍스트로 정확히 복원(OCR)하는 광학적 컨텍스트 압축을 제안·검증. 새 비전 인코더(DeepEncoder)와 3B MoE 디코더를 결합해 적은 비전 토큰으로도 SOTA급 문서 파싱 성능을 보입니다.\u003c/p\u003e\n\u003cp\u003e문제 정의"
    },
    {
      "title": "Duke Nukem: Zero Hour N64 ROM 리버스 엔지니어링 프로젝트 100% 달성",
      "link": "https://news.hada.io/topic?id=23785",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-20T23:34:09+09:00",
      "description": "\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eDuke Nukem: Zero Hour\u003c/strong\u003e의 Nintendo 64 ROM을 완벽하게 디컴파일링한 오픈소스 프로젝트 소개임\u003c/li\u003e\n\u003cli\u003e이 리포지토리는 \u003cstrong\u003e원게임 소프트웨어의 모든 소스 코드를 되살리는 작업\u003c/strong\u003e을 100% 달성함\u003c/li\u003e\n\u003cli\u003e사용자는 직접 게임의 ROM을 소유해야 하며, \u003cstrong\u003e원본 US 또는 프...\u003c/p\u003e"
    },
    {
      "title": "기술공화국 선언",
      "link": "https://news.hada.io/topic?id=23784",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-20T23:30:21+09:00",
      "description": "\u003ch2\u003e기술공화국 선언 — 기술, 인간, 그리고 리더십의 재구성\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e소프트웨어 시대의 본질\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e기술 혁신의 뿌리는 ‘소비자용 앱’이 아니라 \u003cstrong\u003e국가적·산업적 문제를 해결하던 과학자들의 정신\u003c/strong\u003e이었음.\u003c/li\u003e\n\u003cli\u003e창의성이나 진정성 집착은 때로 \u003cstrong\u003e인간 중"
    },
    {
      "title": "브라우저 기반 DuckDB용 SQL IDE인 Duck-UI",
      "link": "https://news.hada.io/topic?id=23783",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-20T22:39:15+09:00",
      "description": "\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eDuck-UI\u003c/strong\u003e는 브라우저에서 동작하는 SQL IDE로, \u003cstrong\u003eDuckDB\u003c/strong\u003e를 지원함\u003c/li\u003e\n\u003cli\u003e별도의 설치 없이 웹 환경에서 데이터 쿼리 작업이 가능함\u003c/li\u003e\n\u003cli\u003e사용자 친화적 인터페이스를 통해 빠른 데이터 분석 환경을 제공함\u003c/li\u003e\n\u003cli\u003e오픈소스 기반으로, 개발자 및 데이터 과학자에게 \u003cs...\u003c/p\u003e"
    },
    {
      "title": "Transportation SEC says SpaceX is behind on moon trip and will reopen contracts",
      "link": "https://www.cnbc.com/2025/10/20/nasa-duffy-spacex-artemis-moon-landing.html",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:57:52Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.cnbc.com/2025/10/20/nasa-duffy-spacex-artemis-moon-landing.html\"\u003ehttps://www.cnbc.com/2025/10/20/nasa-duffy-spacex-artemis-moon-landing.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649951\"\u003ehttps://news.ycombinator.com/item?id=45649951\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "A Looking Glass Half Empty, Part 2: A Series of Unfortunate Events",
      "link": "https://www.filfre.net/2025/10/a-looking-glass-half-empty-part-2-a-series-of-unfortunate-events/",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:57:32Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.filfre.net/2025/10/a-looking-glass-half-empty-part-2-a-series-of-unfortunate-events/\"\u003ehttps://www.filfre.net/2025/10/a-looking-glass-half-empty-part-2-a-series-of-unfortunate-events/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649948\"\u003ehttps://news.ycombinator.com/item?id=45649948\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Cut Up Your Books",
      "link": "https://kobold.blog/cut-up-your-books/",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:57:28Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://kobold.blog/cut-up-your-books/\"\u003ehttps://kobold.blog/cut-up-your-books/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649947\"\u003ehttps://news.ycombinator.com/item?id=45649947\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Distribution of Correlation",
      "link": "https://www.johndcook.com/blog/2025/10/20/distribution-of-correlation/",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:56:37Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.johndcook.com/blog/2025/10/20/distribution-of-correlation/\"\u003ehttps://www.johndcook.com/blog/2025/10/20/distribution-of-correlation/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649934\"\u003ehttps://news.ycombinator.com/item?id=45649934\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Has your iPhone typing accuracy been getting worse? This video may vindicate you",
      "link": "https://www.phonearena.com/news/has-your-iphone-typing-accuracy-been-getting-worse-this-video-may-vindicate-you_id175022",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:55:30Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.phonearena.com/news/has-your-iphone-typing-accuracy-been-getting-worse-this-video-may-vindicate-you_id175022\"\u003ehttps://www.phonearena.com/news/has-your-iphone-typing-accuracy-been-getting-worse-this-video-may-vindicate-you_id175022\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649923\"\u003ehttps://news.ycombinator.com/item?id=45649923\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Does Your Business Need a Content Strategy?",
      "link": "https://www.punch-tape.com/blog/does-your-business-need-a-content-strategy",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:55:00Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.punch-tape.com/blog/does-your-business-need-a-content-strategy\"\u003ehttps://www.punch-tape.com/blog/does-your-business-need-a-content-strategy\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649917\"\u003ehttps://news.ycombinator.com/item?id=45649917\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 1\u003c/p\u003e\n"
    },
    {
      "title": "AI Coding Sucks [video]",
      "link": "https://www.youtube.com/watch?v=0ZUkQF6boNg",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:53:47Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.youtube.com/watch?v=0ZUkQF6boNg\"\u003ehttps://www.youtube.com/watch?v=0ZUkQF6boNg\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649905\"\u003ehttps://news.ycombinator.com/item?id=45649905\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Witness-Network.org",
      "link": "https://witness-network.org/",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:50:50Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://witness-network.org/\"\u003ehttps://witness-network.org/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649878\"\u003ehttps://news.ycombinator.com/item?id=45649878\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Cancer drug combo slashes risk of death by more than 40%",
      "link": "https://newatlas.com/cancer/prostate-cancer-drug-combo/",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:46:28Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://newatlas.com/cancer/prostate-cancer-drug-combo/\"\u003ehttps://newatlas.com/cancer/prostate-cancer-drug-combo/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649835\"\u003ehttps://news.ycombinator.com/item?id=45649835\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Morsel-Driven Parallelism: A NUMA-Aware Query Evaluation Framework [pdf]",
      "link": "https://db.in.tum.de/~leis/papers/morsels.pdf",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:46:26Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://db.in.tum.de/~leis/papers/morsels.pdf\"\u003ehttps://db.in.tum.de/~leis/papers/morsels.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649834\"\u003ehttps://news.ycombinator.com/item?id=45649834\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Grounding with Google Maps: Now Available in the Gemini API",
      "link": "https://blog.google/technology/developers/grounding-google-maps-gemini-api/",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:39:00Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://blog.google/technology/developers/grounding-google-maps-gemini-api/\"\u003ehttps://blog.google/technology/developers/grounding-google-maps-gemini-api/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649737\"\u003ehttps://news.ycombinator.com/item?id=45649737\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 2\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Retinal Implant Restores Central Vision in Patients with Advanced AMD",
      "link": "https://www.upmc.com/media/news/102025-retinal-implant",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:38:47Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.upmc.com/media/news/102025-retinal-implant\"\u003ehttps://www.upmc.com/media/news/102025-retinal-implant\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649734\"\u003ehttps://news.ycombinator.com/item?id=45649734\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 1\u003c/p\u003e\n"
    },
    {
      "title": "Claude Code Sandboxing",
      "link": "https://www.anthropic.com/engineering/claude-code-sandboxing",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:38:44Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.anthropic.com/engineering/claude-code-sandboxing\"\u003ehttps://www.anthropic.com/engineering/claude-code-sandboxing\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649732\"\u003ehttps://news.ycombinator.com/item?id=45649732\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "More streaming video is bad",
      "link": "https://www.slowboring.com/p/more-streaming-video-is-bad",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:36:35Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.slowboring.com/p/more-streaming-video-is-bad\"\u003ehttps://www.slowboring.com/p/more-streaming-video-is-bad\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649707\"\u003ehttps://news.ycombinator.com/item?id=45649707\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 2\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Adherence of traffic-related particles to human red blood cells in vivo",
      "link": "https://publications.ersnet.org/content/erjor/early/2025/09/04/2312054100767-2025",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:35:59Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://publications.ersnet.org/content/erjor/early/2025/09/04/2312054100767-2025\"\u003ehttps://publications.ersnet.org/content/erjor/early/2025/09/04/2312054100767-2025\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649703\"\u003ehttps://news.ycombinator.com/item?id=45649703\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "China claims America is biggest bit burglar",
      "link": "https://www.theregister.com/2025/10/20/china_accuses_us_cyber_warfare/",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:34:41Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.theregister.com/2025/10/20/china_accuses_us_cyber_warfare/\"\u003ehttps://www.theregister.com/2025/10/20/china_accuses_us_cyber_warfare/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649679\"\u003ehttps://news.ycombinator.com/item?id=45649679\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 2\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "I turned my CV into an AI chatbot",
      "link": "https://iamluismarcos.com/",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:33:51Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://iamluismarcos.com/\"\u003ehttps://iamluismarcos.com/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649670\"\u003ehttps://news.ycombinator.com/item?id=45649670\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Elixir-like pipes in Ruby (oh no not again)",
      "link": "https://zverok.space/blog/2024-11-16-elixir-pipes.html",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:33:28Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://zverok.space/blog/2024-11-16-elixir-pipes.html\"\u003ehttps://zverok.space/blog/2024-11-16-elixir-pipes.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649664\"\u003ehttps://news.ycombinator.com/item?id=45649664\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Aegaeon: Effective GPU Pooling for Concurrent LLM Serving on the Market",
      "link": "https://dl.acm.org/doi/pdf/10.1145/3731569.3764815",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:33:26Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://dl.acm.org/doi/pdf/10.1145/3731569.3764815\"\u003ehttps://dl.acm.org/doi/pdf/10.1145/3731569.3764815\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649662\"\u003ehttps://news.ycombinator.com/item?id=45649662\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Figma is reporting service disruptions",
      "link": "https://status.figma.com",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:33:15Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://status.figma.com\"\u003ehttps://status.figma.com\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649657\"\u003ehttps://news.ycombinator.com/item?id=45649657\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 2\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Open Source AI Week — How Developers and Contributors Are Advancing AI Innovation",
      "link": "https://blogs.nvidia.com/blog/open-source-ai-week/",
      "source": "NVIDIA Blog",
      "category": "tech",
      "publishedAt": "2025-10-20T17:29:33Z",
      "description": "NVIDIA’s on the ground at Open Source AI Week. Stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward. Follow NVIDIA AI Developer on social channels for additional news and insights. Andrej Karpathy’s Nanochat Teaches Developers How to Train LLMs in Four Hours 🔗 Computer scientist Andrej\t\u003ca class=\"read-more\" href=\"https://blogs.nvidia.com/blog/open-source-ai-week/\"\u003e\n\t\tRead Article\t\t\u003cspan data-icon=\"y\"\u003e\u003c/span\u003e\n\t\u003c/a\u003e\n\t"
    },
    {
      "title": "NVIDIA and Google Cloud Accelerate Enterprise AI and Industrial Digitalization",
      "link": "https://blogs.nvidia.com/blog/nvidia-google-cloud-enterprise-ai-industrial-digitalization/",
      "source": "NVIDIA Blog",
      "category": "tech",
      "publishedAt": "2025-10-20T16:00:53Z",
      "description": "NVIDIA and Google Cloud are expanding access to accelerated computing to transform the full spectrum of enterprise workloads, from visual computing to agentic and physical AI. Google Cloud today announced the general availability of G4 VMs, powered by NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs. Plus, NVIDIA Omniverse and NVIDIA Isaac Sim are now\t\u003ca class=\"read-more\" href=\"https://blogs.nvidia.com/blog/nvidia-google-cloud-enterprise-ai-industrial-digitalization/\"\u003e\n\t\tRead Article\t\t\u003cspan data-icon=\"y\"\u003e\u003c/span\u003e\n\t\u003c/a\u003e\n\t"
    },
    {
      "title": "How AI Is Unlocking Level 4 Autonomous Driving",
      "link": "https://blogs.nvidia.com/blog/level-4-autonomous-driving-ai/",
      "source": "NVIDIA Blog",
      "category": "tech",
      "publishedAt": "2025-10-20T15:00:51Z",
      "description": "When the Society of Automotive Engineers established its framework for vehicle autonomy in 2014, it created the industry-standard roadmap for self-driving technology. The levels of automation progress from level 1 (driver assistance) to level 2 (partial automation), level 3 (conditional automation), level 4 (high automation) and level 5 (full automation). Predicting when each level would\t\u003ca class=\"read-more\" href=\"https://blogs.nvidia.com/blog/level-4-autonomous-driving-ai/\"\u003e\n\t\tRead Article\t\t\u003cspan data-icon=\"y\"\u003e\u003c/span\u003e\n\t\u003c/a\u003e\n\t"
    },
    {
      "title": "The G4 VM is GA: Expanding our NVIDIA GPU portfolio for visual computing and AI",
      "link": "https://cloud.google.com/blog/products/compute/g4-vms-powered-by-nvidia-rtx-6000-blackwell-gpus-are-ga/",
      "source": "AI \u0026 Machine Learning",
      "category": "tech",
      "publishedAt": "2025-10-20T16:00:00Z",
      "description": "\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eMany of today’s multimodal workloads require a powerful mix of GPU-based accelerators, large GPU memory, and professional graphics to achieve the performance and throughput that they need. Today, we announced the general availability of the \u003c/span\u003e\u003ca href=\"https://cloud.google.com/compute/docs/accelerator-optimized-machines#g4-series\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eG4 VM\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, powered by NVIDIA’s RTX PRO 6000 Blackwell Server Edition GPUs. The addition of the G4 expands our comprehensive NVIDIA GPU portfolio, complementing the specialized scale of the A-series VMs, and the cost-efficiency of G2 VMs. The G4 VM is available now, bringing GPU availability to more Google Cloud regions than ever before, for applications that are latency sensitive or have specific regulatory requirements.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eWe also announced the general availability of NVIDIA Omniverse as a virtual machine image (VMI) on \u003c/span\u003e\u003ca href=\"https://console.cloud.google.com/marketplace/browse?q=Nvidia%20omniverse\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGoogle Cloud Marketplace\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e. When run on G4, it’s easier than ever to develop and deploy industrial digital twin and physical AI simulation applications leveraging \u003c/span\u003e\u003ca href=\"https://www.nvidia.com/en-us/omniverse/\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eNVIDIA Omniverse\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e libraries. G4 VMs provide the necessary infrastructure — up to 768 GB of GDDR7 memory, NVIDIA Tensor Cores, and fourth-generation Ray Tracing (RT) cores — to run the demanding real-time rendering and physically accurate simulations required for enterprise digital twins. Together, they provide a scalable cloud environment to build, deploy, and interact with applications for industrial digital twins or robotics simulation. \u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eA universal GPU platform\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe G4 VM offers a profound leap in performance, with up to 9x the throughput of G2 instances, enabling a step-change in results for a wide spectrum of workloads, from multi-modal AI inference, photorealistic design and visualization, and robotics simulation using applications developed on NVIDIA Omniverse. The G4 currently comes in 1, 2, 4, and 8 NVIDIA RTX PRO 6000 Blackwell GPU options, with fractional GPU options coming soon. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eHere are some of the ways you can use G4 to innovate and accelerate your business:\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eAI training, fine-tuning, and inference\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGenerative AI acceleration and efficiency\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: With its FP4 precision support, G4’s high-efficiency compute accelerates LLM fine-tuning and inference, letting you create real-time generative AI applications such as multimodal and text-to-image creation models.\u003c/span\u003e\u003c/li\u003e\n\u003cli role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eResource optimization with Multi-Instance GPU (MIG) support\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: G4 allows a single GPU to be securely partitioned into up to four fully isolated MIG instances, each with its own high-bandwidth memory, compute cores, and dedicated media engines. This feature maximizes price-performance by enabling multiple smaller distinct workloads to run concurrently with guaranteed resources, isolation, and quality of service..\u003c/span\u003e\u003c/li\u003e\n\u003cli role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eFlexible model capacity and scaling\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: Serve a wide range of models, from less than 30B to over 100B parameters, by leveraging advanced quantization techniques, MIG partitioning, and multi-GPU configurations. \u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eNVIDIA Omniverse and simulation\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eNVIDIA Omniverse integration\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: Choose this foundation to build and connect simulation applications using physically-based simulation and OpenUSD that enable real-time interactivity and the development of AI-accelerated digital twins.\u003c/span\u003e\u003c/li\u003e\n\u003cli role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eLarge-scale digital twin acceleration\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: Accelerate proprietary or commercial computer-aided engineering and simulation software to run scenarios with billions of cells in complex digital twin environments.\u003c/span\u003e\u003c/li\u003e\n\u003cli role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eNear-real-time physics analysis\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: Leverage the G4’s parallel compute power and memory to handle immense computational domains, enabling near-real-time computational fluid dynamics and complex physics analysis for high-fidelity simulations.\u003c/span\u003e\u003c/li\u003e\n\u003cli role=\"presentation\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eRobotics development\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: With \u003c/span\u003e\u003ca href=\"https://console.cloud.google.com/marketplace/product/nvidia/nvidia-isaac-sim-development-workstation-windows?project=nvidia-vgpu-public\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eNVIDIA Isaac Sim\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, an open-source, reference robotic simulation framework, customers are now able to create, train, and simulate AI-driven robots in physical and virtual environments. Isaac Sim is now available on the Google Cloud Marketplace.\u003c/span\u003e\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eAI-driven rendering, graphics and virtual workstations\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eAI-augmented content creation\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: Harness neural shaders and fifth-generation NVIDIA Tensor Cores to integrate AI directly into a programmable rendering pipeline, driving the next decade of AI-augmented graphics innovations, including real-time cinematic rendering and enhanced content creation.\u003c/span\u003e\u003c/li\u003e\n\u003cli role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eMassive scene handling\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: Leverage massive memory (up to 96 GB per GPU on the G4) to create and render large complex 3D models and photorealistic visualizations with stunning detail and accuracy.\u003c/span\u003e\u003c/li\u003e\n\u003cli role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eVirtual workstations\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: Fuel digital twins, simulation, and VFX workloads. The G4’s leap in capability is powered by full support for all NVIDIA DLSS 4 features, the latest NVENC/NVDEC encoders for video streaming and transcode, and fourth-generation RT Cores for real-time ray tracing.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGoogle Cloud scales NVIDIA RTX PRO 6000\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eModern generative AI models often exceed the VRAM of a single GPU, making you use multi-GPU configurations to serve these workloads. While this approach is common, performance can be bottlenecked by the communication speed between the AI architecture. We significantly boosted multi-GPU performance on G4 VMs by implementing an enhanced \u003c/span\u003e\u003ca href=\"https://cloud.google.com/blog/products/compute/g4-vms-p2p-fabric-boosts-multi-gpu-workloads\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003ePCIe-based P2P\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e data path that optimizes critical collective operations like All-Reduce, which is essential for splitting models across GPUs. \u003c/span\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThanks to the G4’s enhanced peer-to-peer capabilities, you can expect up to 168% throughput gains and 41% lower latency (inter-token latency) when using tensor parallelism for model serving compared to standard non-P2P offerings.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eFor your generative AI applications, this technical differentiation translates into:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eFaster user experience: Lower latency means quicker responses from your AI services, enabling more interactive and real-time applications.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eHigher scalability: Increased throughput allows you to serve more concurrent users from a single virtual machine, significantly improving the price-performance and scalability of your service.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGoogle Cloud services integrated with G4 VMs\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eG4 VMs are fully integrated with several Google Cloud services, accelerating your AI workloads from day one.\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGoogle Kubernetes Engine\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e (GKE): G4 GPUs are generally available through GKE. Since \u003c/span\u003e\u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/gke-autopilot-now-available-to-all-qualifying-clusters\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGKE recently extended Autopilot to all qualifying clusters\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, including GKE Standard clusters, you can benefit from GKE's container-optimized compute platform to rapidly scale your G4 GPUs, enabling you to optimize costs. By adding the \u003c/span\u003e\u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/concepts/about-gke-inference-gateway\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGKE Inference Gateway\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, you can stretch the benefits of G4 even further to achieve lower AI serving latency and higher throughput.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003ca href=\"https://cloud.google.com/vertex-ai/docs\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eVertex AI\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: Both inference and training benefit significantly from G4’s large GPU memory (96 GB per GPU, 768 GB total), native FP4 precision support, and global presence. \u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003ca href=\"https://cloud.google.com/dataproc?utm_source=google\u0026amp;utm_medium=cpc\u0026amp;utm_campaign=na-US-all-en-dr-bkws-all-all-trial-e-dr-1710134\u0026amp;utm_content=text-ad-none-any-DEV_c-CRE_772298885641-ADGP_Hybrid+%7C+BKWS+-+EXA+%7C+Txt-Data+Analytics-Data+Analytics-Dataproc-KWID_313714756996-kwd-313714756996\u0026amp;utm_term=KW_google+cloud+dataproc-ST_google+cloud+dataproc\u0026amp;gclsrc=aw.ds\u0026amp;gad_source=1\u0026amp;gad_campaignid=22970352687\u0026amp;gclid=Cj0KCQjw0Y3HBhCxARIsAN7931XkrLeikr7M72cW92ZjX_eyEI_Jz7qT0tJkivNhg1R2QYHsnWmSjUsaAsa2EALw_wcB\u0026amp;hl=en\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eDataproc\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: G4 VMs are fully supported on the Dataproc managed analytics platform, letting you accelerate large-scale Spark and Hadoop workloads. This enables data scientists and data engineers to significantly boost performance for machine learning and large-scale data processing workloads.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003ca href=\"https://cloud.google.com/run?hl=en\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eCloud Run\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: We’ve extended our serverless platform’s AI infrastructure options to include the NVIDIA RTX PRO 6000, so you can perform \u003c/span\u003e\u003cspan style=\"vertical-align: baseline;\"\u003ereal-time AI inference with your preferred LLMs or media rendering \u003c/span\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eusing\u003c/span\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e fully managed, simple, pay-per-use GPUs.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/hyperdisk-ml\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eHyperdisk ML\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, \u003c/span\u003e\u003ca href=\"https://cloud.google.com/products/managed-lustre?hl=en\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eManaged Lustre\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, and \u003c/span\u003e\u003ca href=\"https://cloud.google.com/storage?hl=en\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eCloud Storage\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: When you need to expand beyond local storage for your HPC and large scale AI/ML workloads, you can connect G4 to a variety of Google Cloud storage services. For low latency and up to 500K of IO per instance, Hyperdisk ML is a great option. For high-performance file storage in the same zone, Managed Lustre offers a parallel file system ideal for persistent storage, up to 1TB/s. Finally, if you need nearly unlimited global capacity, with powerful capabilities like \u003c/span\u003e\u003ca href=\"https://cloud.google.com/storage/docs/anywhere-cache\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eAnywhere Cache\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e for use cases like inference, choose Cloud Storage as your primary, highly available, and globally scalable storage platform for training datasets, model artifacts, and feature stores.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eWhat customers are saying\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eHere’s how customers are using G4 to innovate and accelerate within their businesses:\u003c/span\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-style: italic; vertical-align: baseline;\"\u003e“The combination of NVIDIA Omniverse on Google Cloud G4 VMs is the true engine for our creative transformation. It empowers our teams to compress weeks of traditional production into hours, allowing us to instantly generate photorealistic 3D advertising environments at a global scale while ensuring pixel-perfect brand compliance—a capability that redefines speed and personalization in digital marketing.”\u003c/span\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e -\u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003e \u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003ePerry Nightingale, SVP Creative AI, WPP\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-video\"\u003e\n\n\n\n\u003cdiv class=\"article-module article-video \"\u003e\n  \u003cfigure\u003e\n    \u003ca class=\"h-c-video h-c-video--marquee\"\n      href=\"https://youtube.com/watch?v=bRFB8PoAQAc\"\n      data-glue-modal-trigger=\"uni-modal-bRFB8PoAQAc-\"\n      data-glue-modal-disabled-on-mobile=\"true\"\u003e\n\n      \n        \n\n        \u003cdiv class=\"article-video__aspect-image\"\n          style=\"background-image: url(https://storage.googleapis.com/gweb-cloudblog-publish/images/maxresdefault_OSveJmK.max-1000x1000.jpg);\"\u003e\n          \u003cspan class=\"h-u-visually-hidden\"\u003eWPP GCP G4\u003c/span\u003e\n        \u003c/div\u003e\n      \n      \u003csvg role=\"img\" class=\"h-c-video__play h-c-icon h-c-icon--color-white\"\u003e\n        \u003cuse xlink:href=\"#mi-youtube-icon\"\u003e\u003c/use\u003e\n      \u003c/svg\u003e\n    \u003c/a\u003e\n\n    \n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\u003cdiv class=\"h-c-modal--video\"\n     data-glue-modal=\"uni-modal-bRFB8PoAQAc-\"\n     data-glue-modal-close-label=\"Close Dialog\"\u003e\n   \u003ca class=\"glue-yt-video\"\n      data-glue-yt-video-autoplay=\"true\"\n      data-glue-yt-video-height=\"99%\"\n      data-glue-yt-video-vid=\"bRFB8PoAQAc\"\n      data-glue-yt-video-width=\"100%\"\n      href=\"https://youtube.com/watch?v=bRFB8PoAQAc\"\n      ng-cloak\u003e\n   \u003c/a\u003e\n\u003c/div\u003e\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-style: italic; vertical-align: baseline;\"\u003e\"We’re excited to bring the power of Google Cloud G4 VMs into Altair One, so you can run your most demanding simulation and fluid dynamics workloads with the speed, scale, and visual fidelity needed to push innovation further.\" - \u003c/span\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eYeshwant Mummaneni, \u003c/span\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eChief Engineer – Analytics, HPC, IoT \u0026amp; Digital Twin, Altair\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eThe Google Cloud advantage\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eChoosing Google Cloud means selecting a platform engineered for tangible results. The new G4 VM is a prime example, with our custom P2P interconnect unlocking up to 168% more throughput from the underlying NVIDIA RTX PRO 6000 Blackwell GPUs. This focus on optimized performance extends across our comprehensive portfolio; the G4 perfectly complements our existing A-Series and G2 GPUs, ensuring you have the ideal infrastructure for any workload. Beyond raw performance, we deliver turnkey solutions to accelerate your time to value. With NVIDIA Omniverse now available on the Google Cloud Marketplace, you can immediately deploy enterprise-grade digital twin and simulation applications on a fully managed and scalable platform.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eG4 capacity is immediately available. To get started, simply select \u003c/span\u003e\u003ca href=\"https://cloud.google.com/compute/docs/accelerator-optimized-machines#g4-series\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eG4 VMs\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e from the Google Cloud console. \u003c/span\u003e\u003ca href=\"https://console.cloud.google.com/marketplace/browse?pli=1\u0026amp;q=omniverse\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eNVIDIA Omniverse and Isaac Sim\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e are qualified \u003c/span\u003e\u003ca href=\"https://console.cloud.google.com/marketplace/browse?q=Nvidia%20omniverse\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGoogle Cloud Marketplace\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e solutions that can draw down on your Google Cloud commitments; for more information, please contact your Google Cloud sales team or reseller.\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e"
    },
    {
      "title": "G4 VMs under the hood: A custom, high-performance P2P fabric for multi-GPU workloads",
      "link": "https://cloud.google.com/blog/products/compute/g4-vms-p2p-fabric-boosts-multi-gpu-workloads/",
      "source": "AI \u0026 Machine Learning",
      "category": "tech",
      "publishedAt": "2025-10-20T16:00:00Z",
      "description": "\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eToday, we announced the \u003c/span\u003e\u003ca href=\"https://cloud.google.com/blog/products/compute/g4-vms-powered-by-nvidia-rtx-6000-blackwell-gpus-are-ga\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003egeneral availability of the G4 VM family\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e based on NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs. Thanks to unique platform optimizations only available in Google Cloud, G4 VMs deliver the best performance of any commercially available NVIDIA RTX PRO 6000 Blackwell GPU offering for inference and fine-tuning on a wide range of models, from less than 30B to over 100B parameters. In this blog, we discuss the need for these platform optimizations, how they work, and how to use them in your own environment. \u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eCollective communications performance matters \u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eLarge language models (LLMs) vary significantly in size, as characterized by their number of parameters: small (~7B), medium (~70B), and large (~350B+). LLMs often exceed the memory capacity of a single GPU, including the NVIDIA RTX PRO 6000 Blackwell’s, with its 96GB of GDDR7 memory. A common solution is to use tensor parallelism, or TP, which works by distributing individual model layers across multiple GPUs. This involves partitioning a layer's weight matrices, allowing each GPU to perform a partial computation in parallel. However, a significant performance bottleneck arises from the subsequent need to combine these partial results using collective communication operations like All-Gather or All-Reduce.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe G4 family of GPU virtual machines utilizes a PCIe-only interconnect. We drew on our extensive infrastructure expertise to develop this high-performance, software-defined PCIe fabric that supports peer-to-peer (P2P) communication. Crucially, G4’s platform-level P2P optimization substantially accelerates collective communications for workloads that require multi-GPU scaling, resulting in a notable boost for both inference and fine-tuning of LLMs.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eHow G4 accelerates multi-GPU performance\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eMulti-GPU G4 VM shapes get their significantly enhanced PCIe P2P capabilities from a combination of both custom hardware and software. This advancement directly optimizes collective communications, including All-to-All, All-Reduce, and All-Gather collectives for managing GPU data exchange. The result is a low-latency data path that delivers a substantial performance increase for critical workloads like multi-GPU inference and fine-tuning.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eIn fact, across all major collectives, the enhanced G4 P2P capability provides an acceleration of \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eup to 2.2x\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e without requiring any changes to the code or workload.\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-image_full_width\"\u003e\n\n\n\n\n\n\n  \n    \u003cdiv class=\"article-module h-c-page\"\u003e\n      \u003cdiv class=\"h-c-grid\"\u003e\n  \n\n    \u003cfigure class=\"article-image--large\n      \n      \n        h-c-grid__col\n        h-c-grid__col--6 h-c-grid__col--offset-3\n        \n        \n      \"\n      \u003e\n\n      \n      \n        \n        \u003cimg\n            src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/01_collective_communications.max-1000x1000.jpg\"\n        \n          alt=\"01_collective_communications\"\u003e\n        \n        \u003c/a\u003e\n      \n    \u003c/figure\u003e\n\n  \n      \u003c/div\u003e\n    \u003c/div\u003e\n  \n\n\n\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eInference performance boost by P2P on G4\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eOn G4 instances, enhanced peer-to-peer communication directly boosts multi-GPU workload performance, particularly for tensor parallel inference with vLLM, with up to \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003e168% higher throughput\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, and up to \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003e41% lower inter-token latency \u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e(ITL).\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eWe observe these improvements when using tensor parallelism for model serving, especially when compared to standard non-P2P offerings.\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-image_full_width\"\u003e\n\n\n\n\n\n\n  \n    \u003cdiv class=\"article-module h-c-page\"\u003e\n      \u003cdiv class=\"h-c-grid\"\u003e\n  \n\n    \u003cfigure class=\"article-image--large\n      \n      \n        h-c-grid__col\n        h-c-grid__col--6 h-c-grid__col--offset-3\n        \n        \n      \"\n      \u003e\n\n      \n      \n        \n        \u003cimg\n            src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/02_throughput.max-1000x1000.jpg\"\n        \n          alt=\"02_throughput\"\u003e\n        \n        \u003c/a\u003e\n      \n    \u003c/figure\u003e\n\n  \n      \u003c/div\u003e\n    \u003c/div\u003e\n  \n\n\n\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eAt the same time, G4 coupled with software-defined PCIe and P2P innovation, significantly enhances inference throughput and reduces latency, giving you the control to optimize your inference deployment for your business needs.\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-image_full_width\"\u003e\n\n\n\n\n\n\n  \n    \u003cdiv class=\"article-module h-c-page\"\u003e\n      \u003cdiv class=\"h-c-grid\"\u003e\n  \n\n    \u003cfigure class=\"article-image--large\n      \n      \n        h-c-grid__col\n        h-c-grid__col--6 h-c-grid__col--offset-3\n        \n        \n      \"\n      \u003e\n\n      \n      \n        \n        \u003cimg\n            src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/03_latency.max-1000x1000.jpg\"\n        \n          alt=\"03_latency\"\u003e\n        \n        \u003c/a\u003e\n      \n    \u003c/figure\u003e\n\n  \n      \u003c/div\u003e\n    \u003c/div\u003e\n  \n\n\n\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eThroughput or speed: G4 with P2P lets you choose\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe platform-level optimizations on G4 VMs translate directly into a flexible and powerful competitive advantage. For interactive generative AI applications, where user experience is paramount, G4’s P2P technology delivers up to 41% less inter-token latency — the critical delay between generating each part of a response. This results in a noticeably snappier and more reactive end-user experience, increasing their satisfaction with your AI application.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eAlternatively, for workloads where raw throughput is the priority, such as batch inference, G4 with P2P enables customers to serve up to 168% more requests than comparable offerings. This means you can either increase the number of users served by each model instance, or significantly improve the responsiveness of your AI applications. Whether your focus is on latency-sensitive interactions or high-volume throughput, G4 provides a superior return on investment compared to other NVIDIA RTX PRO 6000 offerings in the market.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eScale further with G4 and GKE Inference Gateway\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eWhile P2P optimizes performance for a single model replica, scaling to meet production demand often requires multiple replicas. This is where the \u003c/span\u003e\u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/concepts/about-gke-inference-gateway\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGKE Inference Gateway\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e really shines. It acts as an intelligent traffic manager for your models, using advanced features like prefix-cache-aware routing and custom scheduling to maximize throughput and slash latency across your entire deployment.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eBy combining the vertical scaling of G4's P2P with the horizontal scaling of the Inference Gateway, you can build an end-to-end serving solution that is exceptionally performant and cost-effective for the most demanding generative AI applications. For instance, you can use G4's P2P to efficiently run a 2-GPU Llama-3.1-70B model replica with 66% higher throughput, and then use GKE Inference Gateway to intelligently manage and autoscale multiple of these replicas to meet global user demand.\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-image_full_width\"\u003e\n\n\n\n\n\n\n  \n    \u003cdiv class=\"article-module h-c-page\"\u003e\n      \u003cdiv class=\"h-c-grid\"\u003e\n  \n\n    \u003cfigure class=\"article-image--large\n      \n      \n        h-c-grid__col\n        h-c-grid__col--6 h-c-grid__col--offset-3\n        \n        \n      \"\n      \u003e\n\n      \n      \n        \n        \u003cimg\n            src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/04_inference_gateway.max-1000x1000.jpg\"\n        \n          alt=\"04_inference_gateway\"\u003e\n        \n        \u003c/a\u003e\n      \n    \u003c/figure\u003e\n\n  \n      \u003c/div\u003e\n    \u003c/div\u003e\n  \n\n\n\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eG4 P2P supported VM Shapes\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003ePeer-to-peer capabilities for NVIDIA RTX PRO 6000 Blackwell are available with the following multi-GPU G4 VM shapes:\u003c/span\u003e\u003c/p\u003e\n\u003cdiv align=\"left\"\u003e\n\u003cdiv style=\"color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;\"\u003e\n\u003cdiv style=\"color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;\"\u003e\n\u003cdiv style=\"color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;\"\u003e\n\u003cdiv style=\"color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;\"\u003e\n\u003cdiv style=\"color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;\"\u003e\u003ctable\u003e\u003ccolgroup\u003e\u003ccol/\u003e\u003ccol/\u003e\u003ccol/\u003e\u003ccol/\u003e\u003ccol/\u003e\u003ccol/\u003e\u003ccol/\u003e\u003c/colgroup\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"vertical-align: top; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eMachine Type\u003c/strong\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: top; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGPUs\u003c/strong\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: top; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003ePeer-to-Peer\u003c/strong\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: top; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGPU Memory (GB)\u003c/strong\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: top; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003evCPUs\u003c/strong\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: top; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eHost Memory (GB)\u003c/strong\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: top; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eLocal SSD (GB)\u003c/strong\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eg4-standard-96\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e2\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eYes\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e192\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e96\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e360\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e3,000\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eg4-standard-192\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e4\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eYes\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e384\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e192\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e720\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e6,000\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eg4-standard-384\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e8\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eYes\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e768\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e384\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e1,440\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e12,000\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eFor VM shapes smaller than 8 GPUs, our software defined PCIe fabric ensures path isolation between GPUs assigned to different VMs on the same physical machine. PCIe paths are created dynamically at VM creation and are dependent on the VM shape, ensuring isolation on multiple levels of the platform stack to prevent communication between GPUs that are not assigned to the same VM.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGet started with P2P on G4\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe G4 peer-to-peer capability is transparent to the workload, and requires no changes to the application code or to libraries such as the \u003c/span\u003e\u003ca href=\"https://developer.nvidia.com/nccl\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eNVIDIA Collective Communications Library\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e (NCCL). All peer-to-peer paths are automatically set up during VM creation. You can find more information about enabling peer-to-peer for NCCL-based workloads in the \u003c/span\u003e\u003ca href=\"https://cloud.google.com/compute/docs/accelerator-optimized-machines?hl=en#g4-gpu-p2p\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eG4 documentation\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eTry \u003c/span\u003e\u003ca href=\"https://cloud.google.com/compute/docs/accelerator-optimized-machines#g4-series\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGoogle Cloud G4 VMs\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e with P2P from the Google Cloud console today, and start building your inference platform with GKE Inference Gateway. For more information, please contact your Google Cloud sales team or reseller.\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e"
    },
    {
      "title": "Google named a Leader in the 2025 IDC MarketScape for Worldwide GenAI Life-Cycle Foundation Model Software",
      "link": "https://cloud.google.com/blog/products/ai-machine-learning/google-named-a-leader-in-the-2025-idc-marketscape/",
      "source": "AI \u0026 Machine Learning",
      "category": "tech",
      "publishedAt": "2025-10-20T16:00:00Z",
      "description": "\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eUnlocking real value with AI in the enterprise calls for more than just intelligence. It requires a seamless, end-to-end platform where your model and operational controls are fully integrated. This is the core of our strategy at Google Cloud: combining the most powerful models with the scale and security required for production.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eToday, we are excited to announce that Google has been recognized as a Leader for our Gemini model family in the 2025 IDC MarketScape for Worldwide GenAI Life-Cycle Foundation Model Software (doc # US53007225, October 2025) report.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eWe believe the result validates our multi-year commitment to building the most capable, multimodal AI and delivering it to the enterprise through the Vertex AI platform. It is this combined approach that leads organizations, from innovative startups to the most demanding enterprises, to choose Google Cloud for their critical generative AI deployments.\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-image_full_width\"\u003e\n\n\n\n\n\n\n  \n    \u003cdiv class=\"article-module h-c-page\"\u003e\n      \u003cdiv class=\"h-c-grid\"\u003e\n  \n\n    \u003cfigure class=\"article-image--large\n      \n      \n        h-c-grid__col\n        h-c-grid__col--6 h-c-grid__col--offset-3\n        \n        \n      \"\n      \u003e\n\n      \n      \n        \n        \u003cimg\n            src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/image1_mw7Unqg.max-1000x1000.png\"\n        \n          alt=\"image1\"\u003e\n        \n        \u003c/a\u003e\n      \n        \u003cfigcaption class=\"article-image__caption \"\u003e\u003cp data-block-key=\"378zs\"\u003eSource: \"IDC MarketScape: Worldwide GenAI Life-Cycle Foundation Model Software 2025 Vendor Assessment,\" Doc. #US53007225\u003c/p\u003e\u003c/figcaption\u003e\n      \n    \u003c/figure\u003e\n\n  \n      \u003c/div\u003e\n    \u003c/div\u003e\n  \n\n\n\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGemini 2.5: adaptive thinking and cost control\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eFor companies moving AI workloads into production, the focus quickly shifts from raw intelligence to optimization, speed, and cost control. That’s why in August, we announced General Availability (GA) of the Gemini 2.5 model family, dramatically increasing both intelligence and enterprise readiness. Our pace of innovation hasn’t slowed; we quickly followed up in September with an \u003c/span\u003e\u003ca href=\"https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eimproved Gemini 2.5 Flash and Flash-Lite\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e release.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eGemini 2.5 models are thinking models, meaning they can perform complex, internal reasoning to solve multi-step problems with better accuracy. This advanced capability addresses the need for depth of reasoning while still offering tools to manage compute costs:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eThinking budgets\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e:\u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003e \u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eWe introduced thinking budgets for models like Gemini 2.5 Flash and Gemini 2.5 Flash-Lite. Developers can now set a maximum computational effort, allowing for fine-grained control over cost and latency. You get the full power of a thinking model when the task demands it, and maximum speed for high-volume, low-latency tasks.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eThought summaries:\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e Developers also gain transparency with thought summaries in the API and Vertex AI, providing a clear, structured view of the model's reasoning process. This is essential for auditability.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eModel choice and flexibility\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eBy providing an open ecosystem of multimodal models, enterprises can choose to deploy the best model for any task, and the right modality for any use case.\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eVertex AI Model Garden\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e ensures you always have access to the latest intelligence. This includes our first-party models, leading open source options, and powerful third-party models like Anthropic’s Claude Sonnet 4.5, which we made available upon its release. This empowers you to pick the right tool for every use case.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eNative multimodality:\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e Gemini’s core strength is its native multimodal capability, or the ability to understand and combine information across text, code, images, and audio.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eCreative control with Nano Banana: \u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eNano Banana (Gemini 2.5 Flash Image) provides creators and developers sharp control for visual tasks, enabling conversational editing and maintaining character and product consistency across multiple generations. \u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eBuilding AI agents: Code, speed, and the CLI\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eTo accelerate the transition to AI agents that can execute complex tasks, we prioritized investment in coding performance and tooling for developers:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eCoding performance leap:\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e Gemini 2.5 Pro now excels at complex code generation and problem-solving, offering developers a dramatically improved resource for high-quality software development.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eAgentic developer tools:\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e The launch of the Gemini Command Line Interface (CLI) brings powerful, agentic problem-solving directly to the terminal. This provides developers with the kind of immediate, interactive coding assistance necessary to close gaps and accelerate development velocity.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eUnlocking value with Vertex AI\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eIn addition to powerful models, organizations need a managed, governed platform to move AI projects from pilot to production and achieve real business value. That’s why \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eVertex AI\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e is the critical component for enterprise AI workloads.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eVertex AI provides the secure, end-to-end environment that transforms Gemini’s intelligence into a scalable business solution. It is the single place for developers to manage the full AI lifecycle, allowing companies to stop managing infrastructure and start building innovative agentic AI applications.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eWe focus on three core pillars: \u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eCustomization for differentiation:\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e Tailor model behavior using techniques like Supervised Fine-Tuning (SFT) to embed your unique domain expertise directly into the model's knowledge.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGrounding for accuracy: \u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eEasily connect Gemini to your enterprise data – whether structured data in BigQuery, internal documents via Vertex AI Search, or web data from Google Search or Google Maps – to ensure model responses are accurate, relevant, and trusted.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eSecurity, governance, and compliance:\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e Maintain control over data and models with enterprise-grade security, governance, and data privacy controls built directly into the platform, ensuring stability and protection for your mission-critical applications.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGet started today\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"http://cloud.google.com/resources/content/idc-marketscape-2025-ww-foundation-models\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eDownload\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e the \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003e2025 IDC MarketScape for Worldwide GenAI Life-Cycle Foundation Model Software\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e excerpt to learn why organizations are choosing Google Cloud.\u003c/span\u003e\u003c/p\u003e\n\u003chr/\u003e\n\u003cp\u003e\u003csup\u003e\u003cspan style=\"font-style: italic; vertical-align: baseline;\"\u003eIDC MarketScape vendor analysis model is designed to provide an overview of the competitive fitness of technology and suppliers in a given market. The research methodology utilizes a rigorous scoring methodology based on both qualitative and quantitative criteria that results in a single graphical illustration of each supplier’s position within a given market. The Capabilities score measures supplier product, go-to-market and business execution in the short-term. The Strategy score measures alignment of supplier strategies with customer requirements in a 3-5-year timeframe. Supplier market share is represented by the size of the icons.\u003c/span\u003e\u003c/sup\u003e\u003c/p\u003e\u003c/div\u003e"
    },
    {
      "title": "Building scalable AI agents: Design patterns with Agent Engine on Google Cloud",
      "link": "https://cloud.google.com/blog/topics/partners/building-scalable-ai-agents-design-patterns-with-agent-engine-on-google-cloud/",
      "source": "AI \u0026 Machine Learning",
      "category": "tech",
      "publishedAt": "2025-10-20T16:00:00Z",
      "description": "\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eAI Agents are now a reality, moving beyond chatbots to understand intent, collaborate, and execute complex workflows. This leads to increased efficiency, lower costs, and improved customer and employee experiences. This is a key opportunity for System Integrator (SI) Partners to deliver Google Cloud’s advanced AI to more customers. This post details how to build, scale, and manage enterprise-grade agentic systems using Google Cloud AI products to enable SI Partners to offer these transformative solutions to enterprise clients.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eEnterprise challenges\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe limitations of traditional, rule-based automation are becoming increasingly apparent in the face of today’s complex business challenges. Its inherent rigidity often leads to protracted approval processes, outdated risk models, and a critical lack of agility, thereby impeding the ability to seize new opportunities and respond effectively to operational demands.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eModern enterprises are further compounded by fragmented IT landscapes, characterized by legacy systems and siloed data, which collectively hinder seamless integration and scalable growth. Furthermore, static systems are ill-equipped to adapt instantaneously to market volatility or unforeseen \"black swan\" events. They also fall short in delivering the personalization and operational optimization required to manage escalating complexity—such as in cybersecurity and resource allocation—at scale. In this dynamic environment, AI agents offer the necessary paradigm shift to overcome these persistent limitations.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eHow SI Partners are solving business challenges with AI agents\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eLet's discuss how SIs are working with Google Cloud to solve some of the discussed business challenges;\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eDeloitte: \u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eA major retail client sought to enhance inventory accuracy and streamline reconciliation across its diverse store locations. The client needed various users—Merchants, Supply Chain, Marketing, and Inventory Controls—to interact with inventory data through natural language prompts. This interaction would enable them to check inventory levels, detect anomalies, research reconciliation data, and execute automated actions.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eDeloitte leveraged Google Cloud AI Agents and \u003c/span\u003e\u003ca href=\"https://cloud.google.com/blog/products/ai-machine-learning/introducing-gemini-enterprise?e=48754805\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGemini Enterprise\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e  to create a solution that generates insights, identifies discrepancies, and offers actionable recommendations based on inventory data. This solution utilizes Agentic AI to integrate disparate data sources and deliver real-time recommendations, ultimately aiming to foster trust and confidence in the underlying inventory data.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eQuantiphi\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: To improve customer experience and optimize sales operations, a furniture manufacturer partnered with Quantiphi to deploy Generative AI. to create a dynamic intelligent assistant on Google Cloud. The multi-agent system automates the process of quotation response creation thereby accelerating and speeding the process. At its core is an orchestrator, built with Agent Development Kit (ADK) and an Agent to Agent (A2A) framework that seamlessly coordinates between agents to summarize the right response - whether you're researching market trends, asking about product details, or analyzing sales data. Leveraging the cutting-edge capabilities of Google Cloud’s Gemini models and BigQuery, the assistant delivers unparalleled insights, transforming how one can access data and make decisions. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThese examples represent just a fraction of the numerous use cases spanning diverse industry verticals, including healthcare, manufacturing, and financial services, that are being deployed in the field by SIs working in close collaboration with Google Cloud.\u003c/span\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e \u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eArchitecture and design patterns used by SIs\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe strong partnership between Google Cloud and SIs is instrumental in delivering true business value to customers. Let's examine the scalable architecture patterns employed by Google Cloud SIs in the field to tackle Agentic AI challenges.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eTo comprehend Agentic AI architectures, it's crucial to first understand what an AI agent is. An AI agent is a software entity endowed with the capacity to plan, reason, and execute complex actions for users with minimal human intervention. AI agents leverage advanced AI models for reasoning and informed decision-making, while utilizing tools to fetch data from external sources for real-time and grounded information. Agents typically operate within a compute runtime. The visual diagram  illustrates the basic components of an agent;\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-image_full_width\"\u003e\n\n\n\n\n\n\n  \n    \u003cdiv class=\"article-module h-c-page\"\u003e\n      \u003cdiv class=\"h-c-grid\"\u003e\n  \n\n    \u003cfigure class=\"article-image--large\n      \n      \n        h-c-grid__col\n        h-c-grid__col--6 h-c-grid__col--offset-3\n        \n        \n      \"\n      \u003e\n\n      \n      \n        \n        \u003cimg\n            src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/image2_0O4Fuj5.png\"\n        \n          alt=\"1-Base_AI_Agent_Components\"\u003e\n        \n        \u003c/a\u003e\n      \n        \u003cfigcaption class=\"article-image__caption \"\u003e\u003cp data-block-key=\"mlmk8\"\u003eBase AI Agent Components\u003c/p\u003e\u003c/figcaption\u003e\n      \n    \u003c/figure\u003e\n\n  \n      \u003c/div\u003e\n    \u003c/div\u003e\n  \n\n\n\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe snippet below also demonstrates how an Agent's code appears in the Python programming language;\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-image_full_width\"\u003e\n\n\n\n\n\n\n  \n    \u003cdiv class=\"article-module h-c-page\"\u003e\n      \u003cdiv class=\"h-c-grid\"\u003e\n  \n\n    \u003cfigure class=\"article-image--large\n      \n      \n        h-c-grid__col\n        h-c-grid__col--6 h-c-grid__col--offset-3\n        \n        \n      \"\n      \u003e\n\n      \n      \n        \n        \u003cimg\n            src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/image1_NzKVIVZ.max-1000x1000.png\"\n        \n          alt=\"image1\"\u003e\n        \n        \u003c/a\u003e\n      \n        \u003cfigcaption class=\"article-image__caption \"\u003e\u003cp data-block-key=\"mlmk8\"\u003eCode snippet of an AI Agent\u003c/p\u003e\u003c/figcaption\u003e\n      \n    \u003c/figure\u003e\n\n  \n      \u003c/div\u003e\n    \u003c/div\u003e\n  \n\n\n\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThis agent code snippet showcases the components depicted in the first diagram, where we observe the Agent with a \u003c/span\u003e\u003cstrong style=\"font-style: italic; vertical-align: baseline;\"\u003eName\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, \u003c/span\u003e\u003cstrong style=\"font-style: italic; vertical-align: baseline;\"\u003eLarge Language Model (LLM)\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, \u003c/span\u003e\u003cstrong style=\"font-style: italic; vertical-align: baseline;\"\u003eDescription, Instruction and Tools\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, all of which are utilized to enable the agent to perform its designated functions.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eTo build enterprise-grade agents at scale, several factors must be considered during their ground-up development. Google Cloud has collaborated closely with its Partner ecosystem to employ cutting-edge Google Cloud products to build scalable and enterprise-ready agents.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eA key consideration in agent development is the framework. Without it, developers would be compelled to build everything from scratch, including state management, tool handling, and workflow orchestration. This often results in systems that are complex, difficult to debug, insecure, and ultimately unscalable. \u003c/span\u003e\u003ca href=\"https://google.github.io/adk-docs/\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGoogle Cloud Agent Development Kit\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e (ADK) provides essential scaffolding, tools, and patterns for efficient and secure enterprise agent development at scale. It offers developers the flexibility to customize agents to suit nearly every applicable use case.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eAgent development with any framework, especially multi-agent architectures in enterprises, necessitates robust compute resources and scalable infrastructure. This includes strong security measures, comprehensive tracing, logging, and monitoring capabilities, as well as rigorous evaluation of the agent’s decisions and output.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eFurthermore, agents typically lack inherent memory, meaning they cannot recall past interactions or maintain context for effective operation. While frameworks like ADK offer ephemeral memory storage for agents, enterprise-grade agents demand persistent memory. This persistent memory is vital for equipping agents with the necessary context to enhance their performance and the quality of their output.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eGoogle Cloud’s \u003c/span\u003e\u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eVertex AI Agent Engine\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e provides a secure runtime for agents that manages their lifecycle, orchestrates tools, and drives reasoning. It features built-in security, observability, and critical building blocks such as a memory bank, session service, and sandbox. Agent Engine is accessible to SIs and customers on Google Cloud. Alternative options for running agents at scale include\u003c/span\u003e\u003ca href=\"https://cloud.google.com/run?utm_source=google\u0026amp;utm_medium=cpc\u0026amp;utm_campaign=na-US-all-en-dr-bkws-all-all-trial-e-dr-1710134\u0026amp;utm_content=text-ad-none-any-DEV_c-CRE_772382725898-ADGP_Hybrid+%7C+BKWS+-+EXA+%7C+Txt-AppMod-Serverless-Cloud+Run-KWID_353039629183-kwd-353039629183\u0026amp;utm_term=KW_cloud+run-ST_cloud+run\u0026amp;gclsrc=aw.ds\u0026amp;gad_source=1\u0026amp;gad_campaignid=22980675520\u0026amp;gclid=CjwKCAjwlt7GBhAvEiwAKal0ct7yGb6joV5NddBugRqG4z5nSXqF8svPROtSnu21ZL9YDjjAXB6F7BoCXF4QAvD_BwE\u0026amp;hl=en\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e \u003c/span\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eCloud Run\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e or\u003c/span\u003e\u003ca href=\"https://cloud.google.com/kubernetes-engine?hl=en\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e \u003c/span\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGKE\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eCustomers often opt for these alternatives when they already have existing investments in Cloud Run or GKE infrastructure on Google Cloud, or when they require configuration flexibility concerning compute, storage, and networking, as well as flexible cost management. However, when choosing Cloud Run or GKE, functions like memory and session management must be built and managed from the ground up.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://modelcontextprotocol.io/docs/getting-started/intro\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eModel Context Protocol\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e (MCP) is a crucial element for modern AI agent architectures. This open protocol standardizes how applications provide context to LLMs, thereby improving agent responses by connecting agents and underlying AI models to various data sources and tools. It's important to note that Agents also communicate with enterprise systems using APIs, which are referred to as Tools when employed with agents. MCP enables agents to access fresh external data.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eWhen developing enterprise agents at scale, it is recommended to deploy the MCP servers separately on a serverless platform like Cloud Run or GKE on Google Cloud, with agents running on Agent Engine configured as clients. The sample architecture illustrates the recommended deployment model for MCP integration with ADK agents;\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-image_full_width\"\u003e\n\n\n\n\n\n\n  \n    \u003cdiv class=\"article-module h-c-page\"\u003e\n      \u003cdiv class=\"h-c-grid\"\u003e\n  \n\n    \u003cfigure class=\"article-image--large\n      \n      \n        h-c-grid__col\n        h-c-grid__col--6 h-c-grid__col--offset-3\n        \n        \n      \"\n      \u003e\n\n      \n      \n        \n        \u003cimg\n            src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/image4_jzjerKh.png\"\n        \n          alt=\"2-AI_agent_tool_integration_with_MCP\"\u003e\n        \n        \u003c/a\u003e\n      \n        \u003cfigcaption class=\"article-image__caption \"\u003e\u003cp data-block-key=\"mlmk8\"\u003eAI agent tool integration with MCP\u003c/p\u003e\u003c/figcaption\u003e\n      \n    \u003c/figure\u003e\n\n  \n      \u003c/div\u003e\n    \u003c/div\u003e\n  \n\n\n\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe reference architecture demonstrates how\u003c/span\u003e \u003ca href=\"https://google.github.io/adk-docs/mcp/\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eADK built agents can integrate with MCP\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e to connect data sources and provide context to underlying LLM models. The MCP utilizes Get, Invoke, List, and Call functions to enable tools to connect agents to external data sources. In this scenario, the agent can interact with a Graph database through application APIs using MCP, allowing the agent and the underlying LLM to access up-to-date data for generating meaningful responses.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eFurthermore, when building multi-agent architectures that demand interoperability and communication among agents from different systems, a key consideration is how to facilitate Agent-to-Agent communication. This addresses complex use cases that require workflow execution across various agents from different domains. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eGoogle Cloud launched the\u003c/span\u003e \u003ca href=\"https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eAgent-to-Agent Protocol\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e (A2A) with native support within Agent Engine to tackle the challenge of inter-agent communication at scale. Learn how to implement A2A from this\u003c/span\u003e \u003ca href=\"https://discuss.google.dev/t/building-bridges-deploy-agents-with-a2a-on-vertex-ai-agent-engine/264044\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eblog\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eGoogle Cloud has collaborated with SIs on agentic architecture and design considerations to build multiple agents, assisting clients in addressing various use cases across industry domains such as Retail, Manufacturing, Healthcare, Automotive, and Financial Services. The reference architecture below consolidates these considerations.\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-image_full_width\"\u003e\n\n\n\n\n\n\n  \n    \u003cdiv class=\"article-module h-c-page\"\u003e\n      \u003cdiv class=\"h-c-grid\"\u003e\n  \n\n    \u003cfigure class=\"article-image--large\n      \n      \n        h-c-grid__col\n        h-c-grid__col--6 h-c-grid__col--offset-3\n        \n        \n      \"\n      \u003e\n\n      \n      \n        \n        \u003cimg\n            src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/image3_9qMCmk5.max-1000x1000.png\"\n        \n          alt=\"3-Reference architecture - Agentic_AI_system_with_ADK_MCP_A2A_and_Agent_Engine\"\u003e\n        \n        \u003c/a\u003e\n      \n        \u003cfigcaption class=\"article-image__caption \"\u003e\u003cp data-block-key=\"mlmk8\"\u003eReference architecture - Agentic AI system with ADK, MCP, A2A and Agent Engine\u003c/p\u003e\u003c/figcaption\u003e\n      \n    \u003c/figure\u003e\n\n  \n      \u003c/div\u003e\n    \u003c/div\u003e\n  \n\n\n\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThis reference architecture depicts an enterprise-grade Agent built on Google Cloud to address a supply chain use case. In this architecture, all agents are built with the ADK framework and deployed on Agent Engine. Agent Engine provides a secure compute runtime with authentication, context management using managed\u003c/span\u003e \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/sessions/overview\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003esessions\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e,\u003c/span\u003e\u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/memory-bank/overview\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e \u003c/span\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003ememory\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, and quality assurance through\u003c/span\u003e \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/example-store/overview\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eExample Store\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e and\u003c/span\u003e \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/evaluate\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eEvaluation Services\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, while also offering observability into the deployed agents. Agent Engine delivers all these features and many more as a managed service at scale on GCP. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThis architecture outlines an Agentic supply chain featuring an orchestration agent (Root) and three dedicated sub-agents: \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eTracking, Distributor, and Order Agents\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e. Each of these agents are powered by \u003c/span\u003e\u003ca href=\"https://ai.google.dev/gemini-api/docs/models\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGemini\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e. For optimal performance and tailored responses, especially in specific use cases, we recommend \u003c/span\u003e\u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003etuning\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e your model with domain-specific data before integration with an agent. Model tuning can also help optimize responses for conciseness, potentially leading to reduced token size and lower operational costs.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eFor instance, a user might send a request such as \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003e\"show me the inventory levels for men’s backpack.\" \u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eRoot agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e receives this request and is capable of routing it to the \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eOrder agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, which is responsible for inventory and order operations. This routing is seamless because the A2A protocol utilizes\u003c/span\u003e \u003ca href=\"https://a2a-protocol.org/dev/tutorials/python/3-agent-skills-and-card/\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eagent cards\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e to advertise the capabilities of each respective agent. A2A is\u003c/span\u003e \u003ca href=\"https://google.github.io/adk-docs/a2a/\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003econfigured\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e with a few steps as a wrapper for your agents for Agent Engine deployment.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eIn this example, inventory and order details are stored in\u003c/span\u003e \u003ca href=\"https://cloud.google.com/bigquery?utm_source=google\u0026amp;utm_medium=cpc\u0026amp;utm_campaign=na-US-all-en-dr-bkws-all-all-trial-e-dr-1710134\u0026amp;utm_content=text-ad-none-any-DEV_c-CRE_772298885512-ADGP_Hybrid+%7C+BKWS+-+EXA+%7C+Txt-Data+Analytics-Data+Analytics-BigQuery-KWID_47616965283-kwd-47616965283\u0026amp;utm_term=KW_bigquery-ST_bigquery\u0026amp;gclsrc=aw.ds\u0026amp;gad_source=1\u0026amp;gad_campaignid=22970352687\u0026amp;gclid=CjwKCAjwlt7GBhAvEiwAKal0cguWTbXzjpOrhDcy2MvwgIkyd7YGbTldV885duGilt-aWh2I5Q4U_RoC5sQQAvD_BwE\u0026amp;hl=en\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eBigQuery\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e. Therefore, the agent uses its tool configuration to leverage the MCP server to fetch the inventory details from the BigQuery data warehouse. The response is then returned to the underlying LLM, which generates a formatted natural language response and provides the inventory details for men’s backpacks to the \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eRoot agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e and subsequently to the user. Based on this response, the user can, for example, place an order to replenish the inventory. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eWhen such a request is made, the Root agent routes it to the \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eDistributor agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e. This agent possesses knowledge of all suppliers who provide stock to the business. Depending on the item being requested, the agent will use its tools to initiate an MCP server connection to the correct external API endpoints for the respective supplier to place the order. If the suppliers have agents configured, the A2A protocol can also be utilized to send the request to the supplier's agent for processing. Any acknowledgment of the order is then sent back to the \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eDistributor agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eIn this reference architecture, when the \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eDistributor agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e receives acknowledgment, A2A enables the agent to detect the presence of a \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eTracking agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e that monitors new orders until delivery. The \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eDistributor agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e will pass the order details to the \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eTracking agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e and also send updates back to the user. The \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eTracking agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e will then send order updates to the user via messaging, utilizing the public API endpoint of the supplier. This is merely one example of a workflow that could be built with this reference architecture.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThis modular architecture can be adapted to solve various use cases with Agentic AI built with ADK and deployed to Agent Engine.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe reference architecture allows this multi-agent system to be consumed via a chat interface through a website or a custom-built user interface. It is also possible to integrate this agentic AI architecture with Google Cloud Gemini Enterprise.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e Learn how enterprises can start by using Gemini Enterprise as the front door to Google Cloud AI from this \u003c/span\u003e\u003ca href=\"https://blog.google/products/google-cloud/gemini-enterprise-sundar-pichai/\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eblog\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e from Alphabet CEO Sundar Pichai. This approach helps enterprises to start small using low code out of the box agents. As they mature, they can now implement complex use cases with advanced high code AI agents using this reference archiecture . \u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGetting started\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThis blog post has explored the design patterns for building intelligent enterprise AI agents. For enterprise decision makers, use the \u003c/span\u003e\u003ca href=\"https://cloud.google.com/transform/5-elements-to-start-implementing-agentic-solutions-a-guide-for-leaders?e=13802955\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003e5 essential elements to start implementing agentic solutions\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e to help guide your visionary strategy and decision making when it comes to running enterprise agents at scale.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eWe encourage you to embark on this journey today by collaborating with \u003c/span\u003e\u003ca href=\"https://cloud.google.com/find-a-partner/\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGoogle Cloud Partner Ecosystem\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e to understand your enterprise landscape and identify complex use cases that can be effectively addressed with AI Agents. Utilize these design patterns as your guide and leverage the ADK to transform your enterprise use case into a powerful, scalable solution that delivers tangible business value on Agent Engine with Google Cloud.\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e"
    }
  ],
  "generatedAt": "2025-10-21T07:08:25.623690915+09:00"
}
