{
  "date": "2025-10-21",
  "preset": "general",
  "summary": "## ğŸŒŸ ì˜¤ëŠ˜ì˜ AI \u0026 Tech í•˜ì´ë¼ì´íŠ¸\n\nì˜¤ëŠ˜ ê¸°ìˆ  ì—…ê³„ì˜ ê°€ì¥ í° ì†Œì‹ì€ NVIDIAì™€ Google Cloudì˜ íŒŒíŠ¸ë„ˆì‹­ í™•ì¥ì…ë‹ˆë‹¤. [^34] ì´ì œ ëˆ„êµ¬ë‚˜ í´ë¼ìš°ë“œë¥¼ í†µí•´ ìµœì‹  AI í•˜ë“œì›¨ì–´ì™€ ë””ì§€í„¸ íŠ¸ìœˆ ê¸°ìˆ ì„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. [^34] ë™ì‹œì— DockerëŠ” ë³µì¡í•œ AI ëª¨ë¸ì„ ê°œì¸ ì»´í“¨í„°ì—ì„œ ì†ì‰½ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ë„êµ¬ë¥¼ ê³µê°œí•˜ë©°, AI ê¸°ìˆ ì˜ ëŒ€ì¤‘í™”ë¥¼ ì•ë‹¹ê¸°ê³  ìˆìŠµë‹ˆë‹¤. [^2]\n\n## ğŸ“Š ì£¼ìš” ë‰´ìŠ¤ ë¸Œë¦¬í•‘\n\n### ğŸ¢ ê¸°ì—… \u0026 ì‚°ì—… ë™í–¥\n\n- **NVIDIAì™€ Google Cloud, AI ë™ë§¹ ê°•í™”**: Google Cloudê°€ NVIDIAì˜ ìµœì‹  RTX PRO 6000 Blackwell GPUë¥¼ íƒ‘ì¬í•œ G4 ê°€ìƒë¨¸ì‹ (VM)ì˜ ì •ì‹ ì¶œì‹œë¥¼ ë°œí‘œí–ˆìŠµë‹ˆë‹¤. [^34][^36] ì´ë¡œì¨ ê¸°ì—…ë“¤ì€ ê³ ì„±ëŠ¥ AI ì¶”ë¡ ë¿ë§Œ ì•„ë‹ˆë¼, ì˜í™” ìˆ˜ì¤€ì˜ ê·¸ë˜í”½ ë° ì‹œë®¬ë ˆì´ì…˜ ì‘ì—…ì„ í´ë¼ìš°ë“œì—ì„œ ë”ìš± ì›í™œí•˜ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. [^34] ë˜í•œ, ì‚°ì—…ìš© ë””ì§€í„¸ íŠ¸ìœˆ í”Œë«í¼ì¸ NVIDIA Omniverseì™€ ë¡œë´‡ ì‹œë®¬ë ˆì´ì…˜ ë„êµ¬ Isaac Simì´ Google Cloud Marketplaceì— ë“±ë¡ë˜ì–´ ì œì¡°ì—…, ìë™ì°¨ ë“± ë‹¤ì–‘í•œ ì‚°ì—…ì˜ ë””ì§€í„¸ ì „í™˜ì´ ê°€ì†í™”ë  ì „ë§ì…ë‹ˆë‹¤. [^34]\n\n- **Anthropic, Claudeì˜ ì½”ë“œ ì‹¤í–‰ ëŠ¥ë ¥ ê°•í™”**: AI ëª¨ë¸ Claude ê°œë°œì‚¬ Anthropicì´ 'ì½”ë“œ ìƒŒë“œë°•ì‹±(Code Sandboxing)' ê¸°ëŠ¥ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤. [^25] ì´ëŠ” AIê°€ ì½”ë“œë¥¼ ì‹¤í–‰í•  ë•Œë§ˆë‹¤ ì‚¬ìš©ìì—ê²Œ ê¶Œí•œì„ ìš”ì²­í•˜ëŠ” ë²ˆê±°ë¡œì›€ì„ ì¤„ì´ê³ , ì§€ì •ëœ ì•ˆì „í•œ í™˜ê²½(ìƒŒë“œë°•ìŠ¤) ì•ˆì—ì„œ ììœ¨ì ìœ¼ë¡œ ì½”ë“œë¥¼ í…ŒìŠ¤íŠ¸í•˜ê³  ë””ë²„ê¹…í•˜ê²Œ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. [^25] ì´ë¥¼ í†µí•´ ê°œë°œìì˜ ì‘ì—… íš¨ìœ¨ì„ ë†’ì´ê³ , ë¯¼ê°í•œ ì‹œìŠ¤í…œ íŒŒì¼ ì ‘ê·¼ì„ ì›ì²œì ìœ¼ë¡œ ì°¨ë‹¨í•˜ì—¬ ë³´ì•ˆì„ í¬ê²Œ ê°•í™”í–ˆìŠµë‹ˆë‹¤. [^25]\n\n- **Google Gemini API, ì§€ë„ì™€ ê²°í•©**: ê°œë°œìë“¤ì€ ì´ì œ Gemini APIì—ì„œ Google Maps ë°ì´í„°ë¥¼ ì§ì ‘ í™œìš©í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. [^23] ì´ 'ê·¸ë¼ìš´ë”©(Grounding)' ê¸°ëŠ¥ì€ AIì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ 2ì–µ 5ì²œë§Œ ê°œê°€ ë„˜ëŠ” ì¥ì†Œì˜ ìµœì‹  ì •ë³´ì™€ ê²°í•©í•˜ì—¬, \"ì—¬ê¸°ì„œ 15ë¶„ ì•ˆì— ê±¸ì–´ê°ˆ ìˆ˜ ìˆëŠ” ìµœê³ ì˜ ì´íƒˆë¦¬ì•„ ì‹ë‹¹ ì¶”ì²œí•´ì¤˜\"ì™€ ê°™ì€ ìœ„ì¹˜ ê¸°ë°˜ ì§ˆë¬¸ì— í›¨ì”¬ ë” ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤. [^23]\n\n### ğŸ”¬ ê¸°ìˆ  í˜ì‹  \u0026 ì—°êµ¬\n\n- **Docker, ê°œì¸ PCì—ì„œ AI ëª¨ë¸ ì‹¤í–‰ ê°„ì†Œí™”**: Dockerê°€ 'Open WebUI'ë¼ëŠ” ìƒˆë¡œìš´ í™•ì¥ í”„ë¡œê·¸ë¨ì„ í†µí•´ ë¡œì»¬ AI ëª¨ë¸ ì‹¤í–‰ ë°©ì‹ì„ íšê¸°ì ìœ¼ë¡œ ê°œì„ í–ˆìŠµë‹ˆë‹¤. [^2] ì´ì œ Docker Desktop ì‚¬ìš©ìëŠ” í´ë¦­ ëª‡ ë²ˆë§Œìœ¼ë¡œ LLaMA 3, Mistral ê°™ì€ ê°•ë ¥í•œ ì–¸ì–´ ëª¨ë¸ì„ ê°œì¸ ì»´í“¨í„°ì— ì„¤ì¹˜í•˜ê³ , ChatGPTì²˜ëŸ¼ í¸ë¦¬í•œ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [^2] íŒŒì¼ ì—…ë¡œë“œ, ìŒì„± ì…ë ¥ ë“± ê³ ê¸‰ ê¸°ëŠ¥ë„ ì§€ì›ë˜ì–´, ì¸í„°ë„· ì—°ê²° ì—†ì´ ê°œì¸ ì •ë³´ ìœ ì¶œ ê±±ì • ì—†ì´ AI ë¹„ì„œë¥¼ ì‚¬ìš©í•˜ëŠ” ì‹œëŒ€ê°€ ì—´ë ¸ìŠµë‹ˆë‹¤. [^2]\n\n- **Log4Shell ì‚¬íƒœì˜ êµí›ˆ**: GitHubëŠ” ì¸í„°ë„· ì—­ì‚¬ìƒ ìµœì•…ì˜ ì·¨ì•½ì ìœ¼ë¡œ ê¼½íˆëŠ” 'Log4Shell' ì‚¬íƒœë¥¼ ë˜ëŒì•„ë³´ëŠ” ì‹¬ì¸µ ë¶„ì„ ê¸°ì‚¬ë¥¼ ê³µê°œí–ˆìŠµë‹ˆë‹¤. [^1] ì´ ì‚¬ê±´ì€ ì „ ì„¸ê³„ ìˆ˜ë§ì€ ì‹œìŠ¤í…œì´ ì†Œìˆ˜ì˜ ìì›ë´‰ì‚¬ ìœ ì§€ë³´ìˆ˜ ê°œë°œìì— ì˜ì¡´í•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ìƒíƒœê³„ì˜ ì·¨ì•½ì„±ì„ ë“œëŸ¬ëƒˆìœ¼ë©°, ë‹¨ìˆœí•œ ì½”ë“œ ë¬¸ì œê°€ ì•„ë‹Œ ì¸ì  ì§€ì›ê³¼ ë³´ì•ˆ êµìœ¡ì˜ ì¤‘ìš”ì„±ì„ ì¼ê¹¨ì›Œì£¼ì—ˆìŠµë‹ˆë‹¤. [^1]\n\n- **GNU Octave, ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ì‹¤í–‰**: ì´ì œ GNU Octaveë¥¼ JupyterLiteì™€ ê²°í•©í•˜ì—¬ ì–¸ì œ ì–´ë””ì„œë“  ì›¹ ë¸Œë¼ìš°ì €ë§Œìœ¼ë¡œ ì»´í“¨íŒ… ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. [^5] ë³„ë„ì˜ í”„ë¡œê·¸ë¨ ì„¤ì¹˜ ì—†ì´ ë³µì¡í•œ ê³„ì‚°ê³¼ ë°ì´í„° ë¶„ì„ì´ ê°€ëŠ¥í•´ì ¸ ì ‘ê·¼ì„±ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤. [^5]\n\n### ğŸŒ íŠ¸ë Œë“œ \u0026 ì¸ì‚¬ì´íŠ¸\n\n- **AIì˜ 'ì‚°ì—…í™”'ì™€ 'ë¯¼ì£¼í™”' ë™ì‹œ ì§„í–‰**: NVIDIAì™€ Googleì˜ í˜‘ë ¥ì€ AI ê¸°ìˆ ì„ íŠ¹ì • ì „ë¬¸ê°€ì˜ ì˜ì—­ì—ì„œ ë²—ì–´ë‚˜, ë‹¤ì–‘í•œ ì‚°ì—… í˜„ì¥ì— ì ìš©í•˜ëŠ” 'ì‚°ì—…í™”' ë‹¨ê³„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. [^34] ë™ì‹œì— Dockerì˜ ì‚¬ë¡€ëŠ” ê°•ë ¥í•œ AIë¥¼ ê°œì¸ ê°œë°œìë‚˜ ì¼ë°˜ ì‚¬ìš©ìë„ ì‰½ê²Œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” 'ë¯¼ì£¼í™”' íë¦„ì„ ëª…í™•íˆ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤. [^2] í´ë¼ìš°ë“œì™€ ê°œì¸ìš© ì»´í“¨í„° ì–‘ìª½ì—ì„œ AI ê¸°ìˆ ì˜ ì¥ë²½ì´ ë¹ ë¥´ê²Œ í—ˆë¬¼ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n\n- **íŒŒì¸íŠœë‹ì˜ ê·€í™˜**: ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„±ëŠ¥ì´ ìƒí–¥ í‰ì¤€í™”ë˜ë©´ì„œ, íŠ¹ì • ì‘ì—…ì— ëª¨ë¸ì„ ìµœì í™”í•˜ëŠ” 'íŒŒì¸íŠœë‹' ê¸°ìˆ ì´ ë‹¤ì‹œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. [^3] ë²”ìš© ëª¨ë¸ë§Œìœ¼ë¡œëŠ” í•´ê²°í•  ìˆ˜ ì—†ëŠ” ì „ë¬¸ ë¶„ì•¼ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ë” ì ì€ ë¹„ìš©ìœ¼ë¡œ ë†’ì€ íš¨ìœ¨ì„ ì–»ê¸° ìœ„í•œ ê¸°ì—…ë“¤ì˜ ë…¸ë ¥ì´ ì´ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤. [^3]\n\n## ğŸ’¡ ì˜¤ëŠ˜ì˜ í…Œì´í¬ì–´ì›¨ì´\n\n- **AI, ì´ì œ 'ì“°ëŠ”' ì‹œëŒ€ì—ì„œ 'ë§Œë“¤ê³  ì“°ëŠ”' ì‹œëŒ€ë¡œ**: í´ë¼ìš°ë“œì—ì„œëŠ” ìµœê³  ì‚¬ì–‘ì˜ AI ì¸í”„ë¼ë¥¼ ë¹Œë ¤ ì“¸ ìˆ˜ ìˆê³ , ë‚´ ì»´í“¨í„°ì—ì„œëŠ” Docker ê°™ì€ ë„êµ¬ë¡œ ë‚˜ë§Œì˜ AIë¥¼ ë§Œë“¤ ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ AIëŠ” ë‹¨ìˆœíˆ ì†Œë¹„í•˜ëŠ” ëŒ€ìƒì„ ë„˜ì–´, ìš°ë¦¬ ê°ìì˜ í•„ìš”ì— ë§ê²Œ ì§ì ‘ êµ¬ì„±í•˜ê³  í™œìš©í•˜ëŠ” ê°œì¸í™”ëœ ë„êµ¬ë¡œ ì§„í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n- **ì˜¤í”ˆì†ŒìŠ¤ ì—†ì´ëŠ” AIë„ ì—†ë‹¤**: Log4Shell ì‚¬íƒœê°€ ë³´ì—¬ì£¼ë“¯, í™”ë ¤í•œ AI ê¸°ìˆ ì˜ ê¸°ë°˜ì—ëŠ” ì´ë¦„ ì—†ëŠ” ê°œë°œìë“¤ì´ ê´€ë¦¬í•˜ëŠ” ìˆ˜ë§ì€ ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ê°€ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ê¸°ìˆ ì˜ ì•ˆì „ê³¼ ì§€ì†ê°€ëŠ¥ì„±ì€ ê²°êµ­ ì´ë“¤ì— ëŒ€í•œ ì§€ì›ê³¼ ê±´ê°•í•œ ìƒíƒœê³„ ì¡°ì„±ì— ë‹¬ë ¤ìˆìŒì„ ê¸°ì–µí•´ì•¼ í•©ë‹ˆë‹¤.\n\n[^1]: Inside the breach that broke the internet: The untold story of Log4Shell - https://github.blog/open-source/inside-the-breach-that-broke-the-internet-the-untold-story-of-log4shell/\n[^2]: Docker Model Runner Meets Open WebUI: A Simpler Way to Run Local AI Models - https://www.docker.com/blog/open-webui-docker-desktop-model-runner/\n[^3]: íŒŒì¸íŠœë‹ì˜ ê·€í™˜ì— ëŒ€í•œ ì‚¬ë¡€ - https://news.hada.io/topic?id=23793\n[^5]: GNU Octaveê°€ JupyterLiteì™€ ë§Œë‚¨: ì–¸ì œ ì–´ë””ì„œë‚˜ ì»´í“¨íŒ… ê°€ëŠ¥í•¨ - https://news.hada.io/topic?id=23791\n[^23]: Grounding with Google Maps: Now Available in the Gemini API - https://blog.google/technology/developers/grounding-google-maps-gemini-api/\n[^25]: Claude Code Sandboxing - https://www.anthropic.com/engineering/claude-code-sandboxing\n[^34]: NVIDIA and Google Cloud Accelerate Enterprise AI and Industrial Digitalization - https://blogs.nvidia.com/blog/nvidia-google-cloud-enterprise-ai-industrial-digitalization/\n[^36]: The G4 VM is GA: Expanding our NVIDIA GPU portfolio for visual computing and AI - https://cloud.google.com/blog/products/compute/g4-vms-powered-by-nvidia-rtx-6000-blackwell-gpus-are-ga/",
  "systemPrompt": "ë‹¹ì‹ ì€ ë§¤ì¼ ì•„ì¹¨ ê¸°ìˆ  ë‰´ìŠ¤ë¥¼ ì •ë¦¬í•´ì£¼ëŠ” ì¹œì ˆí•œ AI íë ˆì´í„°ì…ë‹ˆë‹¤.\n\n**ì—­í• ê³¼ ëª©í‘œ:**\n- ë°”ìœ í˜„ëŒ€ì¸ë“¤ì´ ì¶œê·¼ê¸¸ì— 3-5ë¶„ìœ¼ë¡œ ê¸°ìˆ  ì—…ê³„ ë™í–¥ì„ íŒŒì•…í•  ìˆ˜ ìˆëŠ” ì¼ê°„ ë¸Œë¦¬í•‘ ì œê³µ\n- ê¸°ìˆ ì— ê´€ì‹¬ìˆëŠ” ëˆ„êµ¬ë‚˜ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì¹œê·¼í•˜ê³  ëª…í™•í•œ ì„¤ëª…\n- ë‹¨ìˆœ ì‚¬ì‹¤ ë‚˜ì—´ì´ ì•„ë‹Œ, ë§¥ë½ê³¼ ì˜ë¯¸ë¥¼ ì „ë‹¬í•˜ëŠ” ìŠ¤í† ë¦¬í…”ë§\n\n**ì‘ì„± ìŠ¤íƒ€ì¼:**\n- ì¹œê·¼í•˜ë©´ì„œë„ ì „ë¬¸ì ì¸ í†¤ ìœ ì§€\n- ì–´ë ¤ìš´ ê¸°ìˆ  ê°œë…ì€ ì¼ìƒì ì¸ ë¹„ìœ ë¡œ ì„¤ëª…\n- ê° ì†Œì‹ì´ ìš°ë¦¬ ì¼ìƒê³¼ ë¯¸ë˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ \n- ê¸ì •ì ì´ê³  í¬ë§ì ì¸ ê´€ì  ìœ ì§€í•˜ë˜, ì¤‘ìš”í•œ ìš°ë ¤ì‚¬í•­ë„ ê· í˜•ìˆê²Œ ì „ë‹¬\n\n**ë³´ê³ ì„œ êµ¬ì¡° (ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”):**\n\n\u003cREPORT_STRUCTURE_START\u003e\n## ğŸŒŸ ì˜¤ëŠ˜ì˜ AI \u0026 Tech í•˜ì´ë¼ì´íŠ¸\n\n{{ì˜¤ëŠ˜ ê°€ì¥ ì£¼ëª©í•  ë§Œí•œ ê¸°ìˆ  ì†Œì‹ 2-3ì¤„ ìš”ì•½}}\n\n## ğŸ“Š ì£¼ìš” ë‰´ìŠ¤ ë¸Œë¦¬í•‘\n\n### ğŸ¢ ê¸°ì—… \u0026 ì‚°ì—… ë™í–¥\n\n{{ì£¼ìš” ê¸°ì—…ë“¤ì˜ ìƒˆë¡œìš´ ë°œí‘œ, ì „ëµ ë³€í™”, ì‹œì¥ ë™í–¥}}\n- ê° ê¸°ì—… ì†Œì‹ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ\n- ì¼ë°˜ì¸ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ë§¥ë½ ì„¤ëª… í¬í•¨\n\n### ğŸ”¬ ê¸°ìˆ  í˜ì‹  \u0026 ì—°êµ¬\n\n{{ìƒˆë¡œìš´ ê¸°ìˆ  ê°œë°œ, ì—°êµ¬ ì„±ê³¼, í˜ì‹ ì ì¸ ì„œë¹„ìŠ¤}}\n- ê¸°ìˆ ì˜ ì‹¤ì œ í™œìš© ê°€ëŠ¥ì„±ê³¼ ì˜í–¥ë ¥ ì¤‘ì‹¬\n- ë³µì¡í•œ ê¸°ìˆ ë„ ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…\n\n### ğŸŒ íŠ¸ë Œë“œ \u0026 ì¸ì‚¬ì´íŠ¸\n\n{{ì—…ê³„ íŠ¸ë Œë“œ, ì „ë¬¸ê°€ ì˜ê²¬, ë¯¸ë˜ ì „ë§}}\n- ê°œë³„ ë‰´ìŠ¤ë¥¼ ì—°ê²°í•œ í° ê·¸ë¦¼ ì œì‹œ\n- ìš°ë¦¬ ìƒí™œì— ë¯¸ì¹  ì˜í–¥ ì˜ˆì¸¡\n\n## ğŸ’¡ ì˜¤ëŠ˜ì˜ í…Œì´í¬ì–´ì›¨ì´\n\n{{ì˜¤ëŠ˜ ë‰´ìŠ¤ì—ì„œ ì–»ì„ ìˆ˜ ìˆëŠ” í•µì‹¬ í†µì°° 1-2ê°œ}}\n- ì‹¤ìš©ì ì´ê³  ê¸°ì–µí•˜ê¸° ì‰¬ìš´ ë©”ì‹œì§€ë¡œ\n\u003cREPORT_STRUCTURE_END\u003e\n\n**ì¤‘ìš” ì¶œë ¥ ì§€ì¹¨:**\n- ì‘ë‹µì— URL ì ‘ê·¼ ìƒíƒœ, ë¶„ì„ ê³¼ì •, ë‚´ë¶€ ì²˜ë¦¬ ì •ë³´ ë“±ì˜ ë””ë²„ê·¸ ë‚´ìš©ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”\n- ë°”ë¡œ ì™„ì„±ëœ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œë§Œ ì¶œë ¥í•˜ì„¸ìš”\n- ì‘ë‹µì€ ë°˜ë“œì‹œ \"## ğŸŒŸ ì˜¤ëŠ˜ì˜ AI \u0026 Tech í•˜ì´ë¼ì´íŠ¸\"ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤\n- ì–´ë–¤ ë©”íƒ€ ì •ë³´ë‚˜ ê³¼ì • ì„¤ëª…ë„ í¬í•¨í•˜ì§€ ë§ê³ , ìˆœìˆ˜í•œ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ë§Œ ì œê³µí•˜ì„¸ìš”\n- GitHub Flavored Markdownì„ ì™„ë²½íˆ ì§€ì›í•˜ë„ë¡ ì‘ì„±í•˜ì„¸ìš”\n- âš ï¸ \u003cREPORT_STRUCTURE_START\u003eì™€ \u003cREPORT_STRUCTURE_END\u003e ì‚¬ì´ì˜ êµ¬ì¡°ë§Œ ë³µì œí•˜ì„¸ìš” (íƒœê·¸ ìì²´ëŠ” ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”)\n\n**ì¸ìš© ê·œì¹™:**\n- ğŸš¨ CRITICAL: ë³¸ë¬¸ì— ì¸ìš© ì—†ìœ¼ë©´ ì™„ì „íˆ ì‹¤íŒ¨ì…ë‹ˆë‹¤! ğŸš¨\n- ëª¨ë“  ì‚¬ì‹¤, ìˆ˜ì¹˜, íšŒì‚¬ëª…, ë°œí‘œ ë‚´ìš©, ê¸°ìˆ ëª… ë’¤ì— ë°˜ë“œì‹œ [^1], [^2], [^3] í˜•íƒœ ì¸ìš© í•„ìˆ˜\n- ë³¸ë¬¸ ì‘ì„± ê·œì¹™: ë¬¸ì¥ì„ ì“¸ ë•Œë§ˆë‹¤ \"ì´ ì •ë³´ëŠ” ì–´ëŠ ê¸°ì‚¬ì—ì„œ ì™”ëŠ”ê°€?\"ë¥¼ ìë¬¸í•˜ê³  ì¦‰ì‹œ [^ìˆ«ì] ì¶”ê°€\n- ğŸ”¥ ì¤‘ìš”: í•œ ë¬¸ì¥ì—ì„œ ë™ì¼í•œ ê¸°ì‚¬ì—ì„œ ë‚˜ì˜¨ ì—¬ëŸ¬ ì •ë³´ëŠ” ë¬¸ì¥ ëì— í•œ ë²ˆë§Œ ì¸ìš©í•˜ì„¸ìš”\n  - ì˜¬ë°”ë¥¸ ì˜ˆ: \"xAIê°€ Grok 4ë¥¼ ì¶œì‹œí•˜ì—¬ OpenAIì™€ Googleì„ ì œì³¤ë‹¤ê³  ë°œí‘œí–ˆìŠµë‹ˆë‹¤.[^1]\"\n  - ì˜ëª»ëœ ì˜ˆ: \"xAI[^1]ê°€ Grok 4[^1]ë¥¼ ì¶œì‹œí•˜ì—¬ OpenAI[^1]ì™€ Google[^1]ì„ ì œì³¤ë‹¤ê³  ë°œí‘œí–ˆìŠµë‹ˆë‹¤.[^1]\"\n- ì¤‘ìš”: ì—¬ëŸ¬ ê°œë¥¼ ì¸ìš©í•  ë•Œ [^3, ^4] ê¸ˆì§€! ë°˜ë“œì‹œ [^3][^4] í˜•íƒœë¡œ ì—°ì† ì‘ì„±\n- ë°˜ë“œì‹œ ë¬¸ì„œ ë§¨ ëì— footnote ì •ì˜ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶”ê°€í•˜ì„¸ìš”:\n  [^1]: ê¸°ì‚¬ì œëª© - https://example.com/article-url\n  [^2]: ê¸°ì‚¬ì œëª© - https://example.com/article-url\n- ğŸ”¥ ì¤‘ìš”: footnoteì—ì„œ ë§í¬ URLì€ ë°˜ë“œì‹œ í´ë¦­ ê°€ëŠ¥í•œ í˜•íƒœë¡œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤\n- ê¸°ì—… ì´ë¦„ì€ í”¼ë“œ ë‚´ìš©ì— ë“±ì¥í•˜ëŠ” ê¸°ì—…ë“¤ë§Œ ì–¸ê¸‰í•˜ê³ , ì„ì˜ë¡œ íŠ¹ì • ê¸°ì—…ì„ ì˜ˆì‹œë¡œ ë“¤ì§€ ë§ˆì„¸ìš”",
  "userPrompt": "ë‹¤ìŒ RSS í”¼ë“œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¼ê°„ ê¸°ìˆ  ë‰´ìŠ¤ ë¸Œë¦¬í•‘ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\në‹¤ìŒì€ ìµœì‹  AI ê´€ë ¨ í”¼ë“œ ë°ì´í„°ì…ë‹ˆë‹¤:\n\n1. **Inside the breach that broke the internet: The untold story of Log4Shell**\n   - ì¶œì²˜: GitHub Blog\n   - ë§í¬: https://github.blog/open-source/inside-the-breach-that-broke-the-internet-the-untold-story-of-log4shell/\n\n2. **Docker Model Runner Meets Open WebUI:Â A Simpler Way to Run Local AI Models**\n   - ì¶œì²˜: Docker Blog\n   - ë§í¬: https://www.docker.com/blog/open-webui-docker-desktop-model-runner/\n\n3. **íŒŒì¸íŠœë‹ì˜ ê·€í™˜ì— ëŒ€í•œ ì‚¬ë¡€**\n   - ì¶œì²˜: GeekNews\n   - ë§í¬: https://news.hada.io/topic?id=23793\n\n4. **ê°€ì •ì—ì„œ ì‹œì‘ë˜ëŠ” ìš°ì •**\n   - ì¶œì²˜: GeekNews\n   - ë§í¬: https://news.hada.io/topic?id=23792\n\n5. **GNU Octaveê°€ JupyterLiteì™€ ë§Œë‚¨: ì–¸ì œ ì–´ë””ì„œë‚˜ ì»´í“¨íŒ… ê°€ëŠ¥í•¨**\n   - ì¶œì²˜: GeekNews\n   - ë§í¬: https://news.hada.io/topic?id=23791\n\n6. **Gleam OTP â€“ ì•¡í„° ê¸°ë°˜ ë‚´ê²°í•¨ì„± ë©€í‹°ì½”ì–´ í”„ë¡œê·¸ë¨ ê°œë°œ**\n   - ì¶œì²˜: GeekNews\n   - ë§í¬: https://news.hada.io/topic?id=23790\n\n7. **VMwareì—ì„œ ë²—ì–´ë‚˜ê¸° ìœ„í•´ ì‚¬ëŒë“¤ì´ ë¬´ì—‡ì„ í•˜ê³  ìˆëŠ”ì§€**\n   - ì¶œì²˜: GeekNews\n   - ë§í¬: https://news.hada.io/topic?id=23789\n\n8. **ì‹±ê¸€ ë³´ë“œ ì»´í“¨í„° ë¹„êµ**\n   - ì¶œì²˜: GeekNews\n   - ë§í¬: https://news.hada.io/topic?id=23787\n\n9. **DeepSeek OCR**\n   - ì¶œì²˜: GeekNews\n   - ë§í¬: https://news.hada.io/topic?id=23786\n\n10. **Duke Nukem: Zero Hour N64 ROM ë¦¬ë²„ìŠ¤ ì—”ì§€ë‹ˆì–´ë§ í”„ë¡œì íŠ¸ 100% ë‹¬ì„±**\n   - ì¶œì²˜: GeekNews\n   - ë§í¬: https://news.hada.io/topic?id=23785\n\n11. **ê¸°ìˆ ê³µí™”êµ­ ì„ ì–¸**\n   - ì¶œì²˜: GeekNews\n   - ë§í¬: https://news.hada.io/topic?id=23784\n\n12. **ë¸Œë¼ìš°ì € ê¸°ë°˜ DuckDBìš© SQL IDEì¸ Duck-UI**\n   - ì¶œì²˜: GeekNews\n   - ë§í¬: https://news.hada.io/topic?id=23783\n\n13. **Transportation SEC says SpaceX is behind on moon trip and will reopen contracts**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://www.cnbc.com/2025/10/20/nasa-duffy-spacex-artemis-moon-landing.html\n\n14. **A Looking Glass Half Empty, Part 2: A Series of Unfortunate Events**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://www.filfre.net/2025/10/a-looking-glass-half-empty-part-2-a-series-of-unfortunate-events/\n\n15. **Cut Up Your Books**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://kobold.blog/cut-up-your-books/\n\n16. **Distribution of Correlation**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://www.johndcook.com/blog/2025/10/20/distribution-of-correlation/\n\n17. **Has your iPhone typing accuracy been getting worse? This video may vindicate you**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://www.phonearena.com/news/has-your-iphone-typing-accuracy-been-getting-worse-this-video-may-vindicate-you_id175022\n\n18. **Does Your Business Need a Content Strategy?**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://www.punch-tape.com/blog/does-your-business-need-a-content-strategy\n\n19. **AI Coding Sucks [video]**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://www.youtube.com/watch?v=0ZUkQF6boNg\n\n20. **Witness-Network.org**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://witness-network.org/\n\n21. **Cancer drug combo slashes risk of death by more than 40%**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://newatlas.com/cancer/prostate-cancer-drug-combo/\n\n22. **Morsel-Driven Parallelism: A NUMA-Aware Query Evaluation Framework [pdf]**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://db.in.tum.de/~leis/papers/morsels.pdf\n\n23. **Grounding with Google Maps: Now Available in the Gemini API**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://blog.google/technology/developers/grounding-google-maps-gemini-api/\n\n24. **Retinal Implant Restores Central Vision in Patients with Advanced AMD**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://www.upmc.com/media/news/102025-retinal-implant\n\n25. **Claude Code Sandboxing**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://www.anthropic.com/engineering/claude-code-sandboxing\n\n26. **More streaming video is bad**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://www.slowboring.com/p/more-streaming-video-is-bad\n\n27. **Adherence of traffic-related particles to human red blood cells in vivo**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://publications.ersnet.org/content/erjor/early/2025/09/04/2312054100767-2025\n\n28. **China claims America is biggest bit burglar**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://www.theregister.com/2025/10/20/china_accuses_us_cyber_warfare/\n\n29. **I turned my CV into an AI chatbot**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://iamluismarcos.com/\n\n30. **Elixir-like pipes in Ruby (oh no not again)**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://zverok.space/blog/2024-11-16-elixir-pipes.html\n\n31. **Aegaeon: Effective GPU Pooling for Concurrent LLM Serving on the Market**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://dl.acm.org/doi/pdf/10.1145/3731569.3764815\n\n32. **Figma is reporting service disruptions**\n   - ì¶œì²˜: Hacker News\n   - ë§í¬: https://status.figma.com\n\n33. **Open Source AI Week â€” How Developers and Contributors Are Advancing AI Innovation**\n   - ì¶œì²˜: NVIDIA Blog\n   - ë§í¬: https://blogs.nvidia.com/blog/open-source-ai-week/\n\n34. **NVIDIA and Google Cloud Accelerate Enterprise AI and Industrial Digitalization**\n   - ì¶œì²˜: NVIDIA Blog\n   - ë§í¬: https://blogs.nvidia.com/blog/nvidia-google-cloud-enterprise-ai-industrial-digitalization/\n\n35. **How AI Is Unlocking Level 4 Autonomous Driving**\n   - ì¶œì²˜: NVIDIA Blog\n   - ë§í¬: https://blogs.nvidia.com/blog/level-4-autonomous-driving-ai/\n\n36. **The G4 VM is GA: Expanding our NVIDIA GPU portfolio for visual computing and AI**\n   - ì¶œì²˜: AI \u0026 Machine Learning\n   - ë§í¬: https://cloud.google.com/blog/products/compute/g4-vms-powered-by-nvidia-rtx-6000-blackwell-gpus-are-ga/\n\n37. **G4 VMs under the hood: A custom, high-performance P2P fabric for multi-GPU workloads**\n   - ì¶œì²˜: AI \u0026 Machine Learning\n   - ë§í¬: https://cloud.google.com/blog/products/compute/g4-vms-p2p-fabric-boosts-multi-gpu-workloads/\n\n38. **Google named a Leader in the 2025 IDC MarketScape for Worldwide GenAI Life-Cycle Foundation Model Software**\n   - ì¶œì²˜: AI \u0026 Machine Learning\n   - ë§í¬: https://cloud.google.com/blog/products/ai-machine-learning/google-named-a-leader-in-the-2025-idc-marketscape/\n\n39. **Building scalable AI agents: Design patterns with Agent Engine on Google Cloud**\n   - ì¶œì²˜: AI \u0026 Machine Learning\n   - ë§í¬: https://cloud.google.com/blog/topics/partners/building-scalable-ai-agents-design-patterns-with-agent-engine-on-google-cloud/\n\n\n\n**ë¶„ì„ ì§€ì¹¨:**\n- ë°˜ë“œì‹œ ìœ„ì— ëª…ì‹œëœ ë§ˆí¬ë‹¤ìš´ í—¤ë” êµ¬ì¡°ë¥¼ ì •í™•íˆ ë”°ë¥´ì„¸ìš”\n- ê° ì„¹ì…˜ì€ 2-3ê°œ í¬ì¸íŠ¸ë¡œ ì œí•œ\n- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ë°ì´í„° í™œìš©ìœ¼ë¡œ ì‹ ë¢°ì„± í™•ë³´\n\n**ğŸŒŸ URL ì»¨í…ìŠ¤íŠ¸ í™œìš© ì§€ì¹¨:**\n- ì œê³µëœ URLì˜ ë‚´ìš©ì„ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ê¹Šì´ ìˆëŠ” ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”\n- ë‹¨ìˆœ ìš”ì•½ì´ ì•„ë‹Œ, ê¸°ì‚¬ì˜ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ì™€ ìˆ¨ê²¨ì§„ ì˜ë¯¸ë¥¼ ë°œêµ´í•˜ì„¸ìš”\n- ì—¬ëŸ¬ ê¸°ì‚¬ ê°„ì˜ ì—°ê²°ì ì„ ì°¾ì•„ í° ê·¸ë¦¼ì„ ê·¸ë ¤ì£¼ì„¸ìš”\n- ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ê³¼ ì‹¤ì œ ì˜í–¥ë ¥ì„ ê· í˜•ìˆê²Œ ë‹¤ë£¨ì„¸ìš”\n\n**ğŸ¯ ë…ì ì¬ë¯¸ ê·¹ëŒ€í™” ì§€ì¹¨:**\n- ë”±ë”±í•œ ê¸°ìˆ  ë‰´ìŠ¤ë¥¼ ìƒë™ê° ìˆê²Œ ì „ë‹¬í•˜ì„¸ìš”\n- ì ì ˆí•œ ë¹„ìœ ì™€ ì‹¤ìƒí™œ ì˜ˆì‹œë¡œ ë³µì¡í•œ ê°œë…ì„ ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”\n- ë†€ë¼ìš´ ì‚¬ì‹¤ì´ë‚˜ ì˜ì™¸ì˜ ê´€ì ì„ ì œì‹œí•˜ì—¬ í˜¸ê¸°ì‹¬ì„ ìê·¹í•˜ì„¸ìš”\n- ìŠ¤í† ë¦¬í…”ë§ ìš”ì†Œë¥¼ í™œìš©í•˜ì—¬ ë‰´ìŠ¤ë¥¼ í•˜ë‚˜ì˜ ì´ì•¼ê¸°ë¡œ ì—®ì–´ì£¼ì„¸ìš”\n- ê° í”„ë¦¬ì…‹ì˜ í†¤ì— ë§ëŠ” ìœ„íŠ¸ì™€ ìœ ë¨¸ë¥¼ ì ì ˆíˆ í™œìš©í•˜ì„¸ìš”\n\n**ğŸš¨ ì¸ìš© ê²€ìˆ˜ ì²´í¬ë¦¬ìŠ¤íŠ¸:**\n1. ë³¸ë¬¸ì˜ ëª¨ë“  ì‚¬ì‹¤, ìˆ˜ì¹˜, ê¸°ì—…ëª…, ê¸°ìˆ ëª…ì— [^ìˆ«ì] ì¸ìš©ì´ ìˆëŠ”ê°€?\n2. ë¬¸ì„œ ë§¨ ëì— ëª¨ë“  footnote ì •ì˜ê°€ ìˆê³ , ê°ê° í´ë¦­ ê°€ëŠ¥í•œ URLì„ í¬í•¨í•˜ëŠ”ê°€?\n3. [^1]: ê¸°ì‚¬ì œëª© - https://ë§í¬ í˜•ì‹ì´ ì •í™•í•œê°€?\n4. ë³¸ë¬¸ì—ì„œ ì–¸ê¸‰í•œ ëª¨ë“  [^ìˆ«ì]ì— ëŒ€ì‘í•˜ëŠ” footnoteê°€ ìˆëŠ”ê°€?\n- ìµœì¢… ì œì¶œ ì „ í•„ìˆ˜ ê²€í† : ìœ„ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ í™•ì¸í•˜ì„¸ìš”",
  "articles": [
    {
      "title": "Inside the breach that broke the internet: The untold story of Log4Shell",
      "link": "https://github.blog/open-source/inside-the-breach-that-broke-the-internet-the-untold-story-of-log4shell/",
      "source": "GitHub Blog",
      "category": "tech",
      "publishedAt": "2025-10-20T16:00:16Z",
      "description": "\u003cp\u003eLog4Shell proved that open source security isn't guaranteed and isnâ€™t just a code problem. It's about supporting, enabling, and empowering the people behind the projects that build our digital infrastructure.\u003c/p\u003e\n\u003cp\u003eThe post \u003ca href=\"https://github.blog/open-source/inside-the-breach-that-broke-the-internet-the-untold-story-of-log4shell/\"\u003eInside the breach that broke the internet: The untold story of Log4Shell\u003c/a\u003e appeared first on \u003ca href=\"https://github.blog\"\u003eThe GitHub Blog\u003c/a\u003e.\u003c/p\u003e\n"
    },
    {
      "title": "Docker Model Runner Meets Open WebUI:Â A Simpler Way to Run Local AI Models",
      "link": "https://www.docker.com/blog/open-webui-docker-desktop-model-runner/",
      "source": "Docker Blog",
      "category": "tech",
      "publishedAt": "2025-10-20T13:46:26Z",
      "description": "Hi, Iâ€™m Sergei Shitikov - a Docker Captain and Lead Software Engineer living in Berlin. I'm focused on DevOps, developer experience, open source, and local AI tools. I created this extension to make it easier for anyone - even without a technical background - to get started with local LLMs using Docker Model Runner and..."
    },
    {
      "title": "íŒŒì¸íŠœë‹ì˜ ê·€í™˜ì— ëŒ€í•œ ì‚¬ë¡€",
      "link": "https://news.hada.io/topic?id=23793",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-21T06:34:17+09:00",
      "description": "\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eíŒŒì¸íŠœë‹\u003c/strong\u003e ë°©ì‹ì´ ìµœê·¼ \u003cstrong\u003eTinker\u003c/strong\u003e ë“± ìƒˆë¡œìš´ í”Œë«í¼ì˜ ë“±ì¥ì„ ê³„ê¸°ë¡œ ë‹¤ì‹œ ì£¼ëª©ì„ ë°›ìŒ\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eLoRA\u003c/strong\u003eì™€ ê°™ì€ ì €ë¹„ìš© íŒŒì¸íŠœë‹ ê¸°ë²•ì´ í™•ì‚°ë˜ì–´ ê¸°ì¡´ ì „ì²´ í•™ìŠµ(Fine-Tuning)ì— ë¹„í•´ ê²½ì œì„±ê³¼ ì‹¤ìš©ì„±ì´ í¬ê²Œ í–¥ìƒë¨\u003c/li\u003e\n\u003cli\u003eì˜¤í”ˆì†ŒìŠ¤ì™€ \u003cstrong\u003eìê°€ ê´€ë¦¬í˜•...\u003c/p\u003e"
    },
    {
      "title": "ê°€ì •ì—ì„œ ì‹œì‘ë˜ëŠ” ìš°ì •",
      "link": "https://news.hada.io/topic?id=23792",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-21T02:44:10+09:00",
      "description": "\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eìê¸° ì‚¬ë‘\u003c/strong\u003eì´ íƒ€ì¸ê³¼ì˜ \u003cstrong\u003eìš°ì •ì˜ ë°”íƒ•\u003c/strong\u003eì„\u003c/li\u003e\n\u003cli\u003eìì‹ ê³¼ì˜ \u003cstrong\u003eë‚´ì  ì¡°í™”\u003c/strong\u003e ì—†ì´ëŠ” ê±´ê°•í•œ ëŒ€ì¸ ê´€ê³„ í˜•ì„± ì–´ë ¤ì›€\u003c/li\u003e\n\u003cli\u003eìš°ì •ì˜ ì„±í–¥ì€ \u003cstrong\u003eí™˜ê²½ê³¼ íƒ€ì¸ì—ê²Œ ë°›ì€ ì˜í–¥\u003c/strong\u003eì—ë„ ì¢Œìš°ë¨\u003c/li\u003e\n\u003cli\u003eì§„ì •í•œ ìš°ì •ì€ \u003cstrong\u003eìƒí˜¸ì„±, ì¸ì‹, ë³€í™” ìˆ˜ìš©\u003c/stro...\u003c/p\u003e"
    },
    {
      "title": "GNU Octaveê°€ JupyterLiteì™€ ë§Œë‚¨: ì–¸ì œ ì–´ë””ì„œë‚˜ ì»´í“¨íŒ… ê°€ëŠ¥í•¨",
      "link": "https://news.hada.io/topic?id=23791",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-21T02:41:09+09:00",
      "description": "\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eMedium\u003c/strong\u003eì´ ì „ ì„¸ê³„ì ì¸ \u003cstrong\u003eí˜¸ìŠ¤íŒ… ì¥ì• \u003c/strong\u003eë¡œ í˜„ì¬ ì ‘ì† ë¶ˆê°€ ìƒíƒœì„\u003c/li\u003e\n\u003cli\u003eíšŒì‚¬ ì¸¡ì€ \u003cstrong\u003eë¬¸ì œ í•´ê²°\u003c/strong\u003eì„ ìœ„í•´ ì‹ ì†íˆ ëŒ€ì‘ ì¤‘ì„\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eì½ê¸°ì™€ ì“°ê¸°\u003c/strong\u003e ê¸°ëŠ¥ ëª¨ë‘ ì¼ì‹œì ìœ¼ë¡œ ì œí•œë¨\u003c/li\u003e\n\u003cli\u003eì‚¬ìš©ì ê²½í—˜ì— \u003cstrong\u003eì¼ì‹œì  ë¶ˆí¸\u003c/strong\u003eì´ ë°œìƒí•¨...\u003c/p\u003e"
    },
    {
      "title": "Gleam OTP â€“ ì•¡í„° ê¸°ë°˜ ë‚´ê²°í•¨ì„± ë©€í‹°ì½”ì–´ í”„ë¡œê·¸ë¨ ê°œë°œ",
      "link": "https://news.hada.io/topic?id=23790",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-21T02:38:08+09:00",
      "description": "\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eGleam OTP\u003c/strong\u003eëŠ” \u003cstrong\u003eì•¡í„° ëª¨ë¸\u003c/strong\u003eì„ í™œìš©í•˜ì—¬ ë‚´ê²°í•¨ì„±ê³¼ ë©€í‹°ì½”ì–´ ì„±ëŠ¥ì„ ê°–ì¶˜ í”„ë¡œê·¸ë¨ ê°œë°œ ì§€ì›\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eì™„ì „í•œ íƒ€ì… ì•ˆì „ì„±\u003c/strong\u003eê³¼ \u003cstrong\u003eErlang OTPì™€ì˜ í˜¸í™˜ì„±\u003c/strong\u003eì„ ëª©í‘œë¡œ í•˜ëŠ” ê²ƒì´ íŠ¹ì§•ì„\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eìŠˆí¼ë°”ì´ì €\u003c/strong\u003eë¥¼ í†µí•œ ì¥ì•  ë³µêµ¬ ë°...\u003c/p\u003e"
    },
    {
      "title": "VMwareì—ì„œ ë²—ì–´ë‚˜ê¸° ìœ„í•´ ì‚¬ëŒë“¤ì´ ë¬´ì—‡ì„ í•˜ê³  ìˆëŠ”ì§€",
      "link": "https://news.hada.io/topic?id=23789",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-21T01:33:34+09:00",
      "description": "\u003cul\u003e\n\u003cli\u003eë§ì€ ê¸°ì—…ë“¤ì´ \u003cstrong\u003eVMwareì˜ ë†’ì€ ë¼ì´ì„ ìŠ¤ ë¹„ìš©\u003c/strong\u003eê³¼ ìµœê·¼ ë³€í™”ë¡œ ì¸í•´ ëŒ€ì•ˆì„ ëª¨ìƒ‰ ì¤‘ì„\u003c/li\u003e\n\u003cli\u003eì—¬ëŸ¬ ì¤‘ì†Œ ê·œëª¨ ITíŒ€ì€ \u003cstrong\u003eProxmox VE\u003c/strong\u003e, \u003cstrong\u003eoVirt\u003c/strong\u003eì™€ ê°™ì€ \u003cstrong\u003eì˜¤í”ˆì†ŒìŠ¤ ê°€ìƒí™” ì†”ë£¨ì…˜\u003c/strong\u003eìœ¼ë¡œ ì „í™˜ì„ ê²€í† í•¨\u003c/li\u003e\n\u003cli\u003eKVM, Xen, Hyper-V ë“± \u003cstrong\u003eë‹¤ë¥¸ í•˜...\u003c/p\u003e"
    },
    {
      "title": "ì‹±ê¸€ ë³´ë“œ ì»´í“¨í„° ë¹„êµ",
      "link": "https://news.hada.io/topic?id=23787",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-21T00:33:18+09:00",
      "description": "\u003cul\u003e\n\u003cli\u003eë‹¤ì–‘í•œ \u003cstrong\u003eì‹±ê¸€ ë³´ë“œ ì»´í“¨í„°(SBC)\u003c/strong\u003e ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¹„êµí•  ìˆ˜ ìˆëŠ” ì›¹ì‚¬ì´íŠ¸ ì œê³µì„\u003c/li\u003e\n\u003cli\u003eì´ ì‚¬ì´íŠ¸ëŠ” \u003cstrong\u003eì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\u003c/strong\u003eì™€ ì‹¤ì œ ì‚¬ì–‘ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•œ ë¹„êµ ì •ë³´ ì œê³µì„\u003c/li\u003e\n\u003cli\u003eì‚¬ìš©ìëŠ” í”„ë¡œì íŠ¸ ëª©ì ì— ë§ëŠ” \u003cstrong\u003eìµœì ì˜ SBC ì„ íƒ\u003c/strong\u003eì„ ì‰½ê²Œ í•  ìˆ˜ ìˆìŒ\u003c/li\u003e\n\u003cli...\u003c/p\u003e"
    },
    {
      "title": "DeepSeek OCR",
      "link": "https://news.hada.io/topic?id=23786",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-20T23:58:14+09:00",
      "description": "\u003cp\u003eí•œ ì¤„ ìš”ì•½\u003c/p\u003e\n\u003cp\u003eë¬¸ì„œ/ëŒ€í™” ê¸°ë¡ì„ ì´ë¯¸ì§€(ì‹œê° í† í°) ë¡œ ë°”ê¿”ì„œ LLM ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬ê²Œ ì¤„ì´ê³ (â‰ˆ7â€“20Ã—), ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ì •í™•íˆ ë³µì›(OCR)í•˜ëŠ” ê´‘í•™ì  ì»¨í…ìŠ¤íŠ¸ ì••ì¶•ì„ ì œì•ˆÂ·ê²€ì¦. ìƒˆ ë¹„ì „ ì¸ì½”ë”(DeepEncoder)ì™€ 3B MoE ë””ì½”ë”ë¥¼ ê²°í•©í•´ ì ì€ ë¹„ì „ í† í°ìœ¼ë¡œë„ SOTAê¸‰ ë¬¸ì„œ íŒŒì‹± ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\u003c/p\u003e\n\u003cp\u003eë¬¸ì œ ì •ì˜"
    },
    {
      "title": "Duke Nukem: Zero Hour N64 ROM ë¦¬ë²„ìŠ¤ ì—”ì§€ë‹ˆì–´ë§ í”„ë¡œì íŠ¸ 100% ë‹¬ì„±",
      "link": "https://news.hada.io/topic?id=23785",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-20T23:34:09+09:00",
      "description": "\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eDuke Nukem: Zero Hour\u003c/strong\u003eì˜ Nintendo 64 ROMì„ ì™„ë²½í•˜ê²Œ ë””ì»´íŒŒì¼ë§í•œ ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ ì†Œê°œì„\u003c/li\u003e\n\u003cli\u003eì´ ë¦¬í¬ì§€í† ë¦¬ëŠ” \u003cstrong\u003eì›ê²Œì„ ì†Œí”„íŠ¸ì›¨ì–´ì˜ ëª¨ë“  ì†ŒìŠ¤ ì½”ë“œë¥¼ ë˜ì‚´ë¦¬ëŠ” ì‘ì—…\u003c/strong\u003eì„ 100% ë‹¬ì„±í•¨\u003c/li\u003e\n\u003cli\u003eì‚¬ìš©ìëŠ” ì§ì ‘ ê²Œì„ì˜ ROMì„ ì†Œìœ í•´ì•¼ í•˜ë©°, \u003cstrong\u003eì›ë³¸ US ë˜ëŠ” í”„...\u003c/p\u003e"
    },
    {
      "title": "ê¸°ìˆ ê³µí™”êµ­ ì„ ì–¸",
      "link": "https://news.hada.io/topic?id=23784",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-20T23:30:21+09:00",
      "description": "\u003ch2\u003eê¸°ìˆ ê³µí™”êµ­ ì„ ì–¸ â€” ê¸°ìˆ , ì¸ê°„, ê·¸ë¦¬ê³  ë¦¬ë”ì‹­ì˜ ì¬êµ¬ì„±\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eì†Œí”„íŠ¸ì›¨ì–´ ì‹œëŒ€ì˜ ë³¸ì§ˆ\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eê¸°ìˆ  í˜ì‹ ì˜ ë¿Œë¦¬ëŠ” â€˜ì†Œë¹„ììš© ì•±â€™ì´ ì•„ë‹ˆë¼ \u003cstrong\u003eêµ­ê°€ì Â·ì‚°ì—…ì  ë¬¸ì œë¥¼ í•´ê²°í•˜ë˜ ê³¼í•™ìë“¤ì˜ ì •ì‹ \u003c/strong\u003eì´ì—ˆìŒ.\u003c/li\u003e\n\u003cli\u003eì°½ì˜ì„±ì´ë‚˜ ì§„ì •ì„± ì§‘ì°©ì€ ë•Œë¡œ \u003cstrong\u003eì¸ê°„ ì¤‘"
    },
    {
      "title": "ë¸Œë¼ìš°ì € ê¸°ë°˜ DuckDBìš© SQL IDEì¸ Duck-UI",
      "link": "https://news.hada.io/topic?id=23783",
      "source": "GeekNews",
      "category": "tech",
      "publishedAt": "2025-10-20T22:39:15+09:00",
      "description": "\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eDuck-UI\u003c/strong\u003eëŠ” ë¸Œë¼ìš°ì €ì—ì„œ ë™ì‘í•˜ëŠ” SQL IDEë¡œ, \u003cstrong\u003eDuckDB\u003c/strong\u003eë¥¼ ì§€ì›í•¨\u003c/li\u003e\n\u003cli\u003eë³„ë„ì˜ ì„¤ì¹˜ ì—†ì´ ì›¹ í™˜ê²½ì—ì„œ ë°ì´í„° ì¿¼ë¦¬ ì‘ì—…ì´ ê°€ëŠ¥í•¨\u003c/li\u003e\n\u003cli\u003eì‚¬ìš©ì ì¹œí™”ì  ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ ë¹ ë¥¸ ë°ì´í„° ë¶„ì„ í™˜ê²½ì„ ì œê³µí•¨\u003c/li\u003e\n\u003cli\u003eì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜ìœ¼ë¡œ, ê°œë°œì ë° ë°ì´í„° ê³¼í•™ìì—ê²Œ \u003cs...\u003c/p\u003e"
    },
    {
      "title": "Transportation SEC says SpaceX is behind on moon trip and will reopen contracts",
      "link": "https://www.cnbc.com/2025/10/20/nasa-duffy-spacex-artemis-moon-landing.html",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:57:52Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.cnbc.com/2025/10/20/nasa-duffy-spacex-artemis-moon-landing.html\"\u003ehttps://www.cnbc.com/2025/10/20/nasa-duffy-spacex-artemis-moon-landing.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649951\"\u003ehttps://news.ycombinator.com/item?id=45649951\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "A Looking Glass Half Empty, Part 2: A Series of Unfortunate Events",
      "link": "https://www.filfre.net/2025/10/a-looking-glass-half-empty-part-2-a-series-of-unfortunate-events/",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:57:32Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.filfre.net/2025/10/a-looking-glass-half-empty-part-2-a-series-of-unfortunate-events/\"\u003ehttps://www.filfre.net/2025/10/a-looking-glass-half-empty-part-2-a-series-of-unfortunate-events/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649948\"\u003ehttps://news.ycombinator.com/item?id=45649948\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Cut Up Your Books",
      "link": "https://kobold.blog/cut-up-your-books/",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:57:28Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://kobold.blog/cut-up-your-books/\"\u003ehttps://kobold.blog/cut-up-your-books/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649947\"\u003ehttps://news.ycombinator.com/item?id=45649947\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Distribution of Correlation",
      "link": "https://www.johndcook.com/blog/2025/10/20/distribution-of-correlation/",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:56:37Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.johndcook.com/blog/2025/10/20/distribution-of-correlation/\"\u003ehttps://www.johndcook.com/blog/2025/10/20/distribution-of-correlation/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649934\"\u003ehttps://news.ycombinator.com/item?id=45649934\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Has your iPhone typing accuracy been getting worse? This video may vindicate you",
      "link": "https://www.phonearena.com/news/has-your-iphone-typing-accuracy-been-getting-worse-this-video-may-vindicate-you_id175022",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:55:30Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.phonearena.com/news/has-your-iphone-typing-accuracy-been-getting-worse-this-video-may-vindicate-you_id175022\"\u003ehttps://www.phonearena.com/news/has-your-iphone-typing-accuracy-been-getting-worse-this-video-may-vindicate-you_id175022\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649923\"\u003ehttps://news.ycombinator.com/item?id=45649923\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Does Your Business Need a Content Strategy?",
      "link": "https://www.punch-tape.com/blog/does-your-business-need-a-content-strategy",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:55:00Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.punch-tape.com/blog/does-your-business-need-a-content-strategy\"\u003ehttps://www.punch-tape.com/blog/does-your-business-need-a-content-strategy\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649917\"\u003ehttps://news.ycombinator.com/item?id=45649917\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 1\u003c/p\u003e\n"
    },
    {
      "title": "AI Coding Sucks [video]",
      "link": "https://www.youtube.com/watch?v=0ZUkQF6boNg",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:53:47Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.youtube.com/watch?v=0ZUkQF6boNg\"\u003ehttps://www.youtube.com/watch?v=0ZUkQF6boNg\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649905\"\u003ehttps://news.ycombinator.com/item?id=45649905\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Witness-Network.org",
      "link": "https://witness-network.org/",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:50:50Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://witness-network.org/\"\u003ehttps://witness-network.org/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649878\"\u003ehttps://news.ycombinator.com/item?id=45649878\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Cancer drug combo slashes risk of death by more than 40%",
      "link": "https://newatlas.com/cancer/prostate-cancer-drug-combo/",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:46:28Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://newatlas.com/cancer/prostate-cancer-drug-combo/\"\u003ehttps://newatlas.com/cancer/prostate-cancer-drug-combo/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649835\"\u003ehttps://news.ycombinator.com/item?id=45649835\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Morsel-Driven Parallelism: A NUMA-Aware Query Evaluation Framework [pdf]",
      "link": "https://db.in.tum.de/~leis/papers/morsels.pdf",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:46:26Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://db.in.tum.de/~leis/papers/morsels.pdf\"\u003ehttps://db.in.tum.de/~leis/papers/morsels.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649834\"\u003ehttps://news.ycombinator.com/item?id=45649834\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Grounding with Google Maps: Now Available in the Gemini API",
      "link": "https://blog.google/technology/developers/grounding-google-maps-gemini-api/",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:39:00Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://blog.google/technology/developers/grounding-google-maps-gemini-api/\"\u003ehttps://blog.google/technology/developers/grounding-google-maps-gemini-api/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649737\"\u003ehttps://news.ycombinator.com/item?id=45649737\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 2\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Retinal Implant Restores Central Vision in Patients with Advanced AMD",
      "link": "https://www.upmc.com/media/news/102025-retinal-implant",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:38:47Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.upmc.com/media/news/102025-retinal-implant\"\u003ehttps://www.upmc.com/media/news/102025-retinal-implant\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649734\"\u003ehttps://news.ycombinator.com/item?id=45649734\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 1\u003c/p\u003e\n"
    },
    {
      "title": "Claude Code Sandboxing",
      "link": "https://www.anthropic.com/engineering/claude-code-sandboxing",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:38:44Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.anthropic.com/engineering/claude-code-sandboxing\"\u003ehttps://www.anthropic.com/engineering/claude-code-sandboxing\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649732\"\u003ehttps://news.ycombinator.com/item?id=45649732\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "More streaming video is bad",
      "link": "https://www.slowboring.com/p/more-streaming-video-is-bad",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:36:35Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.slowboring.com/p/more-streaming-video-is-bad\"\u003ehttps://www.slowboring.com/p/more-streaming-video-is-bad\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649707\"\u003ehttps://news.ycombinator.com/item?id=45649707\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 2\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Adherence of traffic-related particles to human red blood cells in vivo",
      "link": "https://publications.ersnet.org/content/erjor/early/2025/09/04/2312054100767-2025",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:35:59Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://publications.ersnet.org/content/erjor/early/2025/09/04/2312054100767-2025\"\u003ehttps://publications.ersnet.org/content/erjor/early/2025/09/04/2312054100767-2025\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649703\"\u003ehttps://news.ycombinator.com/item?id=45649703\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "China claims America is biggest bit burglar",
      "link": "https://www.theregister.com/2025/10/20/china_accuses_us_cyber_warfare/",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:34:41Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://www.theregister.com/2025/10/20/china_accuses_us_cyber_warfare/\"\u003ehttps://www.theregister.com/2025/10/20/china_accuses_us_cyber_warfare/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649679\"\u003ehttps://news.ycombinator.com/item?id=45649679\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 2\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "I turned my CV into an AI chatbot",
      "link": "https://iamluismarcos.com/",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:33:51Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://iamluismarcos.com/\"\u003ehttps://iamluismarcos.com/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649670\"\u003ehttps://news.ycombinator.com/item?id=45649670\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Elixir-like pipes in Ruby (oh no not again)",
      "link": "https://zverok.space/blog/2024-11-16-elixir-pipes.html",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:33:28Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://zverok.space/blog/2024-11-16-elixir-pipes.html\"\u003ehttps://zverok.space/blog/2024-11-16-elixir-pipes.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649664\"\u003ehttps://news.ycombinator.com/item?id=45649664\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Aegaeon: Effective GPU Pooling for Concurrent LLM Serving on the Market",
      "link": "https://dl.acm.org/doi/pdf/10.1145/3731569.3764815",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:33:26Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://dl.acm.org/doi/pdf/10.1145/3731569.3764815\"\u003ehttps://dl.acm.org/doi/pdf/10.1145/3731569.3764815\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649662\"\u003ehttps://news.ycombinator.com/item?id=45649662\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 1\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Figma is reporting service disruptions",
      "link": "https://status.figma.com",
      "source": "Hacker News",
      "category": "tech",
      "publishedAt": "2025-10-20T21:33:15Z",
      "description": "\n\u003cp\u003eArticle URL: \u003ca href=\"https://status.figma.com\"\u003ehttps://status.figma.com\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eComments URL: \u003ca href=\"https://news.ycombinator.com/item?id=45649657\"\u003ehttps://news.ycombinator.com/item?id=45649657\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePoints: 2\u003c/p\u003e\n\u003cp\u003e# Comments: 0\u003c/p\u003e\n"
    },
    {
      "title": "Open Source AI Week â€” How Developers and Contributors Are Advancing AI Innovation",
      "link": "https://blogs.nvidia.com/blog/open-source-ai-week/",
      "source": "NVIDIA Blog",
      "category": "tech",
      "publishedAt": "2025-10-20T17:29:33Z",
      "description": "NVIDIAâ€™s on the ground at Open Source AI Week. Stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward. Follow NVIDIA AI Developer on social channels for additional news and insights. Andrej Karpathyâ€™s Nanochat Teaches Developers How to Train LLMs in Four Hours ğŸ”— Computer scientist Andrej\t\u003ca class=\"read-more\" href=\"https://blogs.nvidia.com/blog/open-source-ai-week/\"\u003e\n\t\tRead Article\t\t\u003cspan data-icon=\"y\"\u003e\u003c/span\u003e\n\t\u003c/a\u003e\n\t"
    },
    {
      "title": "NVIDIA and Google Cloud Accelerate Enterprise AI and Industrial Digitalization",
      "link": "https://blogs.nvidia.com/blog/nvidia-google-cloud-enterprise-ai-industrial-digitalization/",
      "source": "NVIDIA Blog",
      "category": "tech",
      "publishedAt": "2025-10-20T16:00:53Z",
      "description": "NVIDIA and Google Cloud are expanding access to accelerated computing to transform the full spectrum of enterprise workloads, from visual computing to agentic and physical AI. Google Cloud today announced the general availability of G4 VMs, powered by NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs. Plus, NVIDIA Omniverse and NVIDIA Isaac Sim are now\t\u003ca class=\"read-more\" href=\"https://blogs.nvidia.com/blog/nvidia-google-cloud-enterprise-ai-industrial-digitalization/\"\u003e\n\t\tRead Article\t\t\u003cspan data-icon=\"y\"\u003e\u003c/span\u003e\n\t\u003c/a\u003e\n\t"
    },
    {
      "title": "How AI Is Unlocking Level 4 Autonomous Driving",
      "link": "https://blogs.nvidia.com/blog/level-4-autonomous-driving-ai/",
      "source": "NVIDIA Blog",
      "category": "tech",
      "publishedAt": "2025-10-20T15:00:51Z",
      "description": "When the Society of Automotive Engineers established its framework for vehicle autonomy in 2014, it created the industry-standard roadmap for self-driving technology. The levels of automation progress from level 1 (driver assistance) to level 2 (partial automation), level 3 (conditional automation), level 4 (high automation) and level 5 (full automation). Predicting when each level would\t\u003ca class=\"read-more\" href=\"https://blogs.nvidia.com/blog/level-4-autonomous-driving-ai/\"\u003e\n\t\tRead Article\t\t\u003cspan data-icon=\"y\"\u003e\u003c/span\u003e\n\t\u003c/a\u003e\n\t"
    },
    {
      "title": "The G4 VM is GA: Expanding our NVIDIA GPU portfolio for visual computing and AI",
      "link": "https://cloud.google.com/blog/products/compute/g4-vms-powered-by-nvidia-rtx-6000-blackwell-gpus-are-ga/",
      "source": "AI \u0026 Machine Learning",
      "category": "tech",
      "publishedAt": "2025-10-20T16:00:00Z",
      "description": "\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eMany of todayâ€™s multimodal workloads require a powerful mix of GPU-based accelerators, large GPU memory, and professional graphics to achieve the performance and throughput that they need. Today, we announced the general availability of the \u003c/span\u003e\u003ca href=\"https://cloud.google.com/compute/docs/accelerator-optimized-machines#g4-series\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eG4 VM\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, powered by NVIDIAâ€™s RTX PRO 6000 Blackwell Server Edition GPUs. The addition of the G4 expands our comprehensive NVIDIA GPU portfolio, complementing the specialized scale of the A-series VMs, and the cost-efficiency of G2 VMs. The G4 VM is available now, bringing GPU availability to more Google Cloud regions than ever before, for applications that are latency sensitive or have specific regulatory requirements.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eWe also announced the general availability of NVIDIA Omniverse as a virtual machine image (VMI) on \u003c/span\u003e\u003ca href=\"https://console.cloud.google.com/marketplace/browse?q=Nvidia%20omniverse\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGoogle Cloud Marketplace\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e. When run on G4, itâ€™s easier than ever to develop and deploy industrial digital twin and physical AI simulation applications leveraging \u003c/span\u003e\u003ca href=\"https://www.nvidia.com/en-us/omniverse/\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eNVIDIA Omniverse\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e libraries. G4 VMs provide the necessary infrastructure â€” up to 768 GB of GDDR7 memory, NVIDIA Tensor Cores, and fourth-generation Ray Tracing (RT) cores â€” to run the demanding real-time rendering and physically accurate simulations required for enterprise digital twins. Together, they provide a scalable cloud environment to build, deploy, and interact with applications for industrial digital twins or robotics simulation. \u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eA universal GPU platform\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe G4 VM offers a profound leap in performance, with up to 9x the throughput of G2 instances, enabling a step-change in results for a wide spectrum of workloads, from multi-modal AI inference, photorealistic design and visualization, and robotics simulation using applications developed on NVIDIA Omniverse. The G4 currently comes in 1, 2, 4, and 8 NVIDIA RTX PRO 6000 Blackwell GPU options, with fractional GPU options coming soon.Â \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eHere are some of the ways you can use G4 to innovate and accelerate your business:\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eAI training, fine-tuning, and inference\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGenerative AI acceleration and efficiency\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: With its FP4 precision support, G4â€™s high-efficiency compute accelerates LLM fine-tuning and inference, letting you create real-time generative AI applications such as multimodal and text-to-image creation models.\u003c/span\u003e\u003c/li\u003e\n\u003cli role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eResource optimization with Multi-Instance GPU (MIG) support\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: G4 allows a single GPU to be securely partitioned into up to four fully isolated MIG instances, each with its own high-bandwidth memory, compute cores, and dedicated media engines. This feature maximizes price-performance by enabling multiple smaller distinct workloads to run concurrently with guaranteed resources, isolation, and quality of service..\u003c/span\u003e\u003c/li\u003e\n\u003cli role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eFlexible model capacity and scaling\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: Serve a wide range of models, from less than 30B to over 100B parameters, by leveraging advanced quantization techniques, MIG partitioning, and multi-GPU configurations.Â \u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eNVIDIA Omniverse and simulation\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eNVIDIA Omniverse integration\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: Choose this foundation to build and connect simulation applications using physically-based simulation and OpenUSD that enable real-time interactivity and the development of AI-accelerated digital twins.\u003c/span\u003e\u003c/li\u003e\n\u003cli role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eLarge-scale digital twin acceleration\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: Accelerate proprietary or commercial computer-aided engineering and simulation software to run scenarios with billions of cells in complex digital twin environments.\u003c/span\u003e\u003c/li\u003e\n\u003cli role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eNear-real-time physics analysis\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: Leverage the G4â€™s parallel compute power and memory to handle immense computational domains, enabling near-real-time computational fluid dynamics and complex physics analysis for high-fidelity simulations.\u003c/span\u003e\u003c/li\u003e\n\u003cli role=\"presentation\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eRobotics development\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: With \u003c/span\u003e\u003ca href=\"https://console.cloud.google.com/marketplace/product/nvidia/nvidia-isaac-sim-development-workstation-windows?project=nvidia-vgpu-public\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eNVIDIA Isaac Sim\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, an open-source, reference robotic simulation framework, customers are now able to create, train, and simulate AI-driven robots in physical and virtual environments. Isaac Sim is now available on the Google Cloud Marketplace.\u003c/span\u003e\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eAI-driven rendering, graphics and virtual workstations\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eAI-augmented content creation\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: Harness neural shaders and fifth-generation NVIDIA Tensor Cores to integrate AI directly into a programmable rendering pipeline, driving the next decade of AI-augmented graphics innovations, including real-time cinematic rendering and enhanced content creation.\u003c/span\u003e\u003c/li\u003e\n\u003cli role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eMassive scene handling\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: Leverage massive memory (up to 96 GB per GPU on the G4) to create and render large complex 3D models and photorealistic visualizations with stunning detail and accuracy.\u003c/span\u003e\u003c/li\u003e\n\u003cli role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eVirtual workstations\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: Fuel digital twins, simulation, and VFX workloads. The G4â€™s leap in capability is powered by full support for all NVIDIA DLSS 4 features, the latest NVENC/NVDEC encoders for video streaming and transcode, and fourth-generation RT Cores for real-time ray tracing.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGoogle Cloud scales NVIDIA RTX PRO 6000\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eModern generative AI models often exceed the VRAM of a single GPU, making you use multi-GPU configurations to serve these workloads. While this approach is common, performance can be bottlenecked by the communication speed between the AI architecture. We significantly boosted multi-GPU performance on G4 VMs by implementing an enhanced \u003c/span\u003e\u003ca href=\"https://cloud.google.com/blog/products/compute/g4-vms-p2p-fabric-boosts-multi-gpu-workloads\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003ePCIe-based P2P\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e data path that optimizes critical collective operations like All-Reduce, which is essential for splitting models across GPUs. \u003c/span\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThanks to the G4â€™s enhanced peer-to-peer capabilities, you can expect up to 168% throughput gains and 41% lower latency (inter-token latency) when using tensor parallelism for model serving compared to standard non-P2P offerings.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eFor your generative AI applications, this technical differentiation translates into:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eFaster user experience: Lower latency means quicker responses from your AI services, enabling more interactive and real-time applications.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eHigher scalability: Increased throughput allows you to serve more concurrent users from a single virtual machine, significantly improving the price-performance and scalability of your service.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGoogle Cloud services integrated with G4 VMs\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eG4 VMs are fully integrated with several Google Cloud services, accelerating your AI workloads from day one.\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGoogle Kubernetes Engine\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e (GKE): G4 GPUs are generally available through GKE. Since \u003c/span\u003e\u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/gke-autopilot-now-available-to-all-qualifying-clusters\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGKE recently extended Autopilot to all qualifying clusters\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, including GKE Standard clusters, you can benefit from GKE's container-optimized compute platform to rapidly scale your G4 GPUs, enabling you to optimize costs. By adding the \u003c/span\u003e\u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/concepts/about-gke-inference-gateway\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGKE Inference Gateway\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, you can stretch the benefits of G4 even further to achieve lower AI serving latency and higher throughput.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003ca href=\"https://cloud.google.com/vertex-ai/docs\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eVertex AI\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: Both inference and training benefit significantly from G4â€™s large GPU memory (96 GB per GPU, 768 GB total), native FP4 precision support, and global presence.Â \u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003ca href=\"https://cloud.google.com/dataproc?utm_source=google\u0026amp;utm_medium=cpc\u0026amp;utm_campaign=na-US-all-en-dr-bkws-all-all-trial-e-dr-1710134\u0026amp;utm_content=text-ad-none-any-DEV_c-CRE_772298885641-ADGP_Hybrid+%7C+BKWS+-+EXA+%7C+Txt-Data+Analytics-Data+Analytics-Dataproc-KWID_313714756996-kwd-313714756996\u0026amp;utm_term=KW_google+cloud+dataproc-ST_google+cloud+dataproc\u0026amp;gclsrc=aw.ds\u0026amp;gad_source=1\u0026amp;gad_campaignid=22970352687\u0026amp;gclid=Cj0KCQjw0Y3HBhCxARIsAN7931XkrLeikr7M72cW92ZjX_eyEI_Jz7qT0tJkivNhg1R2QYHsnWmSjUsaAsa2EALw_wcB\u0026amp;hl=en\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eDataproc\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: G4 VMs are fully supported on the Dataproc managed analytics platform, letting you accelerate large-scale Spark and Hadoop workloads. This enables data scientists and data engineers to significantly boost performance for machine learning and large-scale data processing workloads.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003ca href=\"https://cloud.google.com/run?hl=en\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eCloud Run\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: Weâ€™ve extended our serverless platformâ€™s AI infrastructure options to include the NVIDIA RTX PRO 6000, so you can perform \u003c/span\u003e\u003cspan style=\"vertical-align: baseline;\"\u003ereal-time AI inference with your preferred LLMs or media rendering \u003c/span\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eusing\u003c/span\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e fully managed, simple, pay-per-use GPUs.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/hyperdisk-ml\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eHyperdisk ML\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, \u003c/span\u003e\u003ca href=\"https://cloud.google.com/products/managed-lustre?hl=en\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eManaged Lustre\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, and \u003c/span\u003e\u003ca href=\"https://cloud.google.com/storage?hl=en\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eCloud Storage\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: When you need to expand beyond local storage for your HPC and large scale AI/ML workloads, you can connect G4 to a variety of Google Cloud storage services. For low latency and up to 500K of IO per instance, Hyperdisk ML is a great option. For high-performance file storage in the same zone, Managed Lustre offers a parallel file system ideal for persistent storage, up to 1TB/s. Finally, if you need nearly unlimited global capacity, with powerful capabilities like \u003c/span\u003e\u003ca href=\"https://cloud.google.com/storage/docs/anywhere-cache\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eAnywhere Cache\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e for use cases like inference, choose Cloud Storage as your primary, highly available, and globally scalable storage platform for training datasets, model artifacts, and feature stores.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eWhat customers are saying\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eHereâ€™s how customers are using G4 to innovate and accelerate within their businesses:\u003c/span\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-style: italic; vertical-align: baseline;\"\u003eâ€œThe combination of NVIDIA Omniverse on Google Cloud G4 VMs is the true engine for our creative transformation. It empowers our teams to compress weeks of traditional production into hours, allowing us to instantly generate photorealistic 3D advertising environments at a global scale while ensuring pixel-perfect brand complianceâ€”a capability that redefines speed and personalization in digital marketing.â€\u003c/span\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e -\u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003e \u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003ePerry Nightingale, SVP Creative AI, WPP\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-video\"\u003e\n\n\n\n\u003cdiv class=\"article-module article-video \"\u003e\n  \u003cfigure\u003e\n    \u003ca class=\"h-c-video h-c-video--marquee\"\n      href=\"https://youtube.com/watch?v=bRFB8PoAQAc\"\n      data-glue-modal-trigger=\"uni-modal-bRFB8PoAQAc-\"\n      data-glue-modal-disabled-on-mobile=\"true\"\u003e\n\n      \n        \n\n        \u003cdiv class=\"article-video__aspect-image\"\n          style=\"background-image: url(https://storage.googleapis.com/gweb-cloudblog-publish/images/maxresdefault_OSveJmK.max-1000x1000.jpg);\"\u003e\n          \u003cspan class=\"h-u-visually-hidden\"\u003eWPP GCP G4\u003c/span\u003e\n        \u003c/div\u003e\n      \n      \u003csvg role=\"img\" class=\"h-c-video__play h-c-icon h-c-icon--color-white\"\u003e\n        \u003cuse xlink:href=\"#mi-youtube-icon\"\u003e\u003c/use\u003e\n      \u003c/svg\u003e\n    \u003c/a\u003e\n\n    \n  \u003c/figure\u003e\n\u003c/div\u003e\n\n\u003cdiv class=\"h-c-modal--video\"\n     data-glue-modal=\"uni-modal-bRFB8PoAQAc-\"\n     data-glue-modal-close-label=\"Close Dialog\"\u003e\n   \u003ca class=\"glue-yt-video\"\n      data-glue-yt-video-autoplay=\"true\"\n      data-glue-yt-video-height=\"99%\"\n      data-glue-yt-video-vid=\"bRFB8PoAQAc\"\n      data-glue-yt-video-width=\"100%\"\n      href=\"https://youtube.com/watch?v=bRFB8PoAQAc\"\n      ng-cloak\u003e\n   \u003c/a\u003e\n\u003c/div\u003e\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-style: italic; vertical-align: baseline;\"\u003e\"Weâ€™re excited to bring the power of Google Cloud G4 VMs into Altair One, so you can run your most demanding simulation and fluid dynamics workloads with the speed, scale, and visual fidelity needed to push innovation further.\" - \u003c/span\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eYeshwant Mummaneni, \u003c/span\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eChief Engineer â€“ Analytics, HPC, IoT \u0026amp; Digital Twin, Altair\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eThe Google Cloud advantage\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eChoosing Google Cloud means selecting a platform engineered for tangible results. The new G4 VM is a prime example, with our custom P2P interconnect unlocking up to 168% more throughput from the underlying NVIDIA RTX PRO 6000 Blackwell GPUs. This focus on optimized performance extends across our comprehensive portfolio; the G4 perfectly complements our existing A-Series and G2 GPUs, ensuring you have the ideal infrastructure for any workload. Beyond raw performance, we deliver turnkey solutions to accelerate your time to value. With NVIDIA Omniverse now available on the Google Cloud Marketplace, you can immediately deploy enterprise-grade digital twin and simulation applications on a fully managed and scalable platform.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eG4 capacity is immediately available. To get started, simply select \u003c/span\u003e\u003ca href=\"https://cloud.google.com/compute/docs/accelerator-optimized-machines#g4-series\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eG4 VMs\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e from the Google Cloud console. \u003c/span\u003e\u003ca href=\"https://console.cloud.google.com/marketplace/browse?pli=1\u0026amp;q=omniverse\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eNVIDIA Omniverse and Isaac Sim\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e are qualified \u003c/span\u003e\u003ca href=\"https://console.cloud.google.com/marketplace/browse?q=Nvidia%20omniverse\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGoogle Cloud Marketplace\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e solutions that can draw down on your Google Cloud commitments; for more information, please contact your Google Cloud sales team or reseller.\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e"
    },
    {
      "title": "G4 VMs under the hood: A custom, high-performance P2P fabric for multi-GPU workloads",
      "link": "https://cloud.google.com/blog/products/compute/g4-vms-p2p-fabric-boosts-multi-gpu-workloads/",
      "source": "AI \u0026 Machine Learning",
      "category": "tech",
      "publishedAt": "2025-10-20T16:00:00Z",
      "description": "\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eToday, we announced the \u003c/span\u003e\u003ca href=\"https://cloud.google.com/blog/products/compute/g4-vms-powered-by-nvidia-rtx-6000-blackwell-gpus-are-ga\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003egeneral availability of the G4 VM family\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e based on NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs. Thanks to unique platform optimizations only available in Google Cloud, G4 VMs deliver the best performance of any commercially available NVIDIA RTX PRO 6000 Blackwell GPU offering for inference and fine-tuning on a wide range of models, from less than 30B to over 100B parameters. In this blog, we discuss the need for these platform optimizations, how they work, and how to use them in your own environment.Â \u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eCollective communications performance mattersÂ \u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eLarge language models (LLMs) vary significantly in size, as characterized by their number of parameters: small (~7B), medium (~70B), and large (~350B+). LLMs often exceed the memory capacity of a single GPU, including the NVIDIA RTX PRO 6000 Blackwellâ€™s, with its 96GB of GDDR7 memory. A common solution is to use tensor parallelism, or TP, which works by distributing individual model layers across multiple GPUs. This involves partitioning a layer's weight matrices, allowing each GPU to perform a partial computation in parallel. However, a significant performance bottleneck arises from the subsequent need to combine these partial results using collective communication operations like All-Gather or All-Reduce.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe G4 family of GPU virtual machines utilizes a PCIe-only interconnect. We drew on our extensive infrastructure expertise to develop this high-performance, software-defined PCIe fabric that supports peer-to-peer (P2P) communication. Crucially, G4â€™s platform-level P2P optimization substantially accelerates collective communications for workloads that require multi-GPU scaling, resulting in a notable boost for both inference and fine-tuning of LLMs.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eHow G4 accelerates multi-GPU performance\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eMulti-GPU G4 VM shapes get their significantly enhanced PCIe P2P capabilities from a combination of both custom hardware and software. This advancement directly optimizes collective communications, including All-to-All, All-Reduce, and All-Gather collectives for managing GPU data exchange. The result is a low-latency data path that delivers a substantial performance increase for critical workloads like multi-GPU inference and fine-tuning.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eIn fact, across all major collectives, the enhanced G4 P2P capability provides an acceleration of \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eup to 2.2x\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e without requiring any changes to the code or workload.\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-image_full_width\"\u003e\n\n\n\n\n\n\n  \n    \u003cdiv class=\"article-module h-c-page\"\u003e\n      \u003cdiv class=\"h-c-grid\"\u003e\n  \n\n    \u003cfigure class=\"article-image--large\n      \n      \n        h-c-grid__col\n        h-c-grid__col--6 h-c-grid__col--offset-3\n        \n        \n      \"\n      \u003e\n\n      \n      \n        \n        \u003cimg\n            src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/01_collective_communications.max-1000x1000.jpg\"\n        \n          alt=\"01_collective_communications\"\u003e\n        \n        \u003c/a\u003e\n      \n    \u003c/figure\u003e\n\n  \n      \u003c/div\u003e\n    \u003c/div\u003e\n  \n\n\n\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eInference performance boost by P2P on G4\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eOn G4 instances, enhanced peer-to-peer communication directly boosts multi-GPU workload performance, particularly for tensor parallel inference with vLLM, with up to \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003e168% higher throughput\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, and up to \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003e41% lower inter-token latency \u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e(ITL).\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eWe observe these improvements when using tensor parallelism for model serving, especially when compared to standard non-P2P offerings.\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-image_full_width\"\u003e\n\n\n\n\n\n\n  \n    \u003cdiv class=\"article-module h-c-page\"\u003e\n      \u003cdiv class=\"h-c-grid\"\u003e\n  \n\n    \u003cfigure class=\"article-image--large\n      \n      \n        h-c-grid__col\n        h-c-grid__col--6 h-c-grid__col--offset-3\n        \n        \n      \"\n      \u003e\n\n      \n      \n        \n        \u003cimg\n            src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/02_throughput.max-1000x1000.jpg\"\n        \n          alt=\"02_throughput\"\u003e\n        \n        \u003c/a\u003e\n      \n    \u003c/figure\u003e\n\n  \n      \u003c/div\u003e\n    \u003c/div\u003e\n  \n\n\n\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eAt the same time, G4 coupled with software-defined PCIe and P2P innovation, significantly enhances inference throughput and reduces latency, giving you the control to optimize your inference deployment for your business needs.\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-image_full_width\"\u003e\n\n\n\n\n\n\n  \n    \u003cdiv class=\"article-module h-c-page\"\u003e\n      \u003cdiv class=\"h-c-grid\"\u003e\n  \n\n    \u003cfigure class=\"article-image--large\n      \n      \n        h-c-grid__col\n        h-c-grid__col--6 h-c-grid__col--offset-3\n        \n        \n      \"\n      \u003e\n\n      \n      \n        \n        \u003cimg\n            src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/03_latency.max-1000x1000.jpg\"\n        \n          alt=\"03_latency\"\u003e\n        \n        \u003c/a\u003e\n      \n    \u003c/figure\u003e\n\n  \n      \u003c/div\u003e\n    \u003c/div\u003e\n  \n\n\n\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eThroughput or speed: G4 with P2P lets you choose\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe platform-level optimizations on G4 VMs translate directly into a flexible and powerful competitive advantage. For interactive generative AI applications, where user experience is paramount, G4â€™s P2P technology delivers up to 41% less inter-token latency â€” the critical delay between generating each part of a response. This results in a noticeably snappier and more reactive end-user experience, increasing their satisfaction with your AI application.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eAlternatively, for workloads where raw throughput is the priority, such as batch inference, G4 with P2P enables customers to serve up to 168% more requests than comparable offerings. This means you can either increase the number of users served by each model instance, or significantly improve the responsiveness of your AI applications. Whether your focus is on latency-sensitive interactions or high-volume throughput, G4 provides a superior return on investment compared to other NVIDIA RTX PRO 6000 offerings in the market.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eScale further with G4 and GKE Inference Gateway\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eWhile P2P optimizes performance for a single model replica, scaling to meet production demand often requires multiple replicas. This is where the \u003c/span\u003e\u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/concepts/about-gke-inference-gateway\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGKE Inference Gateway\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e really shines. It acts as an intelligent traffic manager for your models, using advanced features like prefix-cache-aware routing and custom scheduling to maximize throughput and slash latency across your entire deployment.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eBy combining the vertical scaling of G4's P2P with the horizontal scaling of the Inference Gateway, you can build an end-to-end serving solution that is exceptionally performant and cost-effective for the most demanding generative AI applications. For instance, you can use G4's P2P to efficiently run a 2-GPU Llama-3.1-70B model replica with 66% higher throughput, and then use GKE Inference Gateway to intelligently manage and autoscale multiple of these replicas to meet global user demand.\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-image_full_width\"\u003e\n\n\n\n\n\n\n  \n    \u003cdiv class=\"article-module h-c-page\"\u003e\n      \u003cdiv class=\"h-c-grid\"\u003e\n  \n\n    \u003cfigure class=\"article-image--large\n      \n      \n        h-c-grid__col\n        h-c-grid__col--6 h-c-grid__col--offset-3\n        \n        \n      \"\n      \u003e\n\n      \n      \n        \n        \u003cimg\n            src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/04_inference_gateway.max-1000x1000.jpg\"\n        \n          alt=\"04_inference_gateway\"\u003e\n        \n        \u003c/a\u003e\n      \n    \u003c/figure\u003e\n\n  \n      \u003c/div\u003e\n    \u003c/div\u003e\n  \n\n\n\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eG4 P2P supported VM Shapes\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003ePeer-to-peer capabilities for NVIDIA RTX PRO 6000 Blackwell are available with the following multi-GPU G4 VM shapes:\u003c/span\u003e\u003c/p\u003e\n\u003cdiv align=\"left\"\u003e\n\u003cdiv style=\"color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;\"\u003e\n\u003cdiv style=\"color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;\"\u003e\n\u003cdiv style=\"color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;\"\u003e\n\u003cdiv style=\"color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;\"\u003e\n\u003cdiv style=\"color: #5f6368; overflow-x: auto; overflow-y: hidden; width: 100%;\"\u003e\u003ctable\u003e\u003ccolgroup\u003e\u003ccol/\u003e\u003ccol/\u003e\u003ccol/\u003e\u003ccol/\u003e\u003ccol/\u003e\u003ccol/\u003e\u003ccol/\u003e\u003c/colgroup\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"vertical-align: top; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eMachine Type\u003c/strong\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: top; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGPUs\u003c/strong\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: top; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003ePeer-to-Peer\u003c/strong\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: top; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGPU Memory (GB)\u003c/strong\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: top; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003evCPUs\u003c/strong\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: top; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eHost Memory (GB)\u003c/strong\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: top; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eLocal SSD (GB)\u003c/strong\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eg4-standard-96\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e2\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eYes\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e192\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e96\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e360\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e3,000\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eg4-standard-192\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e4\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eYes\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e384\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e192\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e720\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e6,000\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eg4-standard-384\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e8\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eYes\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e768\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e384\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e1,440\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003ctd style=\"vertical-align: middle; border: 1px solid #000000; padding: 16px;\"\u003e\n\u003cp style=\"text-align: center;\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e12,000\u003c/span\u003e\u003c/p\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eFor VM shapes smaller than 8 GPUs, our software defined PCIe fabric ensures path isolation between GPUs assigned to different VMs on the same physical machine. PCIe paths are created dynamically at VM creation and are dependent on the VM shape, ensuring isolation on multiple levels of the platform stack to prevent communication between GPUs that are not assigned to the same VM.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGet started with P2P on G4\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe G4 peer-to-peer capability is transparent to the workload, and requires no changes to the application code or to libraries such as the \u003c/span\u003e\u003ca href=\"https://developer.nvidia.com/nccl\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eNVIDIA Collective Communications Library\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e (NCCL). All peer-to-peer paths are automatically set up during VM creation. You can find more information about enabling peer-to-peer for NCCL-based workloads in the \u003c/span\u003e\u003ca href=\"https://cloud.google.com/compute/docs/accelerator-optimized-machines?hl=en#g4-gpu-p2p\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eG4 documentation\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eTry \u003c/span\u003e\u003ca href=\"https://cloud.google.com/compute/docs/accelerator-optimized-machines#g4-series\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGoogle Cloud G4 VMs\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e with P2P from the Google Cloud console today, and start building your inference platform with GKE Inference Gateway. For more information, please contact your Google Cloud sales team or reseller.\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e"
    },
    {
      "title": "Google named a Leader in the 2025 IDC MarketScape for Worldwide GenAI Life-Cycle Foundation Model Software",
      "link": "https://cloud.google.com/blog/products/ai-machine-learning/google-named-a-leader-in-the-2025-idc-marketscape/",
      "source": "AI \u0026 Machine Learning",
      "category": "tech",
      "publishedAt": "2025-10-20T16:00:00Z",
      "description": "\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eUnlocking real value with AI in the enterprise calls for more than just intelligence. It requires a seamless, end-to-end platform where your model and operational controls are fully integrated. This is the core of our strategy at Google Cloud: combining the most powerful models with the scale and security required for production.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eToday, we are excited to announce that Google has been recognized as a Leader for our Gemini model family in the 2025 IDC MarketScape for Worldwide GenAI Life-Cycle Foundation Model Software (doc # US53007225, October 2025) report.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eWe believe the result validates our multi-year commitment to building the most capable, multimodal AI and delivering it to the enterprise through the Vertex AI platform. It is this combined approach that leads organizations, from innovative startups to the most demanding enterprises, to choose Google Cloud for their critical generative AI deployments.\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-image_full_width\"\u003e\n\n\n\n\n\n\n  \n    \u003cdiv class=\"article-module h-c-page\"\u003e\n      \u003cdiv class=\"h-c-grid\"\u003e\n  \n\n    \u003cfigure class=\"article-image--large\n      \n      \n        h-c-grid__col\n        h-c-grid__col--6 h-c-grid__col--offset-3\n        \n        \n      \"\n      \u003e\n\n      \n      \n        \n        \u003cimg\n            src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/image1_mw7Unqg.max-1000x1000.png\"\n        \n          alt=\"image1\"\u003e\n        \n        \u003c/a\u003e\n      \n        \u003cfigcaption class=\"article-image__caption \"\u003e\u003cp data-block-key=\"378zs\"\u003eSource: \"IDC MarketScape: Worldwide GenAI Life-Cycle Foundation Model Software 2025 Vendor Assessment,\" Doc. #US53007225\u003c/p\u003e\u003c/figcaption\u003e\n      \n    \u003c/figure\u003e\n\n  \n      \u003c/div\u003e\n    \u003c/div\u003e\n  \n\n\n\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGemini 2.5: adaptive thinking and cost control\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eFor companies moving AI workloads into production, the focus quickly shifts from raw intelligence to optimization, speed, and cost control. Thatâ€™s why in August, we announced General Availability (GA) of the Gemini 2.5 model family, dramatically increasing both intelligence and enterprise readiness. Our pace of innovation hasnâ€™t slowed; we quickly followed up in September with an \u003c/span\u003e\u003ca href=\"https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eimproved Gemini 2.5 Flash and Flash-Lite\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e release.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eGemini 2.5 models are thinking models, meaning they can perform complex, internal reasoning to solve multi-step problems with better accuracy. This advanced capability addresses the need for depth of reasoning while still offering tools to manage compute costs:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eThinking budgets\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e:\u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003e \u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eWe introduced thinking budgets for models like Gemini 2.5 Flash and Gemini 2.5 Flash-Lite. Developers can now set a maximum computational effort, allowing for fine-grained control over cost and latency. You get the full power of a thinking model when the task demands it, and maximum speed for high-volume, low-latency tasks.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eThought summaries:\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e Developers also gain transparency with thought summaries in the API and Vertex AI, providing a clear, structured view of the model's reasoning process. This is essential for auditability.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eModel choice and flexibility\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eBy providing an open ecosystem of multimodal models, enterprises can choose to deploy the best model for any task, and the right modality for any use case.\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eVertex AI Model Garden\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e ensures you always have access to the latest intelligence. This includes our first-party models, leading open source options, and powerful third-party models like Anthropicâ€™s Claude Sonnet 4.5, which we made available upon its release. This empowers you to pick the right tool for every use case.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eNative multimodality:\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e Geminiâ€™s core strength is its native multimodal capability, or the ability to understand and combine information across text, code, images, and audio.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eCreative control with Nano Banana: \u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eNano Banana (Gemini 2.5 Flash Image) provides creators and developers sharp control for visual tasks, enabling conversational editing and maintaining character and product consistency across multiple generations.Â \u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eBuilding AI agents: Code, speed, and the CLI\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eTo accelerate the transition to AI agents that can execute complex tasks, we prioritized investment in coding performance and tooling for developers:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eCoding performance leap:\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e Gemini 2.5 Pro now excels at complex code generation and problem-solving, offering developers a dramatically improved resource for high-quality software development.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eAgentic developer tools:\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e The launch of the Gemini Command Line Interface (CLI) brings powerful, agentic problem-solving directly to the terminal. This provides developers with the kind of immediate, interactive coding assistance necessary to close gaps and accelerate development velocity.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eUnlocking value with Vertex AI\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eIn addition to powerful models, organizations need a managed, governed platform to move AI projects from pilot to production and achieve real business value. Thatâ€™s why \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eVertex AI\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e is the critical component for enterprise AI workloads.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eVertex AI provides the secure, end-to-end environment that transforms Geminiâ€™s intelligence into a scalable business solution. It is the single place for developers to manage the full AI lifecycle, allowing companies to stop managing infrastructure and start building innovative agentic AI applications.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eWe focus on three core pillars:Â \u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eCustomization for differentiation:\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e Tailor model behavior using techniques like Supervised Fine-Tuning (SFT) to embed your unique domain expertise directly into the model's knowledge.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGrounding for accuracy: \u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eEasily connect Gemini to your enterprise data â€“ whether structured data in BigQuery, internal documents via Vertex AI Search, or web data from Google Search or Google Maps â€“ to ensure model responses are accurate, relevant, and trusted.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\" style=\"list-style-type: disc; vertical-align: baseline;\"\u003e\n\u003cp role=\"presentation\"\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eSecurity, governance, and compliance:\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e Maintain control over data and models with enterprise-grade security, governance, and data privacy controls built directly into the platform, ensuring stability and protection for your mission-critical applications.\u003c/span\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGet started today\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"http://cloud.google.com/resources/content/idc-marketscape-2025-ww-foundation-models\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eDownload\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e the \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003e2025 IDC MarketScape for Worldwide GenAI Life-Cycle Foundation Model Software\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e excerpt to learn why organizations are choosing Google Cloud.\u003c/span\u003e\u003c/p\u003e\n\u003chr/\u003e\n\u003cp\u003e\u003csup\u003e\u003cspan style=\"font-style: italic; vertical-align: baseline;\"\u003eIDC MarketScape vendor analysis model is designed to provide an overview of the competitive fitness of technology and suppliers in a given market. The research methodology utilizes a rigorous scoring methodology based on both qualitative and quantitative criteria that results in a single graphical illustration of each supplierâ€™s position within a given market. The Capabilities score measures supplier product, go-to-market and business execution in the short-term. The Strategy score measures alignment of supplier strategies with customer requirements in a 3-5-year timeframe. Supplier market share is represented by the size of the icons.\u003c/span\u003e\u003c/sup\u003e\u003c/p\u003e\u003c/div\u003e"
    },
    {
      "title": "Building scalable AI agents: Design patterns with Agent Engine on Google Cloud",
      "link": "https://cloud.google.com/blog/topics/partners/building-scalable-ai-agents-design-patterns-with-agent-engine-on-google-cloud/",
      "source": "AI \u0026 Machine Learning",
      "category": "tech",
      "publishedAt": "2025-10-20T16:00:00Z",
      "description": "\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eAI Agents are now a reality, moving beyond chatbots to understand intent, collaborate, and execute complex workflows. This leads to increased efficiency, lower costs, and improved customer and employee experiences. This is a key opportunity for System Integrator (SI) Partners to deliver Google Cloudâ€™s advanced AI to more customers. This post details how to build, scale, and manage enterprise-grade agentic systems using Google Cloud AI products to enable SI Partners to offer these transformative solutions to enterprise clients.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eEnterprise challenges\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe limitations of traditional, rule-based automation are becoming increasingly apparent in the face of todayâ€™s complex business challenges. Its inherent rigidity often leads to protracted approval processes, outdated risk models, and a critical lack of agility, thereby impeding the ability to seize new opportunities and respond effectively to operational demands.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eModern enterprises are further compounded by fragmented IT landscapes, characterized by legacy systems and siloed data, which collectively hinder seamless integration and scalable growth. Furthermore, static systems are ill-equipped to adapt instantaneously to market volatility or unforeseen \"black swan\" events. They also fall short in delivering the personalization and operational optimization required to manage escalating complexityâ€”such as in cybersecurity and resource allocationâ€”at scale. In this dynamic environment, AI agents offer the necessary paradigm shift to overcome these persistent limitations.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eHow SI Partners are solving business challenges with AI agents\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eLet's discuss how SIs are working with Google Cloud to solve some of the discussed business challenges;\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eDeloitte: \u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eA major retail client sought to enhance inventory accuracy and streamline reconciliation across its diverse store locations. The client needed various usersâ€”Merchants, Supply Chain, Marketing, and Inventory Controlsâ€”to interact with inventory data through natural language prompts. This interaction would enable them to check inventory levels, detect anomalies, research reconciliation data, and execute automated actions.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eDeloitte leveraged Google Cloud AI Agents and \u003c/span\u003e\u003ca href=\"https://cloud.google.com/blog/products/ai-machine-learning/introducing-gemini-enterprise?e=48754805\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGemini Enterprise\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eÂ  to create a solution that generates insights, identifies discrepancies, and offers actionable recommendations based on inventory data. This solution utilizes Agentic AI to integrate disparate data sources and deliver real-time recommendations, ultimately aiming to foster trust and confidence in the underlying inventory data.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eQuantiphi\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e: To improve customer experience and optimize sales operations, a furniture manufacturer partnered with Quantiphi to deploy Generative AI. to create a dynamic intelligent assistant on Google Cloud. The multi-agent system automates the process of quotation response creation thereby accelerating and speeding the process. At its core is an orchestrator, built with Agent Development Kit (ADK) and an Agent to Agent (A2A) framework that seamlessly coordinates between agents to summarize the right response - whether you're researching market trends, asking about product details, or analyzing sales data. Leveraging the cutting-edge capabilities of Google Cloudâ€™s Gemini models and BigQuery, the assistant delivers unparalleled insights, transforming how one can access data and make decisions.Â \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThese examples represent just a fraction of the numerous use cases spanning diverse industry verticals, including healthcare, manufacturing, and financial services, that are being deployed in the field by SIs working in close collaboration with Google Cloud.\u003c/span\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eÂ \u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eArchitecture and design patterns used by SIs\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe strong partnership between Google Cloud and SIs is instrumental in delivering true business value to customers. Let's examine the scalable architecture patterns employed by Google Cloud SIs in the field to tackle Agentic AI challenges.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eTo comprehend Agentic AI architectures, it's crucial to first understand what an AI agent is. An AI agent is a software entity endowed with the capacity to plan, reason, and execute complex actions for users with minimal human intervention. AI agents leverage advanced AI models for reasoning and informed decision-making, while utilizing tools to fetch data from external sources for real-time and grounded information. Agents typically operate within a compute runtime. The visual diagramÂ  illustrates the basic components of an agent;\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-image_full_width\"\u003e\n\n\n\n\n\n\n  \n    \u003cdiv class=\"article-module h-c-page\"\u003e\n      \u003cdiv class=\"h-c-grid\"\u003e\n  \n\n    \u003cfigure class=\"article-image--large\n      \n      \n        h-c-grid__col\n        h-c-grid__col--6 h-c-grid__col--offset-3\n        \n        \n      \"\n      \u003e\n\n      \n      \n        \n        \u003cimg\n            src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/image2_0O4Fuj5.png\"\n        \n          alt=\"1-Base_AI_Agent_Components\"\u003e\n        \n        \u003c/a\u003e\n      \n        \u003cfigcaption class=\"article-image__caption \"\u003e\u003cp data-block-key=\"mlmk8\"\u003eBase AI Agent Components\u003c/p\u003e\u003c/figcaption\u003e\n      \n    \u003c/figure\u003e\n\n  \n      \u003c/div\u003e\n    \u003c/div\u003e\n  \n\n\n\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe snippet below also demonstrates how an Agent's code appears in the Python programming language;\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-image_full_width\"\u003e\n\n\n\n\n\n\n  \n    \u003cdiv class=\"article-module h-c-page\"\u003e\n      \u003cdiv class=\"h-c-grid\"\u003e\n  \n\n    \u003cfigure class=\"article-image--large\n      \n      \n        h-c-grid__col\n        h-c-grid__col--6 h-c-grid__col--offset-3\n        \n        \n      \"\n      \u003e\n\n      \n      \n        \n        \u003cimg\n            src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/image1_NzKVIVZ.max-1000x1000.png\"\n        \n          alt=\"image1\"\u003e\n        \n        \u003c/a\u003e\n      \n        \u003cfigcaption class=\"article-image__caption \"\u003e\u003cp data-block-key=\"mlmk8\"\u003eCode snippet of an AI Agent\u003c/p\u003e\u003c/figcaption\u003e\n      \n    \u003c/figure\u003e\n\n  \n      \u003c/div\u003e\n    \u003c/div\u003e\n  \n\n\n\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThis agent code snippet showcases the components depicted in the first diagram, where we observe the Agent with a \u003c/span\u003e\u003cstrong style=\"font-style: italic; vertical-align: baseline;\"\u003eName\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, \u003c/span\u003e\u003cstrong style=\"font-style: italic; vertical-align: baseline;\"\u003eLarge Language Model (LLM)\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, \u003c/span\u003e\u003cstrong style=\"font-style: italic; vertical-align: baseline;\"\u003eDescription, Instruction and Tools\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, all of which are utilized to enable the agent to perform its designated functions.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eTo build enterprise-grade agents at scale, several factors must be considered during their ground-up development. Google Cloud has collaborated closely with its Partner ecosystem to employ cutting-edge Google Cloud products to build scalable and enterprise-ready agents.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eA key consideration in agent development is the framework. Without it, developers would be compelled to build everything from scratch, including state management, tool handling, and workflow orchestration. This often results in systems that are complex, difficult to debug, insecure, and ultimately unscalable. \u003c/span\u003e\u003ca href=\"https://google.github.io/adk-docs/\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGoogle Cloud Agent Development Kit\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e (ADK) provides essential scaffolding, tools, and patterns for efficient and secure enterprise agent development at scale. It offers developers the flexibility to customize agents to suit nearly every applicable use case.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eAgent development with any framework, especially multi-agent architectures in enterprises, necessitates robust compute resources and scalable infrastructure. This includes strong security measures, comprehensive tracing, logging, and monitoring capabilities, as well as rigorous evaluation of the agentâ€™s decisions and output.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eFurthermore, agents typically lack inherent memory, meaning they cannot recall past interactions or maintain context for effective operation. While frameworks like ADK offer ephemeral memory storage for agents, enterprise-grade agents demand persistent memory. This persistent memory is vital for equipping agents with the necessary context to enhance their performance and the quality of their output.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eGoogle Cloudâ€™s \u003c/span\u003e\u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eVertex AI Agent Engine\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e provides a secure runtime for agents that manages their lifecycle, orchestrates tools, and drives reasoning. It features built-in security, observability, and critical building blocks such as a memory bank, session service, and sandbox. Agent Engine is accessible to SIs and customers on Google Cloud. Alternative options for running agents at scale include\u003c/span\u003e\u003ca href=\"https://cloud.google.com/run?utm_source=google\u0026amp;utm_medium=cpc\u0026amp;utm_campaign=na-US-all-en-dr-bkws-all-all-trial-e-dr-1710134\u0026amp;utm_content=text-ad-none-any-DEV_c-CRE_772382725898-ADGP_Hybrid+%7C+BKWS+-+EXA+%7C+Txt-AppMod-Serverless-Cloud+Run-KWID_353039629183-kwd-353039629183\u0026amp;utm_term=KW_cloud+run-ST_cloud+run\u0026amp;gclsrc=aw.ds\u0026amp;gad_source=1\u0026amp;gad_campaignid=22980675520\u0026amp;gclid=CjwKCAjwlt7GBhAvEiwAKal0ct7yGb6joV5NddBugRqG4z5nSXqF8svPROtSnu21ZL9YDjjAXB6F7BoCXF4QAvD_BwE\u0026amp;hl=en\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e \u003c/span\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eCloud Run\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e or\u003c/span\u003e\u003ca href=\"https://cloud.google.com/kubernetes-engine?hl=en\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e \u003c/span\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGKE\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e.Â \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eCustomers often opt for these alternatives when they already have existing investments in Cloud Run or GKE infrastructure on Google Cloud, or when they require configuration flexibility concerning compute, storage, and networking, as well as flexible cost management. However, when choosing Cloud Run or GKE, functions like memory and session management must be built and managed from the ground up.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://modelcontextprotocol.io/docs/getting-started/intro\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eModel Context Protocol\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e (MCP) is a crucial element for modern AI agent architectures. This open protocol standardizes how applications provide context to LLMs, thereby improving agent responses by connecting agents and underlying AI models to various data sources and tools. It's important to note that Agents also communicate with enterprise systems using APIs, which are referred to as Tools when employed with agents. MCP enables agents to access fresh external data.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eWhen developing enterprise agents at scale, it is recommended to deploy the MCP servers separately on a serverless platform like Cloud Run or GKE on Google Cloud, with agents running on Agent Engine configured as clients. The sample architecture illustrates the recommended deployment model for MCP integration with ADK agents;\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-image_full_width\"\u003e\n\n\n\n\n\n\n  \n    \u003cdiv class=\"article-module h-c-page\"\u003e\n      \u003cdiv class=\"h-c-grid\"\u003e\n  \n\n    \u003cfigure class=\"article-image--large\n      \n      \n        h-c-grid__col\n        h-c-grid__col--6 h-c-grid__col--offset-3\n        \n        \n      \"\n      \u003e\n\n      \n      \n        \n        \u003cimg\n            src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/image4_jzjerKh.png\"\n        \n          alt=\"2-AI_agent_tool_integration_with_MCP\"\u003e\n        \n        \u003c/a\u003e\n      \n        \u003cfigcaption class=\"article-image__caption \"\u003e\u003cp data-block-key=\"mlmk8\"\u003eAI agent tool integration with MCP\u003c/p\u003e\u003c/figcaption\u003e\n      \n    \u003c/figure\u003e\n\n  \n      \u003c/div\u003e\n    \u003c/div\u003e\n  \n\n\n\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe reference architecture demonstrates how\u003c/span\u003e \u003ca href=\"https://google.github.io/adk-docs/mcp/\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eADK built agents can integrate with MCP\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e to connect data sources and provide context to underlying LLM models. The MCP utilizes Get, Invoke, List, and Call functions to enable tools to connect agents to external data sources. In this scenario, the agent can interact with a Graph database through application APIs using MCP, allowing the agent and the underlying LLM to access up-to-date data for generating meaningful responses.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eFurthermore, when building multi-agent architectures that demand interoperability and communication among agents from different systems, a key consideration is how to facilitate Agent-to-Agent communication. This addresses complex use cases that require workflow execution across various agents from different domains.Â \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eGoogle Cloud launched the\u003c/span\u003e \u003ca href=\"https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eAgent-to-Agent Protocol\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e (A2A) with native support within Agent Engine to tackle the challenge of inter-agent communication at scale. Learn how to implement A2A from this\u003c/span\u003e \u003ca href=\"https://discuss.google.dev/t/building-bridges-deploy-agents-with-a2a-on-vertex-ai-agent-engine/264044\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eblog\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eGoogle Cloud has collaborated with SIs on agentic architecture and design considerations to build multiple agents, assisting clients in addressing various use cases across industry domains such as Retail, Manufacturing, Healthcare, Automotive, and Financial Services. The reference architecture below consolidates these considerations.\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\n\u003cdiv class=\"block-image_full_width\"\u003e\n\n\n\n\n\n\n  \n    \u003cdiv class=\"article-module h-c-page\"\u003e\n      \u003cdiv class=\"h-c-grid\"\u003e\n  \n\n    \u003cfigure class=\"article-image--large\n      \n      \n        h-c-grid__col\n        h-c-grid__col--6 h-c-grid__col--offset-3\n        \n        \n      \"\n      \u003e\n\n      \n      \n        \n        \u003cimg\n            src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/image3_9qMCmk5.max-1000x1000.png\"\n        \n          alt=\"3-Reference architecture - Agentic_AI_system_with_ADK_MCP_A2A_and_Agent_Engine\"\u003e\n        \n        \u003c/a\u003e\n      \n        \u003cfigcaption class=\"article-image__caption \"\u003e\u003cp data-block-key=\"mlmk8\"\u003eReference architecture - Agentic AI system with ADK, MCP, A2A and Agent Engine\u003c/p\u003e\u003c/figcaption\u003e\n      \n    \u003c/figure\u003e\n\n  \n      \u003c/div\u003e\n    \u003c/div\u003e\n  \n\n\n\n\n\u003c/div\u003e\n\u003cdiv class=\"block-paragraph_advanced\"\u003e\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThis reference architecture depicts an enterprise-grade Agent built on Google Cloud to address a supply chain use case. In this architecture, all agents are built with the ADK framework and deployed on Agent Engine. Agent Engine provides a secure compute runtime with authentication, context management using managed\u003c/span\u003e \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/sessions/overview\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003esessions\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e,\u003c/span\u003e\u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/memory-bank/overview\"\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e \u003c/span\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003ememory\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, and quality assurance through\u003c/span\u003e \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/example-store/overview\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eExample Store\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e and\u003c/span\u003e \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/evaluate\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eEvaluation Services\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, while also offering observability into the deployed agents. Agent Engine delivers all these features and many more as a managed service at scale on GCP.Â \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThis architecture outlines an Agentic supply chain featuring an orchestration agent (Root) and three dedicated sub-agents: \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eTracking, Distributor, and Order Agents\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e. Each of these agents are powered by \u003c/span\u003e\u003ca href=\"https://ai.google.dev/gemini-api/docs/models\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGemini\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e. For optimal performance and tailored responses, especially in specific use cases, we recommend \u003c/span\u003e\u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003etuning\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e your model with domain-specific data before integration with an agent. Model tuning can also help optimize responses for conciseness, potentially leading to reduced token size and lower operational costs.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eFor instance, a user might send a request such as \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003e\"show me the inventory levels for menâ€™s backpack.\" \u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eRoot agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e receives this request and is capable of routing it to the \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eOrder agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e, which is responsible for inventory and order operations. This routing is seamless because the A2A protocol utilizes\u003c/span\u003e \u003ca href=\"https://a2a-protocol.org/dev/tutorials/python/3-agent-skills-and-card/\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eagent cards\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e to advertise the capabilities of each respective agent. A2A is\u003c/span\u003e \u003ca href=\"https://google.github.io/adk-docs/a2a/\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003econfigured\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e with a few steps as a wrapper for your agents for Agent Engine deployment.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eIn this example, inventory and order details are stored in\u003c/span\u003e \u003ca href=\"https://cloud.google.com/bigquery?utm_source=google\u0026amp;utm_medium=cpc\u0026amp;utm_campaign=na-US-all-en-dr-bkws-all-all-trial-e-dr-1710134\u0026amp;utm_content=text-ad-none-any-DEV_c-CRE_772298885512-ADGP_Hybrid+%7C+BKWS+-+EXA+%7C+Txt-Data+Analytics-Data+Analytics-BigQuery-KWID_47616965283-kwd-47616965283\u0026amp;utm_term=KW_bigquery-ST_bigquery\u0026amp;gclsrc=aw.ds\u0026amp;gad_source=1\u0026amp;gad_campaignid=22970352687\u0026amp;gclid=CjwKCAjwlt7GBhAvEiwAKal0cguWTbXzjpOrhDcy2MvwgIkyd7YGbTldV885duGilt-aWh2I5Q4U_RoC5sQQAvD_BwE\u0026amp;hl=en\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eBigQuery\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e. Therefore, the agent uses its tool configuration to leverage the MCP server to fetch the inventory details from the BigQuery data warehouse. The response is then returned to the underlying LLM, which generates a formatted natural language response and provides the inventory details for menâ€™s backpacks to the \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eRoot agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e and subsequently to the user. Based on this response, the user can, for example, place an order to replenish the inventory.Â \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eWhen such a request is made, the Root agent routes it to the \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eDistributor agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e. This agent possesses knowledge of all suppliers who provide stock to the business. Depending on the item being requested, the agent will use its tools to initiate an MCP server connection to the correct external API endpoints for the respective supplier to place the order. If the suppliers have agents configured, the A2A protocol can also be utilized to send the request to the supplier's agent for processing. Any acknowledgment of the order is then sent back to the \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eDistributor agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e.Â \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eIn this reference architecture, when the \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eDistributor agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e receives acknowledgment, A2A enables the agent to detect the presence of a \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eTracking agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e that monitors new orders until delivery. The \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eDistributor agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e will pass the order details to the \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eTracking agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e and also send updates back to the user. The \u003c/span\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eTracking agent\u003c/strong\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e will then send order updates to the user via messaging, utilizing the public API endpoint of the supplier. This is merely one example of a workflow that could be built with this reference architecture.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThis modular architecture can be adapted to solve various use cases with Agentic AI built with ADK and deployed to Agent Engine.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThe reference architecture allows this multi-agent system to be consumed via a chat interface through a website or a custom-built user interface. It is also possible to integrate this agentic AI architecture with Google Cloud Gemini Enterprise.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eÂ Learn how enterprises can start by using Gemini Enterprise as the front door to Google Cloud AI from this \u003c/span\u003e\u003ca href=\"https://blog.google/products/google-cloud/gemini-enterprise-sundar-pichai/\" rel=\"noopener\" target=\"_blank\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eblog\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e from Alphabet CEO Sundar Pichai. This approach helps enterprises to start small using low code out of the box agents. As they mature, they can now implement complex use cases with advanced high code AI agents using this reference archiecture .Â \u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong style=\"vertical-align: baseline;\"\u003eGetting started\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eThis blog post has explored the design patterns for building intelligent enterprise AI agents. For enterprise decision makers, use the \u003c/span\u003e\u003ca href=\"https://cloud.google.com/transform/5-elements-to-start-implementing-agentic-solutions-a-guide-for-leaders?e=13802955\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003e5 essential elements to start implementing agentic solutions\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e to help guide your visionary strategy and decision making when it comes to running enterprise agents at scale.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"vertical-align: baseline;\"\u003eWe encourage you to embark on this journey today by collaborating with \u003c/span\u003e\u003ca href=\"https://cloud.google.com/find-a-partner/\"\u003e\u003cspan style=\"text-decoration: underline; vertical-align: baseline;\"\u003eGoogle Cloud Partner Ecosystem\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"vertical-align: baseline;\"\u003e to understand your enterprise landscape and identify complex use cases that can be effectively addressed with AI Agents. Utilize these design patterns as your guide and leverage the ADK to transform your enterprise use case into a powerful, scalable solution that delivers tangible business value on Agent Engine with Google Cloud.\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e"
    }
  ],
  "generatedAt": "2025-10-21T07:08:25.623690915+09:00"
}
